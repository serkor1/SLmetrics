# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Fowlkes-Mallows Index (FMI)
#'
#' @description
#' The [fmi()]-function computes the [Fowlkes-Mallows Index](https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index) (FMI), a measure of the similarity between two sets of clusterings, between
#' two vectors of predicted and observed [factor()] values.
#'
#' @usage
#' # fowlkes-mallows index
#' fmi(
#'   actual,
#'   predicted
#' )
#'
#' @example man/examples/scr_fmi.R
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The FMI Index is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \sqrt{\frac{\#TP_k}{\#TP_k + \#FP_k} \times \frac{\#TP_k}{\#TP_k + \#FN_k}}
#' }
#'
#' Where \eqn{\#TP_k}, \eqn{\#FP_k}, and \eqn{\#FN_k} represent the number of true positives, false positives, and false negatives for each class \eqn{k}, respectively.
#'
#'
#' @returns
#' A <[numeric]> vector of [length] 1
#'
#' @family classification
#'
#' @export
fmi <- function(actual, predicted) {
    .Call(`_SLmetrics_fmi`, actual, predicted)
}

#' Accuracy
#'
#' The [accuracy()]-function computes the [accuracy](https://en.wikipedia.org/wiki/Precision_and_recall) between two
#' vectors of predicted and observed [factor()] values.
#'
#' @usage
#' accuracy(
#'   actual,
#'   predicted
#' )
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' Accuracy is a global metric that measures the proportion of correct predictions (both true positives and true negatives) out of all predictions, and is calculated as follows,
#'
#' \deqn{
#'   \frac{\#TP + \#TN}{\#TP + \#TN + \#FP + \#FN}
#' }
#'
#' Where \eqn{\#TP}, \eqn{\#TN}, \eqn{\#FP}, and \eqn{\#FN} represent the true positives, true negatives, false positives, and false negatives, respectively.
#'
#' Accuracy provides an overall performance measure of the model across all classes.
#'
#' @returns
#'
#' A <[numeric]>-vector of [length] 1
#'
#' @example man/examples/scr_accuracy.R
#'
#' @family classification
#'
#' @export
accuracy <- function(actual, predicted) {
    .Call(`_SLmetrics_accuracy`, actual, predicted)
}

#' Confusion Matrix
#'
#' @description
#'
#' The [cmatrix()]-function uses cross-classifying factors to build
#' a confusion matrix of the counts at each combination of the [factor] levels.
#' Each row of the [matrix] represents the actual [factor] levels, while each
#' column represents the predicted [factor] levels.
#'
#' @usage
#' cmatrix(
#'   actual,
#'   predicted
#' )
#'
#' @param actual A <[factor]>-vector of [length] \eqn{n}, and \eqn{k} levels.
#' @param predicted A <[factor]>-vector of [length] \eqn{n}, and \eqn{k} levels.
#'
#' @example man/examples/scr_confusionmatrix.R
#' @family classification
#'
#' @inherit specificity details
#'
#' @section Dimensions:
#'
#' There is no robust defensive measure against misspecififying
#' the confusion matrix. If the arguments are correctly specified, the resulting
#' confusion matrix is on the form:
#'
#' |            | A (Predicted) | B (Predicted) |
#' | :----------|:-------------:| -------------:|
#' | A (Actual) | Value         | Value         |
#' | B (Actual) | Value         | Value         |
#'
#'
#' @returns
#'
#' A named \eqn{k} x \eqn{k} <[matrix]> of [class] <cmatrix>
#'
#' @export
cmatrix <- function(actual, predicted) {
    .Call(`_SLmetrics_cmatrix`, actual, predicted)
}

#' Compute the \eqn{\text{Diagnostic Odds Ratio}}
#'
#' @description
#' The [dor()]-function computes the [Diagnostic Odds Ratio](https://en.wikipedia.org/wiki/Diagnostic_odds_ratio) (DOR), a single indicator of test performance, between
#' two vectors of predicted and observed [factor()] values.
#'
#' When `aggregate = TRUE`, the function returns the micro-average DOR across all classes \eqn{k}. By default, it returns the class-wise DOR.
#'
#' @usage
#' dor(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @example man/examples/scr_diagnosticodssratio.R
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The Diagnostic Odds Ratio (DOR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \text{DOR}_k = \frac{\text{PLR}_k}{\text{NLR}_k}
#' }
#'
#' Where \eqn{\text{PLR}_k} and \eqn{\text{NLR}_k} is the Positive and Negative Likelihood Ratio for class \eqn{k}, respectively. See [plr()] and [nlr()] for more details.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated as,
#'
#' \deqn{
#'   \overline{\text{DOR}} = \frac{\overline{\text{PLR}_k}}{\overline{\text{NLR}_k}}
#' }
#'
#' Where \eqn{\overline{\text{PLR}}} and \eqn{\overline{\text{NLR}}} is the micro-averaged is the Positive and Negative Likelihood Ratio, respectively.
#'
#' @family classification
#'
#' @export
dor <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_dor`, actual, predicted, aggregate)
}

#' \eqn{F_{\beta}}-score
#'
#' @description
#' The [fbeta()]-function computes the [F-beta score](https://en.wikipedia.org/wiki/F1_score), a weighted harmonic mean of precision and recall, between
#' two vectors of predicted and observed [factor()] values. The parameter \eqn{\beta} determines the weight of precision and recall in the combined score. When `aggregate = TRUE`, the function returns the micro-average F-beta score across all classes \eqn{k}.
#' By default, it returns the class-wise F-beta score.
#'
#' @usage
#' # fbeta-score
#' fbeta(
#'   actual,
#'   predicted,
#'   beta = 1,
#'   aggregate = FALSE
#' )
#'
#' @example man/examples/scr_fbeta.R
#'
#' @inherit specificity
#'
#' @param beta A <[numeric]> vector of length 1. 1 by default, see details.
#' @param aggregate A <[logical]>-value of [length] 1. [FALSE] by default. If [TRUE] it returns the
#' micro average across all k-classes
#'
#' @section Calculation:
#'
#'
#' The F-beta score is a weighted harmonic mean of precision and recall, calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   (1 + \beta^2) \cdot \frac{\text{Precision}_k \cdot \text{Recall}_k}{(\beta^2 \cdot \text{Precision}_k) + \text{Recall}_k}
#' }
#'
#' Where precision is \eqn{\frac{\#TP_k}{\#TP_k + \#FP_k}} and recall (sensitivity) is \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}}, and \eqn{\beta} determines the weight of precision relative to recall.
#'
#' When `aggregate = TRUE`, the `micro`-average F-beta score is calculated,
#'
#' \deqn{
#'   (1 + \beta^2) \cdot \frac{\sum_{k=1}^K \text{Precision}_k \cdot \sum_{k=1}^K \text{Recall}_k}{(\beta^2 \cdot \sum_{k=1}^K \text{Precision}_k) + \sum_{k=1}^K \text{Recall}_k}
#' }
#'
#'
#' @family classification
#' @export
fbeta <- function(actual, predicted, beta = 1.0, aggregate = FALSE) {
    .Call(`_SLmetrics_fbeta`, actual, predicted, beta, aggregate)
}

#' False Discovery Rate (FDR)
#'
#' @description
#' The [fdr()]-function computes the [False Discovery Rate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR), the proportion of false positives among the predicted positives, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average FDR across all classes \eqn{k}.
#' By default, it returns the class-wise FDR.
#'
#' @example man/examples/scr_fdr.R
#'
#' @usage
#' # false discovery rate;
#' fdr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The False Discovery Rate (FDR). The metric is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#FP_k}{\#TP_k+\#FP_k}
#' }
#'
#' Where \eqn{\#TP_k} and \eqn{\#FP_k} is the number of true psotives and false positives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE` the `micro`-average is calculated,
#'
#' \deqn{
#'  \frac{\sum_{k=1}^k \#FP_k}{\sum_{k=1}^k \#TP_k + \sum_{k=1}^k \#FP_k}
#' }
#'
#' @family classification
#' @export
fdr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fdr`, actual, predicted, aggregate)
}

#' False Exclusion Rate (FER)
#'
#' @description
#' The [fer()]-function computes the [False Omission Rate](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#False_omission_rate) (FOR), the proportion of false negatives among the predicted negatives, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average FOR across all classes \eqn{k}.
#' By default, it returns the class-wise FOR.
#'
#' @example man/examples/scr_for.R
#'
#' @usage
#' # false exclusion rate
#' fer(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The False Omission Rate (FOR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#FN_k}{\#FN_k + \#TN_k}
#' }
#'
#' Where \eqn{\#FN_k} and \eqn{\#TN_k} are the number of false negatives and true negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#FN_k}{\sum_{k=1}^k \#FN_k + \sum_{k=1}^k \#TN_k}
#' }
#'
#' @family classification
#' @export
fer <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fer`, actual, predicted, aggregate)
}

#' False Positive Rate (FPR)
#'
#' @description
#' The [fpr()]-function computes the [False Positive Rate](https://en.wikipedia.org/wiki/False_positive_rate) (FPR), also known as the fall-out ([fallout()]), between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average FPR across all classes \eqn{k}.
#' By default, it returns the class-wise FPR.
#'
#' @usage
#' # using`fpr()`
#' fpr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @example man/examples/scr_fpr.R
#'
#' @section Calculation:
#'
#' The False Positive Rate (FPR) for each class \eqn{k} is calculated as follows,
#'
#' \deqn{
#'   \frac{\#FP_k}{\#FP_k + \#TN_k}
#' }
#'
#' Where \eqn{\#FP_k} and \eqn{\#TN_k} represent the number of false positives and true negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the micro-average is calculated across all classes,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#FP_k}{\sum_{k=1}^k \#FP_k + \sum_{k=1}^k \#TN_k}
#' }
#'
#' The FPR is the complement of specificity, such that \eqn{\text{FPR} = 1 - \text{Specificity}}.
#'
#' @family classification
#' @export
fpr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fpr`, actual, predicted, aggregate)
}

#' @rdname fpr
#' @usage
#' # using `fallout()`
#' fallout(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
fallout <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fallout`, actual, predicted, aggregate)
}

#' Jaccard Index
#'
#' @description
#' The [jaccard()]-function computes the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index), also known as the Intersection over Union, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average Jaccard Index across all classes \eqn{k}.
#' By default, it returns the class-wise Jaccard Index.
#'
#' @usage
#' # using `jaccard()`-function
#' jaccard(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The Jaccard Index is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k}{\#TP_k + \#FP_k + \#FN_k}
#' }
#'
#' Where \eqn{\#TP_k}, \eqn{\#FP_k}, and \eqn{\#FN_k} represent the number of true positives, false positives, and false negatives for each class \eqn{k}, respectively.
#'
#' When `aggregate = TRUE`, the micro-average is calculated as,
#'
#' \deqn{
#'   \frac{\sum_{i = 1}^{k} TP_i}{\sum_{i = 1}^{k} TP_i + \sum_{i = 1}^{k} FP_k + \sum_{i = 1}^{k} FN_k}
#' }
#'
#' @example man/examples/scr_jaccard.R
#'
#' @family classification
#'
#' @export
jaccard <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_jaccard`, actual, predicted, aggregate)
}

#' @rdname jaccard
#'
#' @usage
#' # using `csi()`-function
#' csi(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
csi <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_csi`, actual, predicted, aggregate)
}

#' @rdname jaccard
#'
#' @usage
#' # using `tscore()`-function
#' tscore(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
tscore <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_tscore`, actual, predicted, aggregate)
}

#' Cohen's \eqn{\kappa}-statistic
#'
#' @description
#' The [kappa()]-function computes [Cohen's \eqn{\kappa}](https://en.wikipedia.org/wiki/Cohen%27s_kappa), a statistic that measures inter-rater agreement for categorical items between
#' two vectors of predicted and observed [factor()] values. If \eqn{\beta \neq 0} the off-diagonals of the confusion matrix are penalized with a factor of
#' \eqn{(y_{+} - y_{i,-})^\beta}. See below for further details.
#'
#' @usage
#' kappa(
#'   actual,
#'   predicted,
#'   beta = 0
#' )
#'
#' @example man/examples/scr_kappa.R
#'
#' @inherit specificity
#'
#' @inheritParams specificity
#' @param beta A <[numeric]> value of [length] 1. 0 by default. If set to a value different from zero, the off-diagonal confusion matrix will be penalized.
#'
#'
#' @section Calculation
#'
#' @family classification
#' @export
kappa <- function(actual, predicted, beta = 0) {
    .Call(`_SLmetrics_kappa`, actual, predicted, beta)
}

#' Positive Likelihood Ratio (PLR)
#'
#' @description
#' The [plr()]-function computes the [positive likelihood ratio](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing), also known as the likelihood ratio for positive results, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average PLR across all classes \eqn{k}.
#' By default, it returns the class-wise PLR.
#'
#' @usage
#' plr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @example man/examples/scr_plr_nlr.R
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The Positive Likelihood Ratio (PLR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\text{Sensitivity}_k}{1 - \text{Specificity}_k}
#' }
#'
#' Where sensitivity (or true positive rate) is calculated as \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}} and specificity (or true negative rate) is calculated as \eqn{\frac{\#TN_k}{\#TN_k + \#FP_k}}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \text{Sensitivity}_k}{1 - \sum_{k=1}^k \text{Specificity}_k}
#' }
#'
#' @seealso
#'
#' The [nlr()]-function for the Negative Likehood Ratio (LR-)
#'
#' @family classification
#' @export
plr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_plr`, actual, predicted, aggregate)
}

#' Negative Likelihood Ratio (NLR)
#'
#' @description
#' The [nlr()]-function computes the [negative likelihood ratio](https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing), also known as the likelihood ratio for negative results, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average NLR across all classes \eqn{k}.
#' By default, it returns the class-wise NLR.
#'
#' @usage
#' nlr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @example man/examples/scr_plr_nlr.R
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The Negative Likelihood Ratio (NLR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{1 - \text{Sensitivity}_k}{\text{Specificity}_k}
#' }
#'
#' Where sensitivity (or true positive rate) is calculated as \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}} and specificity (or true negative rate) is calculated as \eqn{\frac{\#TN_k}{\#TN_k + \#FP_k}}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k (1 - \text{Sensitivity}_k)}{\sum_{k=1}^k \text{Specificity}_k}
#' }
#'
#' @seealso
#'
#' The [plr()]-function for the Positive Likehood Ratio (LR+)
#' @family classification
#' @export
nlr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_nlr`, actual, predicted, aggregate)
}

#' Matthews Correlation Coefficient (MCC)
#'
#' @description
#' The [mcc()]-function computes the [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) (MCC), also known as the \eqn{\phi}-coefficient, between
#' two vectors of predicted and observed [factor()] values.
#'
#' @usage
#' # 1) `mcc()`-function
#' mcc(
#'   actual,
#'   predicted
#' )
#'
#' @example man/examples/scr_mcc.R
#'
#' @inherit precision
#'
#' @section Calculation:
#'
#' The MCC is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k \times \#TN_k - \#FP_k \times \#FN_k}{\sqrt{(\#TP_k + \#FP_k)(\#TP_k + \#FN_k)(\#TN_k + \#FP_k)(\#TN_k + \#FN_k)}}
#' }
#'
#'
#' @returns
#' A named <[numeric]> vector of length k
#'
#' @family classification
#'
#' @export
mcc <- function(actual, predicted) {
    .Call(`_SLmetrics_mcc`, actual, predicted)
}

#' @rdname mcc
#'
#' @usage
#' # 2) `phi()`-function
#' phi(
#'   actual,
#'   predicted
#' )
#'
#' @export
phi <- function(actual, predicted) {
    .Call(`_SLmetrics_phi`, actual, predicted)
}

#' Negative Predictive Value (NPV)
#'
#' @description
#' The [npv()]-function computes the [negative predictive value](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values), also known as the True Negative Predictive Value, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average NPV across all classes \eqn{k}.
#' By default, it returns the class-wise NPV.
#'
#' @usage
#' npv(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @example man/examples/scr_npv.R
#'
#' @section Calculation:
#'
#' The Negative Predictive Value (NPV) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TN_k}{\#TN_k + \#FN_k}
#' }
#'
#' Where \eqn{\#TN_k} and \eqn{\#FN_k} are the number of true negatives and false negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TN_k}{\sum_{k=1}^k \#TN_k + \sum_{k=1}^k \#FN_k}
#' }
#'
#' @family classification
#'
#' @export
npv <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_npv`, actual, predicted, aggregate)
}

#' Precision (Positive Predictive Value)
#'
#' @description
#' The [precision()]-function computes the [precision](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values), also known as the Positive Predictive Value (PPV), between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average precision across all classes \eqn{k}.
#' By default, it returns the class-wise precision.
#'
#' @usage
#' # 1) `precision()`-function
#' precision(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' The Precision (Positive Predictive Value, PPV) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k}{\#TP_k + \#FP_k}
#' }
#'
#' Where \eqn{\#TP_k} and \eqn{\#FP_k} are the number of true positives and false positives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TP_k}{\sum_{k=1}^k \#TP_k + \sum_{k=1}^k \#FP_k}
#' }
#'
#' @example man/examples/scr_precision.R
#'
#' @returns
#' A named <[numeric]> vector of length k
#'
#'
#' @family classification
#'
#' @export
precision <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_precision`, actual, predicted, aggregate)
}

#' @rdname precision
#'
#'
#' @usage
#' # 2) `ppv()`-function
#' ppv(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
ppv <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_ppv`, actual, predicted, aggregate)
}

#' Compute the \eqn{recall}, \eqn{sensitivity} or \eqn{true~positive~rate}
#'
#' @description
#' The [recall()]-function computes the [recall](https://en.wikipedia.org/wiki/Sensitivity_and_specificity), also known as sensitivity or the True Positive Rate (TPR), between
#' two vectors of predicted and observed [factor()] values.
#'
#' When `aggregate = TRUE`, the function returns the micro-averaged recall across all classes \eqn{k}. By default, it returns the class-wise recall.
#'
#' @usage
#'  # 1) `recall()`-function
#' recall(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @example man/examples/scr_recall.R
#'
#' @section Calculation:
#'
#' The metric is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k}{\#TP_k + \#FN_k}
#' }
#'
#' Where \eqn{\#TP_k} and \eqn{\#FN_k} is the number of true positives and false negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE` the `micro`-average is calculated as follows,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TP_k}{\sum_{k=1}^k \#TP_k + \sum_{k=1}^k \#FN_k}
#' }
#'
#' @family classification
#'
#' @export
recall <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_recall`, actual, predicted, aggregate)
}

#' @rdname recall
#'
#' @usage
#' # 2) `sensitivity()`-function
#' sensitivity(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @export
sensitivity <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_sensitivity`, actual, predicted, aggregate)
}

#' @rdname recall
#'
#' @usage
#' # 3) `tpr()`-function
#' tpr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @export
tpr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_tpr`, actual, predicted, aggregate)
}

#' Specificity (True Negative Rate)
#'
#' @description
#' The  [specificity()]-function computes the [specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity), also known as the True Negative Rate (TNR) or selectivity, between
#' two vectors of predicted and observed [factor()] values. When `aggregate = TRUE`, the function returns the micro-average specificity across all classes \eqn{k}.
#' By default, it returns the class-wise specificity.
#'
#'
#' @usage
#' # using `specificity()`
#' specificity(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inheritParams cmatrix
#' @param aggregate A <[logical]>-value of [length] \eqn{1}. [FALSE] by default. If [TRUE] it returns the
#' micro average across all \eqn{k} classes
#'
#'
#' @details
#'
#' Consider a classification problem with three classes: `A`, `B`, and `C`. The actual vector of [factor()] values is defined as follows:
#'
#' ```{r output, echo = TRUE}
#' ## actual
#' factor(
#'   x = sample(x = 1:3, size = 10, replace = TRUE),
#'   levels = c(1, 2, 3),
#'   labels = c("A", "B", "C")
#' )
#' ```
#'
#' Here, the values 1, 2, and 3 are mapped to `A`, `B`, and `C`, respectively. Now, suppose your model does not predict any `B`'s. The predicted vector of [factor()] values would be defined as follows:
#'
#' ```{r output, echo = TRUE}
#' ## predicted
#' factor(
#'   x = sample(x = c(1, 3), size = 10, replace = TRUE),
#'   levels = c(1, 2, 3),
#'   labels = c("A", "B", "C")
#' )
#' ```
#'
#' In both cases, \eqn{k = 3}, determined indirectly by the `levels` argument.
#'
#' @returns
#'
#' If `aggregate` is [FALSE] (the default), a named <[numeric]>-vector of [length] k
#'
#' If `aggregate` is [TRUE], a <[numeric]>-vector of [length] 1
#'
#' @example man/examples/scr_specificity.R
#'
#'
#' @section Calculation:
#' The metric is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TN_k}{\#TN_k+\#FP_k}
#' }
#'
#' Where \eqn{\#TN_k} and \eqn{\#FP_k} is the number of true negatives and false positives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE` the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TN_k}{\sum_{k=1}^k \#TN_k + \sum_{k=1}^k \#FP_k}
#' }
#'
#'
#' @family classification
#'
#' @export
specificity <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_specificity`, actual, predicted, aggregate)
}

#' @rdname specificity
#'
#' @usage
#' # using `tnr()`
#' tnr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @export
tnr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_tnr`, actual, predicted, aggregate)
}

#' @rdname specificity
#'
#' @usage
#' # using `selectivity()`
#' selectivity(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
selectivity <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_selectivity`, actual, predicted, aggregate)
}

#' Zero-One Loss
#'
#' @description
#' The [zerooneloss()]-function computes the [Zero-One Loss](https://en.wikipedia.org/wiki/Loss_functions_for_classification), a classification loss function that calculates the proportion of misclassified instances between
#' two vectors of predicted and observed [factor()] values.
#'
#' @usage
#' zerooneloss(
#'   actual,
#'   predicted
#' )
#'
#' @inherit specificity
#'
#' @section Calculation:
#'
#' Zero-One Loss is a global metric that measures the proportion of incorrect predictions made by the model. It is calculated as follows,
#'
#' \deqn{
#'   \frac{\#FP + \#FN}{\#TP + \#TN + \#FP + \#FN}
#' }
#'
#' Where \eqn{\#TP}, \eqn{\#TN}, \eqn{\#FP}, and \eqn{\#FN} represent the true positives, true negatives, false positives, and false negatives, respectively.
#'
#' Zero-One Loss provides an overall measure of the model's prediction errors across all classes.
#'
#' @returns
#'
#' A <[numeric]>-vector of [length] 1
#'
#' @example man/examples/scr_zerooneloss.R
#'
#' @family classification
#'
#' @export
zerooneloss <- function(actual, predicted) {
    .Call(`_SLmetrics_zerooneloss`, actual, predicted)
}

#' Concordance Correlation Coefficient (CCC)
#'
#' Calculate the CCC using the [ccc()]-function for comparing actual and predicted values.
#'
#' @usage
#' # simple;
#' ccc(
#'   actual,
#'   predicted,
#'   correction = FALSE
#' )
#'
#' @inherit huberloss
#' @param correction A <[logical]> vector of [length] 1. [FALSE] by default. If [TRUE] the variance and covariance
#' will be adjusted with \eqn{\frac{1-n}{n}}
#'
#' @example man/examples/scr_ccc.R
#'
#' @family regression
#' @export
ccc <- function(actual, predicted, correction = FALSE) {
    .Call(`_SLmetrics_ccc`, actual, predicted, correction)
}

#' @rdname ccc
#'
#' @usage
#' # weighted;
#' wccc(
#'   actual,
#'   predicted,
#'   w,
#'   correction = FALSE
#' )
#'
#' @family regression
#' @export
wccc <- function(actual, predicted, w, correction = FALSE) {
    .Call(`_SLmetrics_wccc`, actual, predicted, w, correction)
}

#' Huber Loss
#'
#' @description
#'
#' Calculate the Huber Loss. [whuberloss()] calculates the arithmetic weighted average, and [huberloss()] calculates the arithmetic average.
#'
#' @usage
#' # simple mean
#' huberloss(
#'   actual,
#'   predicted,
#'   delta = 1
#' )
#'
#' @param actual A <[numeric]>-vector of [length] \eqn{n}. The observed (continuous) response variable.
#' @param predicted A <[numeric]>-vector of [length] \eqn{n}. The estimated (continuous) response variable.
#' @param delta A <[numeric]>-vector of [length] 1. 1 by default. The threshold value for switch between functions (see details).
#'
#' @details
#'
#' The Huber Loss is calculated as,
#'
#' \deqn{
#'  \frac{1}{2} (y_i - \hat{y}_i)^2 ~for~ |y_i - \hat{y}_i| \leq \delta
#' }
#'
#' \deqn{
#'   \delta |y_i-\hat{y}_i|-\frac{1}{2} \delta^2 ~for~ |y_i - \hat{y}_i| > \delta
#' }
#'
#' for each \eqn{i},
#'
#'
#' @example man/examples/scr_huberloss.R
#'
#'
#' @family regression
#'
#' @returns A <[numeric]>-value of [length] 1.
#'
#' @export
huberloss <- function(actual, predicted, delta = 1) {
    .Call(`_SLmetrics_huberloss`, actual, predicted, delta)
}

#' @rdname huberloss
#' @usage
#' # weighted mean
#' whuberloss(
#'   actual,
#'   predicted,
#'   w,
#'   delta = 1
#' )
#' @param w A <[numeric]>-vector of [length] N. The weight assigned to each observation in the data. See [stats::weighted.mean()] for more details.
#' @export
whuberloss <- function(actual, predicted, w, delta = 1) {
    .Call(`_SLmetrics_whuberloss`, actual, predicted, w, delta)
}

#' Mean Absolute Error (MAE)
#'
#' Calculate the MAE using the [mae()]-function for the (arithmetic) simple mean, or [wmae()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # MAE (Simple Mean)
#' mae(
#'   actual,
#'   predicted
#' )
#'
#'
#' @inherit huberloss
#'
#' @family regression
#'
#' @export
mae <- function(actual, predicted) {
    .Call(`_SLmetrics_mae`, actual, predicted)
}

#' @rdname mae
#'
#' @usage
#' # MAE (Weighted Mean)
#' wmae(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wmae <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wmae`, actual, predicted, w)
}

#' Mean Absolute Percentage Error (MAPE)
#'
#' Calculate the MAPE using the [mape()]-function for the simple mean, or [wmape()]-function for the weighted mean.
#'
#' @usage
#' # simple MAPE
#' mape(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#' @export
mape <- function(actual, predicted) {
    .Call(`_SLmetrics_mape`, actual, predicted)
}

#' @rdname mape
#'
#' @usage
#' # weighted MAPE
#' wmape(
#'   actual,
#'   predicted,
#'   w
#' )
#' @family regression
#' @export
wmape <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wmape`, actual, predicted, w)
}

#' Mean Percentage Error (MPE)
#'
#' Calculate the MPE using the [mpe()]-function for the simple mean, or [wmpe()]-function for the weighted mean.
#'
#' @usage
#' # simple MPE
#' mpe(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#' @family regression
#' @export
mpe <- function(actual, predicted) {
    .Call(`_SLmetrics_mpe`, actual, predicted)
}

#' @rdname mpe
#'
#' @usage
#' # weighted MPE
#' wmpe(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wmpe <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wmpe`, actual, predicted, w)
}

#' Mean Squared Error (MSE)
#'
#' Calculate the MSE using the [mse()]-function for the (arithmetic) simple mean, or [wmse()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # simple mean
#' mse(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#' @export
mse <- function(actual, predicted) {
    .Call(`_SLmetrics_mse`, actual, predicted)
}

#' @rdname mse
#'
#' @usage
#' # weighted mean
#' wmse(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wmse <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wmse`, actual, predicted, w)
}

#' Root Mean Squared Error (RMSE)
#'
#' Calculate the RMSE using the [rmse()]-function for the (arithmetic) simple mean, or [wrmse()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # simple RMSE
#' rmse(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#' @export
rmse <- function(actual, predicted) {
    .Call(`_SLmetrics_rmse`, actual, predicted)
}

#' @rdname rmse
#'
#' @usage
#' # weighted RMSE
#' wrmse(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wrmse <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wrmse`, actual, predicted, w)
}

#' Root Mean Squared Logarithmic Error (RMSLE)
#'
#' Calculate the RMSLE using the [rmsle()]-function for the (arithmetic) simple mean, or [wrmsle()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # simple RMSLE
#' rmsle(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#' @export
rmsle <- function(actual, predicted) {
    .Call(`_SLmetrics_rmsle`, actual, predicted)
}

#' @rdname rmsle
#'
#' @usage
#' # weighted RMSLE
#' wrmsle(
#'   actual,
#'   predicted,
#'   w
#' )
#'
#' @export
wrmsle <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wrmsle`, actual, predicted, w)
}

#' \eqn{R^2}
#'
#' @description
#' Calculate the \eqn{R^2} of two <[numeric]> vectors.
#'
#' @usage
#' rsq(
#'   actual,
#'   predicted,
#'   k = 0
#' )
#'
#' @inherit huberloss
#' @param k A <[numeric]>-vector of [length] 1. 0 by default. If \eqn{k>0}
#' the function returns the adjusted \eqn{R^2}.
#'
#' @details
#'
#' The \eqn{R^2} is calculated as,
#'
#' \deqn{
#'   1 - \frac{SSE}{SST} \frac{n-1}{n - (k + 1)}
#' }
#'
#' @family regression
#'
rsq <- function(actual, predicted, k = 0) {
    .Call(`_SLmetrics_rsq`, actual, predicted, k)
}

#' Symmetric Mean Absolute Percentage Error (SMAPE)
#'
#' Calculate the SMAPE using the [smape()]-function for the simple mean, or [wsmape()]-function for the weighted mean.
#'
#' @usage
#' # simple SMAPE
#' smape(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#' @export
smape <- function(actual, predicted) {
    .Call(`_SLmetrics_smape`, actual, predicted)
}

#' @rdname smape
#'
#' @usage
#' # weighted SMAPE
#' wsmape(
#'   actual,
#'   predicted,
#'   w
#' )
#'
#' @family regression
#' @export
wsmape <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wsmape`, actual, predicted, w)
}

