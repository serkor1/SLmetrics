{
  "hash": "7e0751aa8f193ab2c345472e6421d905",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat:\n    gfm:\n        default-image-extension: \".png\"\nalways_allow_html: true\nexecute: \n  cache: true\n  freeze: auto\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\" \n    dpi: 1280\n    fig.height: 6\n    out.width: 100%\n---\n\n\n\n# Version 0.3-1\n\n> Version 0.3-1 is considered pre-release of {SLmetrics}. We do not\n> expect any breaking changes, unless a major bug/issue is reported and its nature\n> forces breaking changes.\n\n## :rocket: Improvements\n\n* **OpenMP Support (PR https://github.com/serkor1/SLmetrics/pull/40):** {SLmetrics} now supports parallelization through OpenMP. The OpenMP can be utilized as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) probability distribution\n# generator\nrand.sum <- function(n){\n    x <- sort(runif(n-1))\n    c(x,1) - c(0,x)\n  }\n\n# 2) generate probability\n# matrix\nset.seed(1903)\npk <- t(replicate(100,rand.sum(1e3)))\n\n# 3) Enable OpenMP\nSLmetrics::setUseOpenMP(TRUE)\n#> OpenMP usage set to: enabled\nsystem.time(SLmetrics::entropy(pk))\n#>    user  system elapsed \n#>   0.211   0.001   0.010\n\n# 3) Disable OpenMP\nSLmetrics::setUseOpenMP(FALSE)\n#> OpenMP usage set to: disabled\nsystem.time(SLmetrics::entropy(pk))\n#>    user  system elapsed \n#>   0.001   0.000   0.001\n```\n:::\n\n\n\n* **Entropy with soft labels (https://github.com/serkor1/SLmetrics/issues/37):** `entropy()`, `cross.entropy()` and `relative.entropy()` have been introduced. These functions are heavily inspired by {scipy}.  The functions can be used as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Define actual\n# and observed probabilities\n\n# 1.1) actual probabilies\npk <- matrix(\n  cbind(1/2, 1/2),\n  ncol = 2\n)\n\n# 1.2) observed (estimated) probabilites\nqk <- matrix(\n  cbind(9/10, 1/10), \n  ncol = 2\n)\n\n# 2) calculate\n# Entropy\ncat(\n  \"Entropy\", SLmetrics::entropy(\n    pk\n  ),\n  \"Relative Entropy\", SLmetrics::relative.entropy(\n    pk,\n    qk\n  ),\n  \"Cross Entropy\", SLmetrics::cross.entropy(\n    pk,\n    qk\n  ),\n  sep = \"\\n\"\n)\n#> Entropy\n#> 0.6931472\n#> Relative Entropy\n#> 0.5108256\n#> Cross Entropy\n#> 1.203973\n```\n:::\n\n\n\n## :warning: Breaking changes\n\n* **logloss:** The argument `response` have ben renamed to `qk` as in the `entropy()`-family to maintain some degree of consistency.\n* **entropy.factor():** The function have been deleted and is no more. This was mainly due to avoid the documentation from being too large. The `logloss()`-function replaces it.\n\n## :bug: Bug-fixes\n\n* **Plot-method in ROC and prROC (https://github.com/serkor1/SLmetrics/issues/36):** Fixed a bug in  `plot.ROC()` and `plot.prROC()` where if `panels = FALSE` additional lines would be added to the plot.\n\n# Version 0.3-0\n\n## Improvements\n\n## New Feature\n\n* **Relative Root Mean Squared Error:** The function normalizes the Root Mean Squared Error by a facttor. There is no official way of normalizing it - and in {SLmetrics} the RMSE can be normalized using three options; mean-, range- and IQR-normalization. It can be used as follows,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) define values\nactual <- rnorm(1e3)\npredicted <- actual + rnorm(1e3)\n\n# 2) calculate Relative Root Mean Squared Error\ncat(\n  \"Mean Relative Root Mean Squared Error\", SLmetrics::rrmse(\n    actual        = actual,\n    predicted     = predicted,\n    normalization = 0\n  ),\n  \"Range Relative Root Mean Squared Error\", SLmetrics::rrmse(\n    actual        = actual,\n    predicted     = predicted,\n    normalization = 1\n  ),\n  \"IQR Relative Root Mean Squared Error\", SLmetrics::rrmse(\n    actual        = actual,\n    predicted     = predicted,\n    normalization = 2\n  ),\n  sep = \"\\n\"\n)\n#> Mean Relative Root Mean Squared Error\n#> 40.74819\n#> Range Relative Root Mean Squared Error\n#> 0.1556036\n#> IQR Relative Root Mean Squared Error\n#> 0.738214\n```\n:::\n\n\n\n* **Log Loss:** Weighted and unweighted Log Loss, with and without normalization. The function can be used as follows,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create factors and response probabilities (qk)\nactual   <- factor(c(\"Class A\", \"Class B\", \"Class A\"))\nweights  <- c(0.3,0.9,1) \nqk <- matrix(cbind(\n    0.2, 0.8,\n    0.8, 0.2,\n    0.7, 0.3\n),nrow = 3, ncol = 2)\n\ncat(\n    \"Unweighted Log Loss:\",\n    SLmetrics::logloss(\n        actual,\n        qk\n    ),\n    \"Weighted log Loss:\",\n    SLmetrics::weighted.logloss(\n        actual   = actual,\n        qk       = qk,\n        w        = weights\n    ),\n    sep = \"\\n\"\n)\n#> Unweighted Log Loss:\n#> 0.7297521\n#> Weighted log Loss:\n#> 0.4668102\n```\n:::\n\n\n\n* **Weighted Receiver Operator Characteristics:** `weighted.ROC()`, the function calculates the weighted True Positive and False Positive Rates for each threshold.\n\n* **Weighted Precision-Recall Curve:** `weighted.prROC()`, the function calculates the weighted Recall and Precsion for each threshold.\n\n## Breaking Changes\n\n* **Weighted Confusion Matix:** The `w`-argument in `cmatrix()` has been removed in favor of the more verbose weighted confusion matrix call `weighted.cmatrix()`-function. See below,\n\nPrior to version `0.3-0` the weighted confusion matrix were a part of the `cmatrix()`-function and were called as follows,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSLmetrics::cmatrix(\n    actual    = actual,\n    predicted = predicted,\n    w         = weights\n)\n```\n:::\n\n\n\nThis solution, although simple, were inconsistent with the remaining implementation of weighted metrics in {SLmetrics}. To regain consistency and simplicity the weighted confusion matrix are now retrieved as follows,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) define factors\nactual    <- factor(sample(letters[1:3], 100, replace = TRUE))\npredicted <- factor(sample(letters[1:3], 100, replace = TRUE))\nweights   <- runif(length(actual))\n\n# 2) without weights\nSLmetrics::cmatrix(\n    actual    = actual,\n    predicted = predicted\n)\n#>    a  b  c\n#> a 12 10 15\n#> b 10 15  8\n#> c  5 14 11\n\n# 2) with weights\nSLmetrics::weighted.cmatrix(\n    actual    = actual,\n    predicted = predicted,\n    w         = weights\n)\n#>          a        b        c\n#> a 3.846279 5.399945 7.226539\n#> b 4.988230 7.617554 4.784221\n#> c 2.959719 5.045980 4.725642\n```\n:::\n\n\n\n## :bug: Bug-fixes\n\n* **Return named vectors:** The classification metrics when `micro == NULL` were not returning named vectors. This has been fixed. \n\n# Version 0.2-0\n\n## Improvements\n\n* **documentation:** The documentation has gotten some extra love, and now all functions have their formulas embedded, the details section have been freed from a general description of [factor] creation. This will make room for future expansions on the various functions where more details are required.\n\n* **weighted classification metrics:** The `cmatrix()`-function now accepts the argument `w` which is the sample weights; if passed the respective method will return the weighted metric. Below is an example using sample weights for the confusion matrix,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) define factors\nactual    <- factor(sample(letters[1:3], 100, replace = TRUE))\npredicted <- factor(sample(letters[1:3], 100, replace = TRUE))\nweights   <- runif(length(actual))\n\n# 2) without weights\nSLmetrics::cmatrix(\n    actual    = actual,\n    predicted = predicted\n)\n#>    a  b  c\n#> a 14  9 14\n#> b 12 15 10\n#> c  6  9 11\n\n# 2) with weights\nSLmetrics::weighted.cmatrix(\n    actual    = actual,\n    predicted = predicted,\n    w         = weights\n)\n#>          a        b        c\n#> a 6.197341 4.717194 6.122321\n#> b 6.244226 7.511618 5.114025\n#> c 2.417569 5.487810 5.760531\n```\n:::\n\n\n\nCalculating weighted metrics manually or by using `foo.cmatrix()`-method,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) weigthed confusion matrix\n# and weighted accuray\nconfusion_matrix <- SLmetrics::cmatrix(\n    actual    = actual,\n    predicted = predicted,\n    w         = weights\n)\n\n# 2) pass into accuracy\n# function\nSLmetrics::accuracy(\n    confusion_matrix\n)\n#> [1] 0.4\n\n# 3) calculate the weighted\n# accuracy manually\nSLmetrics::weighted.accuracy(\n    actual    = actual,\n    predicted = predicted,\n    w         = weights\n)\n#> [1] 0.3927467\n```\n:::\n\n\n\nPlease note, however, that it is not possible to pass `cmatix()`-into `weighted.accurracy()`,\n\n* **Unit-testing:** All functions are now being tested for edge-cases in balanced and imbalanced classifcation problems, and regression problems, individually. This will enable a more robust development process and prevent avoidable bugs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntry(\n    SLmetrics::weighted.accuracy(\n        confusion_matrix\n    )\n)\n#> Error in UseMethod(generic = \"weighted.accuracy\", object = ..1) : \n#>   no applicable method for 'weighted.accuracy' applied to an object of class \"cmatrix\"\n```\n:::\n\n\n\n## :bug: Bug-fixes\n\n* **Floating precision:** Metrics would give different results based on the method used. This means that `foo.cmatrix()` and `foo.factor()` would produce different results (See Issue https://github.com/serkor1/SLmetrics/issues/16). This has been fixed by using higher precision `Rcpp::NumericMatrix` instead of `Rcpp::IntegerMatrix`.\n\n* **Miscalculation of Confusion Matrix elements:** An error in how `FN`, `TN`, `FP` and `TP` were calculated have been fixed. No issue has been raised for this bug. This was not something that was caught by the unit-tests, as the total samples were too high to spot this error. It has, however, been fixed now. This means that all metrics that uses these explicitly are now stable, and produces the desired output.\n\n* **Calculation Error in Fowlks Mallows Index:** A bug in the calculation of the `fmi()`-function has been fixed. The `fmi()`-function now correctly calculates the measure.\n\n* **Calculation Error in Pinball Deviance and Concordance Correlation Coefficient:** See issue https://github.com/serkor1/SLmetrics/issues/19. Switched to unbiased variance calculation in `ccc()`-function. The `pinball()`-function were missing a weighted quantile function. The issue is now fixed.\n\n* **Calculation Error in Balanced Accuracy:** See issue https://github.com/serkor1/SLmetrics/issues/24. The function now correctly adjusts for random chance, and the result matches that of {scikit-learn}\n\n* **Calculation Error in F-beta Score:** See issue https://github.com/serkor1/SLmetrics/issues/23. The function werent respecting `na.rm` and `micro`, this has been fixed accordingly.\n\n* **Calculation Error in Relative Absolute Error:** The function was incorrectly calculating means, instead of sums. This has been fixed.\n\n## Breaking changes\n\n* All regression metrics have had `na.rm`- and `w`-arguments removed. All  weighted regression metrics have a seperate function on the `weighted.foo()` to increase consistency across all metrics. See example below,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) define regression problem\nactual    <- rnorm(n = 1e3)\npredicted <- actual + rnorm(n = 1e3)\nw         <- runif(n = 1e3)\n\n# 2) unweighted metrics\nSLmetrics::rmse(actual, predicted)\n#> [1] 0.9989386\n\n# 3) weighted metrics\nSLmetrics::weighted.rmse(actual, predicted, w = w)\n#> [1] 1.013139\n```\n:::\n\n\n\n* The `rrmse()`-function have been removed in favor of the `rrse()`-function. This function was incorrectly specified and described in the package.\n\n# Version 0.1-1\n\n## General\n\n* **Backend changes:** All pair-wise metrics arer moved from {Rcpp} to C++, this have reduced execution time by half. All pair-wise metrics are now faster.\n\n## Improvements\n\n* **NA-controls:** All pair-wise metrics that doesn't have a `micro`-argument were handling missing values as according to C++ and {Rcpp} internals. See [Issue](https://github.com/serkor1/SLmetrics/issues/8). Thank you @EmilHvitfeldt for pointing this out. This has now been fixed so functions uses an `na.rm`-argument to explicitly control for this. See below,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) define factors\nactual    <- factor(c(\"no\", \"yes\"))\npredicted <- factor(c(NA, \"no\"))\n\n# 2) accuracy with na.rm = TRUE\nSLmetrics::accuracy(\n    actual    = actual,\n    predicted = predicted,\n    na.rm     = TRUE\n)\n\n# 2) accuracy with na.rm = FALSE\nSLmetrics::accuracy(\n    actual    = actual,\n    predicted = predicted,\n    na.rm     = FALSE\n)\n```\n:::\n\n\n\n## :bug: Bug-fixes\n\n* The `plot.prROC()`- and `plot.ROC()`-functions now adds a line to the plot when `panels = FALSE`. See Issue https://github.com/serkor1/SLmetrics/issues/9.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) define actual\n# classes\nactual <- factor(\n  sample(letters[1:2], size = 100, replace = TRUE)\n)\n\n# 2) define response\n# probabilities\nresponse <- runif(100)\n\n# 3) calculate\n# ROC and prROC\n\n# 3.1) ROC\nroc <- SLmetrics::ROC(\n    actual,\n    response\n)\n\n# 3.2) prROC\nprroc <- SLmetrics::prROC(\n    actual,\n    response\n)\n\n# 4) plot with panels\n# FALSE\npar(mfrow = c(1,2))\nplot(\n  roc,\n  panels = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](NEWS_files/figure-commonmark/unnamed-chunk-12-1.png){width=100%}\n:::\n\n```{.r .cell-code}\n\nplot(\n    prroc,\n    panels = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](NEWS_files/figure-commonmark/unnamed-chunk-12-2.png){width=100%}\n:::\n:::\n\n\n\n\n# Version 0.1-0\n\n## General\n\n* {SLmetrics} is a collection of Machine Learning performance evaluation functions for supervised learning. Visit the online documentation on [GitHub Pages](https://serkor1.github.io/SLmetrics/).\n\n## Examples\n\n### Supervised classification metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) actual classes\nprint(\n    actual <- factor(\n        sample(letters[1:3], size = 10, replace = TRUE)\n    )\n)\n#>  [1] a b a c b a a a c b\n#> Levels: a b c\n\n# 2) predicted classes\nprint(\n    predicted <- factor(\n        sample(letters[1:3], size = 10, replace = TRUE)\n    )\n)\n#>  [1] b a c c c c c c a a\n#> Levels: a b c\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) calculate confusion\n# matrix and summarise\n# it\nsummary(\n    confusion_matrix <- SLmetrics::cmatrix(\n        actual    = actual,\n        predicted = predicted\n    )\n)\n#> Confusion Matrix (3 x 3) \n#> ================================================================================\n#>   a b c\n#> a 0 1 4\n#> b 2 0 1\n#> c 1 0 1\n#> ================================================================================\n#> Overall Statistics (micro average)\n#>  - Accuracy:          0.10\n#>  - Balanced Accuracy: 0.17\n#>  - Sensitivity:       0.10\n#>  - Specificity:       0.55\n#>  - Precision:         0.10\n\n# 2) calculate false positive\n# rate using micro average\nSLmetrics::fpr(\n    confusion_matrix\n)\n#>         a         b         c \n#> 0.6000000 0.1428571 0.6250000\n```\n:::\n\n\n\n\n### Supervised regression metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) actual values\nactual <- rnorm(n = 100)\n\n# 2) predicted values\npredicted <- actual + rnorm(n = 100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) calculate\n# huber loss\nSLmetrics::huberloss(\n    actual    = actual,\n    predicted = predicted\n)\n#> [1] 0.4389594\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}