{
  "hash": "47674ab87ce8b28dc362913a1234900c",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: \n    gfm:\n        default-image-extension: \"\"\nalways_allow_html: true\nexecute: \n  cache: true\n  freeze: auto\n  dir: meta\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\" \n    dpi: 1280\n    fig.height: 6\n    out.width: 100%\n---\n\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n\n\n\n\n# {SLmetrics}: Machine learning performance evaluation on steroids <img src=\"man/figures/logo.png\" align=\"right\" height=\"150\" alt=\"\" />\n\n<!-- badges: start -->\n[![CRAN status](https://www.r-pkg.org/badges/version/SLmetrics)](https://CRAN.R-project.org/package=SLmetrics)\n[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/last-month/SLmetrics?color=blue)](https://r-pkg.org/pkg/SLmetrics)\n[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)\n[![R-CMD-check](https://github.com/serkor1/SLmetrics/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/serkor1/SLmetrics/actions/workflows/R-CMD-check.yaml)\n[![R-hub](https://github.com/serkor1/SLmetrics/actions/workflows/rhub.yaml/badge.svg)](https://github.com/serkor1/SLmetrics/actions/workflows/rhub.yaml)\n[![codecov](https://codecov.io/gh/serkor1/SLmetrics/branch/development/graph/badge.svg?token=X2osJDSRlN)](https://app.codecov.io/gh/serkor1/SLmetrics)\n[![CodeFactor](https://www.codefactor.io/repository/github/serkor1/slmetrics/badge)](https://www.codefactor.io/repository/github/serkor1/slmetrics)\n<!-- badges: end -->\n\n[{SLmetrics}](https://serkor1.github.io/SLmetrics/) is a lightweight `R` package written in `C++` and  [{Rcpp}](https://github.com/RcppCore/Rcpp) for *memory-efficient* and *lightning-fast* machine learning performance evaluation; it's like using a supercharged [{yardstick}](https://github.com/tidymodels/yardstick) but without the risk of soft to super-hard deprecations. [{SLmetrics}](https://serkor1.github.io/SLmetrics/) covers both regression and classification metrics and provides (almost) the same array of metrics as [{scikit-learn}](https://github.com/scikit-learn/scikit-learn) and [{PyTorch}](https://github.com/pytorch/pytorch) all without [{reticulate}](https://github.com/rstudio/reticulate) and the Python compile-run-(crash)-debug cylce.\n\nDepending on the mood and alignment of planets [{SLmetrics}](https://serkor1.github.io/SLmetrics/) stands for Supervised Learning metrics, or Statistical Learning metrics. If [{SLmetrics}](https://serkor1.github.io/SLmetrics/) catches on, the latter will be the core philosophy and include unsupervised learning metrics. If not, then it will remain a {pkg} for Supervised Learning metrics, and a sandbox for me to develop my `C++` skills.\n\n## :books: Table of Contents\n\n* [:rocket: Gettting Started](#rocket-gettting-started)\n  + [:shield: Installation](#shield-installation)\n  + [:books: Basic Usage](#books-basic-usage)\n* [:information_source: Why?](#information_source-why)\n* [:zap: Performance Comparison](#zap-performance-comparison)\n  + [:fast_forward: Speed comparison](#fast_forward-speed-comparison)\n  + [:floppy_disk: Memory-efficiency](#floppy_disk-memory-efficiency)\n* [:information_source: Basic usage](#information_source-basic-usage)\n  + [:books: Regression](#books-regression)\n  + [:books: Classification](#books-classification)\n* [:information_source: Enable OpenMP](#information_source-enable-openmp)\n  + [:books: Entropy without OpenMP](#books-entropy-without-openmp)\n  + [:books: Entropy with OpenMP](#books-entropy-with-openmp)\n* [:information_source: Installation](#information_source-installation)\n  + [:shield: Stable version](#shield-stable-version)\n  + [:hammer_and_wrench: Development version](#hammer_and_wrench-development-version)\n* [:information_source: Code of Conduct](#information_source-code-of-conduct)\n\n\n## :rocket: Gettting Started\n\nBelow you’ll find instructions to install [{SLmetrics}](https://serkor1.github.io/SLmetrics/) and get started with your first metric, the Root Mean Squared Error (RMSE).\n\n### :shield: Installation \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## install stable release\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics@*release',\n  ref  = 'main'\n)\n```\n:::\n\n\n\n### :books: Basic Usage\n\nBelow is a minimal example demonstrating how to compute both unweighted and weighted RMSE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SLmetrics)\n\nactual    <- c(10.2, 12.5, 14.1)\npredicted <- c(9.8, 11.5, 14.2)\nweights   <- c(0.2, 0.5, 0.3)\n\ncat(\n  \"Root Mean Squared Error\", rmse(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Root Mean Squared Error (weighted)\", weighted.rmse(\n    actual    = actual,\n    predicted = predicted,\n    w         = weights\n  ),\n  sep = \"\\n\"\n)\n#> Root Mean Squared Error\n#> 0.6244998\n#> Root Mean Squared Error (weighted)\n#> 0.7314369\n```\n:::\n\n\n\nThat’s all! Now you can explore the rest of this README for in-depth usage, performance comparisons, and more details about [{SLmetrics}](https://serkor1.github.io/SLmetrics/).\n\n## :information_source: Why?\n\nMachine learning can be a complicated task; the steps from feature engineering to model deployment require carefully measured actions and decisions. One low-hanging fruit to simplify this process is *performance evaluation*. \n\nAt its core, performance evaluation is essentially just comparing two vectors — a programmatically and, at times, mathematically trivial step in the machine learning pipeline, but one that can become complicated due to:\n\n1. Dependencies and potential deprecations\n2. Needlessly complex or repetitive arguments  \n3. Performance and memory bottlenecks at scale  \n\n[{SLmetrics}](https://serkor1.github.io/SLmetrics/) solves these issues by being:\n\n1. **Fast:** Powered by `C++` and [{Rcpp}](https://github.com/RcppCore/Rcpp)  \n2. **Memory-efficient:** Everything is structured around pointers and references\n3. **Lightweight:** Only depends on [{Rcpp}](https://github.com/RcppCore/Rcpp), [{RcppEigen}](https://github.com/RcppCore/RcppEigen), and [{lattice}](https://github.com/deepayan/lattice)\n4. **Simple:** S3-based, minimal overhead, and flexible inputs\n\nPerformance evaluation should be plug-and-play and “just work” out of the box — there’s no need to worry about *quasiquations*, *dependencies*, *deprecations*, or variations of the same functions relative to their arguments when using [{SLmetrics}](https://serkor1.github.io/SLmetrics/).\n\n## :zap: Performance Comparison\n\nOne, obviously, can't build an `R`-package on `C++` and [{Rcpp}](https://github.com/RcppCore/Rcpp) without a proper pissing contest at the urinals - below is a comparison in execution time and memory efficiency of two simple cases  that any {pkg} should be able to handle gracefully; computing a 2 x 2 confusion matrix and computing the RMSE[^1].\n\n### :fast_forward: Speed comparison\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](README_files/figure-commonmark/unnamed-chunk-4-1.png){width=100%}\n:::\n:::\n\n\n\nAs shown in the chart, [{SLmetrics}](https://serkor1.github.io/SLmetrics/) maintains consistently low(er) execution times across different sample sizes.\n\n### :floppy_disk: Memory-efficiency\n\nBelow are the results for garbage collections and total memory allocations when computing a 2×2 confusion matrix (N = 1e7) and RMSE (N = 1e7) [^2]. Notice that [{SLmetrics}](https://serkor1.github.io/SLmetrics/) requires no GC calls for these operations.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: 2 x 2 Confusion Matrix (N = 1e7)\n\n|               | Iterations| Garbage Collections [gc()]| gc() pr. second| Memory Allocation (MB)|\n|:--------------|----------:|--------------------------:|---------------:|----------------------:|\n|{SLmetrics}    |        100|                          0|            0.00|                      0|\n|{yardstick}    |        100|                        190|            4.44|                    381|\n|{MLmetrics}    |        100|                        186|            4.50|                    381|\n|{mlr3measures} |        100|                        371|            3.93|                    916|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: RMSE (N = 1e7)\n\n|               | Iterations| Garbage Collections [gc()]| gc() pr. second| Memory Allocation (MB)|\n|:--------------|----------:|--------------------------:|---------------:|----------------------:|\n|{SLmetrics}    |        100|                          0|            0.00|                      0|\n|{yardstick}    |        100|                        149|            4.30|                    420|\n|{MLmetrics}    |        100|                         15|            2.00|                     76|\n|{mlr3measures} |        100|                         12|            1.29|                     76|\n\n\n:::\n:::\n\n\n\nIn both tasks, [{SLmetrics}](https://serkor1.github.io/SLmetrics/) remains extremely memory-efficient, even at large sample sizes.\n\n> [!IMPORTANT]  \n>\n> From [{bench}](https://github.com/r-lib/bench) documentation: *Total amount of memory allocated by R while running the expression. Memory allocated outside the R heap, e.g. by `malloc()` or new directly is not tracked, take care to avoid misinterpreting the results if running code that may do this.*\n\n## :information_source: Basic usage\n\nIn its simplest form, [{SLmetrics}](https://serkor1.github.io/SLmetrics/)-functions work directly with pairs of `<numeric>` vectors (for regression) or `<factor>` vectors (for classification). Below we demonstrate this on two well-known datasets, `mtcars` (regression) and `iris` (classification).\n\n### :books: Regression\n\nWe first fit a linear model to predict `mpg` in the `mtcars` dataset, then compute the in-sample RMSE:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Evaluate a linear model on mpg (mtcars)\nmodel <- lm(mpg ~ ., data = mtcars)\nrmse(mtcars$mpg, fitted(model))\n#> [1] 2.146905\n```\n:::\n\n\n\n### :books: Classification\n\nNow we recode the `iris` dataset into a binary problem (\"virginica\" vs. \"others\") and fit a logistic regression. Then we generate predicted classes, compute the confusion matrix and summarize it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) recode iris\n# to binary problem\niris$species_num <- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel <- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted <- factor(\n  as.numeric(\n    predict(model, type = \"response\") > 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) generate actual\n# values as factor\nactual <- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4) generate\n# confusion matrix\nsummary(\n  confusion_matrix <-  cmatrix(\n    actual    = actual,\n    predicted = predicted\n  )\n)\n#> Confusion Matrix (2 x 2) \n#> ================================================================================\n#>           Virginica Others\n#> Virginica        35     15\n#> Others           14     86\n#> ================================================================================\n#> Overall Statistics (micro average)\n#>  - Accuracy:          0.81\n#>  - Balanced Accuracy: 0.78\n#>  - Sensitivity:       0.81\n#>  - Specificity:       0.81\n#>  - Precision:         0.81\n```\n:::\n\n\n\n## :information_source: Enable OpenMP\n\n\n::: {.cell}\n\n:::\n\n\n\n> [!IMPORTANT]  \n>\n> OpenMP support in [{SLmetrics}](https://serkor1.github.io/SLmetrics/) is experimental. Use it with caution, as performance gains and stability may vary based on your system configuration and workload.\n\nYou can control OpenMP usage within [{SLmetrics}](https://serkor1.github.io/SLmetrics/) using the setUseOpenMP function. Below are examples demonstrating how to enable and disable OpenMP:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# enable OpenMP\nSLmetrics::openmp.on()\n#> OpenMP enabled!\n\n# disable OpenMP\nSLmetrics::openmp.off()\n#> OpenMP disabled!\n```\n:::\n\n\n\nTo illustrate the impact of OpenMP on performance, consider the following benchmarks for calculating entropy on a 1,000,000 x 200 matrix over 100 iterations[^3].\n\n### :books: Entropy without OpenMP\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: 1e6 x 200 matrix without OpenMP\n\n| Iterations| Runtime (sec)| Garbage Collections [gc()]| gc() pr. second| Memory Allocation (MB)|\n|----------:|-------------:|--------------------------:|---------------:|----------------------:|\n|        100|          0.86|                          0|               0|                      0|\n\n\n:::\n:::\n\n\n\n### :books: Entropy with OpenMP\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: 1e6 x 200 matrix with OpenMP\n\n| Iterations| Runtime (sec)| Garbage Collections [gc()]| gc() pr. second| Memory Allocation (MB)|\n|----------:|-------------:|--------------------------:|---------------:|----------------------:|\n|        100|          0.15|                          0|               0|                      0|\n\n\n:::\n:::\n\n\n\n### :shield: Stable version\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## install stable release\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics@*release',\n  ref  = 'main'\n)\n```\n:::\n\n\n\n### :hammer_and_wrench: Development version\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## install development version\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics',\n  ref  = 'development'\n)\n```\n:::\n\n\n\n## :information_source: Code of Conduct\n\nPlease note that the [{SLmetrics}](https://serkor1.github.io/SLmetrics/) project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/1/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.\n\n[^1]: The source code is available [here](https://github.com/serkor1/SLmetrics/blob/d9b6cdbc1fccbdb0d45364b0fc37ebe953df30b9/data-raw/classification_performance.R) and [here](https://github.com/serkor1/SLmetrics/blob/d9b6cdbc1fccbdb0d45364b0fc37ebe953df30b9/data-raw/regression_performance.R).\n[^2]: The source code is available [here](https://github.com/serkor1/SLmetrics/blob/d9b6cdbc1fccbdb0d45364b0fc37ebe953df30b9/data-raw/memory_performance.R).\n[^3]: The source code is available [here](https://github.com/serkor1/SLmetrics/blob/d9b6cdbc1fccbdb0d45364b0fc37ebe953df30b9/data-raw/OpenMP_perfomance.R).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}