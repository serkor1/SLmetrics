---
title: "{SLmetrics}: Classification"
subtitle: "{SLmetrics} x {lightgbm} in classification problems"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{{SLmetrics}: Classification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  message  = FALSE
)
```

```{r setup}
# load libraries
library(SLmetrics)
```


In this vignette, we will demonstrate how to use [{SLmetrics}](https://github.com/serkor1/SLmetrics) with [{lightgbm}](https://github.com/microsoft/LightGBM) for classification problems. 

> **Prerequisite:** Familiarity with [{lightgbm}](https://github.com/microsoft/LightGBM)  and the general AI/ML-workflow is assumed.

We will use the [Glass](https://archive.ics.uci.edu/dataset/42/glass+identification) dataset, made available via [{mlbench}](https://cran.r-project.org/web/packages/mlbench/index.html), as an example throughout this vignette.

## Setup

In this section we setup the essential the workflow for using [{SLmetrics}](https://github.com/serkor1/SLmetrics) with [{lightgbm}](https://github.com/microsoft/LightGBM).

### The data

```{r data}
# 1) load data
# from {mlbench}
data("Glass", package = "mlbench")
```

<details>
<summary> Showcase: speed comparison </summary>

```{r}
# 1.1) define the features
# and outcomes
outcome  <- c("Type")
features <- setdiff(x = colnames(Glass), y = outcome)

# 2) split data in training
# and test

# 2.1) set seed for 
# for reproducibility
set.seed(1903)

# 2.2) exttract
# indices with a simple
# 80/10 split
index <- sample(1:nrow(Glass), size = 0.8 * nrow(Glass))

# 1.1) extract training
# data and construct
# as lgb.Dataset
train <- Glass[index,]
dtrain <- lightgbm::lgb.Dataset(
    data  = data.matrix(train[,features]),
    label = train$Type
)
# 1.2) extract test
# data
test <- Glass[-index,]


# 1.2.1) extract actual
# values and constuct
# as.factor for {SLmetrics}
# methods
actual <- as.factor(
    test$Type
)

# 1.2.2) construct as data.matrix
# for predict method
test <- data.matrix(
    test[,features]
)
```

</details>


### Setting up parameters
```{r parameters}
# 1) define parameters
# across the vignette
parameters <- list(
    objective     = "multiclass",
    num_leaves    = 4L,
    learning_rate = 0.5,
    num_class     = 8
)
```

### Evaluation function: F score

As the custom evaluation function we use the $F_{score}$ with $\beta = 2$ to emphasize that precision is more imporant than recall. The function is defined as,

$$
f_\beta = (1 + \beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}
$$

The `fbeta()`-function returns a vector of scores for each class. We want to maximize the micro-average.

```{r}
# 1) define the custom
# evaluation metric
evaluation_metric <- function(
    dtrain, 
    preds) {

        # 1) extract values
        actual    <- as.factor(dtrain)
        predicted <- lightgbm::get_field(preds, "label")
        value     <- fbeta(
            actual    = actual,
            predicted = predicted,
            beta      = 2,
            micro     = TRUE
        )

        # 2) construnct output
        # list
        list(
            name          = "fbeta",
            value         = value,
            higher_better = TRUE 
        )
    
}
```

## Training model

We train the the model using the `lgb.train()`-function,

```{r}
model <- lightgbm::lgb.train(
    params  = parameters,
    data    = dtrain,
    nrounds = 10L,
    eval    = evaluation_metric,
    verbose = -1
)
```

## Performance Evaluation

### Classification

We extract the predicted classes using the `predict()`-function,

```{r forecasts}
# 1) prediction
# from the model
predicted <- as.factor(
    predict(
        model,
        newdata = test,
        type = "class"
    )
)
```

```{r cmatrix}
# 1) construct confusion
# matrix
confusion_matrix <- cmatrix(
    actual = actual,
    predicted = predicted
)

# 2) visualize
plot(
    confusion_matrix
)

# 3) summarize
summary(
    confusion_matrix
)
```


### Response

We extract the response values using the `predict()`-function,

```{r response}
# 1) prediction
# from the model
response <- predict(
        model,
        newdata = test
    )

```

The `response` can be passed into the `ROC()`-function,

```{r}
# 1) calculate the reciever
# operator characteristics
roc <- ROC(
    actual   = actual,
    response = response
)

# 2) print the roc
# object
print(roc)
```

The `ROC()`-function returns a `data.frame`-object, with `r nrow(roc)` rows corresponding to the `length` of `response` multiplied with number of classes in the data. The `roc`-object can be plotted as follows,

```{r}
# 1) plot roc
# object
plot(roc)
```


The `ROC()`-function accepts a custom `threshold`-argument, which can be passed as follows,

```{r custom thresholds}
# 1) create custom
# thresholds
thresholds <- seq(
    from = min(roc$threshold),
    to   = max(roc$threshold),
    length.out = 10
)

# 2) pass the custom thresholds
# to the ROC()-function
roc <- ROC(
    actual     = actual,
    response   = response,
    thresholds = thresholds 
)

# 3) print the roc
# object
print(roc)
```

The new object has `r nrow(roc)` rows. 

```{r ROC}
# 1) viasualize
# ROC
plot(roc)
```

```{r summary of ROC}
# 1) summarise ROC
summary(roc)
```