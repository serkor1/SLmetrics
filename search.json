[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{SLmetrics}: Machine Learning Performance Evaluation on Steroids",
    "section": "",
    "text": "Preface\n{SLmetrics} began as a small personal project to learn C++. Over time, as the scope expanded and the number of commits grew, the name stuck, and releasing a fully functioning data science R package became the natural next step.\nThe primary goal of {SLmetrics} is to be a fast, memory efficient and reliable low-level successor to {MLmetrics}. Current benchmarks in 3  Benchmarking suggets that this goal has, in fact, been achieved. Implementations in {SLmetrics} draws inspirations from well-known libraries such as scikit-learn (Pedregosa et al. (2011)), SciPy (Virtanen et al. (2020)) and PyTorch (Ansel et al. (2024)).\nBy leveraging C++ under the hood, {SLmetrics} aims to minimize overhead and provide a robust suite of metrics. In the future, we plan to expand its coverage to additional metrics and reinforce integration with established R workflows.\n\n\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. “PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.” In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24). ACM. https://doi.org/10.1145/3620665.3640366.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.” Nature Methods 17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The status-quo of {pkgs}\nThere are currently three {pkgs} that are developed with machine leaning performance evaluation in mind: {MLmetrics}, {yardstick}, {mlr3measures}. These {pkgs} have historically bridged the gap between R and Python in terms of machine learning and data science.\n{MLmetrics} can be considered the legacy code when it comes to performance evaluation, and it served as a backend in {yardstick} up to version 0.0.2. It is built entirely on base R, and has been stable since its inception almost 10 years ago.\nHowever, it appears that the development has reached it’s peak and is currently stale - see, for example, this stale PR related to this issue. Micro- and macro-averages have been implented in {scikit-learn} for many years, and {MLmetrics} simply didn’t keep up with the development.\n{yardstick}, on the other hand, carried the torch forward and implemented these modern features. {yardstick} closely follows the syntax, naming and functionality of {scikit-learn} but is built with {tidyverse} tools; although the source code is nice to look at, it does introduce some serious overhead and carries the risk of deprecations.\nFurthermore, it complicates a simple application by its verbose function naming, see for example metric()-function for &lt;tbl&gt; and metric_vec()-function for &lt;numeric&gt; - the output is the same, but the call is different. {yardstick} can’t handle more than one positive class at a time, so the end-user is forced to run the same function more than once to get performance metrics for the adjacent classes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-slmetrics",
    "href": "intro.html#why-slmetrics",
    "title": "1  Introduction",
    "section": "1.2 Why {SLmetrics}?",
    "text": "1.2 Why {SLmetrics}?\nAs the name suggests, {SLmetrics} closely resembles {MLmetrics} in it’s simplistic and low-level implementation of machine learning metrics. The resemblance ends there, however. {SLmetrics} are developed with three things in mind: speed, efficiency and scalability. And therefore addresses the shortcomings of the status-quo by construction - the {pkg} is built on C++ and {Rcpp} from the ground up. Furthermore, {SLmetrics} has a larger, and up-to-date, suite of performance metrics than {MLmetrics}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#key-takeaways",
    "href": "intro.html#key-takeaways",
    "title": "1  Introduction",
    "section": "1.3 Key takeaways",
    "text": "1.3 Key takeaways\nThe existing {pkgs} are outdated, inefficient and insufficient for modern large-scale machine learning applications. {SLmetrics} is, in the worst-case scenario, on par with low-level R implementations of equivalent metrics and is a multitude more memory-efficient than any of the {pkgs}. A detailed benchmark can be found in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "2.1 Basic Usage\nComputing performance metrics with {SLmetrics} is straightforward. At its most basic level, {SLmetrics} accepts vectors of &lt;factor&gt; or &lt;numeric&gt; values as input. Here are two examples demonstrating its basic usage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html#basic-usage",
    "href": "summary.html#basic-usage",
    "title": "2  Summary",
    "section": "",
    "text": "2.1.1 Classification metrics\n\n## 1) actual and predicted\n## classes\nfct_actual &lt;- factor(c(\"A\", \"A\", \"B\"))\nfct_predicted &lt;- factor(c(\"B\", \"A\", \"B\"))\n\n## 2) confusion matrix\nconfusion_matrix &lt;- SLmetrics::cmatrix(\n    actual    = fct_actual, \n    predicted = fct_predicted\n)\n\n## 3) summarize confusion\n## matrix\nsummary(confusion_matrix)\n\nConfusion Matrix (2 x 2) \n================================================================================\n  A B\nA 1 1\nB 0 1\n================================================================================\nOverall Statistics (micro average)\n - Accuracy:          0.67\n - Balanced Accuracy: 0.75\n - Sensitivity:       0.67\n - Specificity:       0.67\n - Precision:         0.67\n\n\n\n\n2.1.2 Regression metrics\n\n## 1) actual and predicted\n## values\nnum_actual    &lt;- c(10.2, 12.5, 14.1)\nnum_predicted &lt;- c(9.8, 11.5, 14.2)\n\n## 2) RMSE\nSLmetrics::rmse(\n    actual    = num_actual,\n    predicted = num_predicted\n)\n\n[1] 0.6244998",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html#installation",
    "href": "summary.html#installation",
    "title": "2  Summary",
    "section": "2.2 Installation",
    "text": "2.2 Installation\n\n2.2.1 Stable version\n\n## install stable release\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics@*release',\n  ref  = 'main'\n)\n\n\n\n2.2.2 Development version\n\n## install development version\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics',\n  ref  = 'development'\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "3  Benchmarking",
    "section": "",
    "text": "3.1 The setup\nIn this section a detailed benchmark of {SLmetrics} is conducted. The benchmarks will be conducted on randomly selected functions, and then compared to {pkg} discussed in Chapter 1. The benchmarks are conducted on three parameters: median execution time, memory usage and gc() calls.\nThis section strucutred as follows, Section 3.1 sets up the infrastructure needed to conduct the benchmark in an unbiased way, in Section 3.2 the benchmarks are conducted and discussed and summarized in Section 3.3 and Section 3.4 respectively.\nTo conduct the benchmarking two functions are defined. create_regression() and create_factor(), both functions returns a vector of actual and predicted values with a length of 10,000,000 rows.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-setup",
    "href": "benchmark.html#sec-bench-setup",
    "title": "3  Benchmarking",
    "section": "",
    "text": "3.1.1 Regression problems\nThe benchmarks on regression metrics is conducted on correlated absolute value &lt;numeric&gt;-vectors, with uniformly distributed weights. create_regression() returns a named list, and is defined below:\n\n# regression function\ncreate_regression &lt;- function(\n    n = 1e7) {\n\n  # 1) actual\n  # values\n  actual &lt;- abs(rnorm(n = n))\n\n  # 2) predicted\n  # values\n  predicted &lt;- actual + abs(rnorm(n = n))\n\n  # 3) generate\n  # weights\n  w &lt;- runif(n)\n\n  list(\n    actual    = actual,\n    predicted = predicted,\n    w         = w\n  )\n}\n\n\n\n3.1.2 Classification problems\nThe benchmarks on classification metrics is conducted on the randomly sampled letters c(\"a\", \"b\", \"c\"). create_regression() returns a vector of &lt;factor&gt;, and is defined below:\n\n# classification function\ncreate_factor &lt;- function(\n    k = 3,\n    balanced = TRUE,\n    n = 1e7) {\n\n  probs &lt;- NULL\n\n  if (!balanced) {\n\n    probs &lt;- rbeta(\n      n = k,\n      shape1 = 10,\n      shape2 = 2\n    )\n\n    probs[which.min(probs)] &lt;- 0\n\n    probs &lt;- probs / sum(probs)\n\n  }\n\n  factor(\n    x = sample(\n      1:k,\n      size = n,\n      replace = TRUE,\n      prob = probs\n    ),\n    labels = letters[1:k],\n    levels = 1:k\n  )\n}\n\n\n\n3.1.3 Staging the testing ground\nThe vectors used in the benchmarks are created with the seed 1903 for reproducibility, see below:\n\n# 1) set seed for reproducibility\nset.seed(1903)\n\n# 2) create classification\n# problem\nfct_actual &lt;- create_factor()\nfct_predicted &lt;- create_factor()\n\n# 3) create regression\n# problem\n\n# 3.1) store results\n# in regression\nlst_regression &lt;- create_regression()\n\n# 3.2) assign the values\n# accordingly\nnum_actual &lt;- lst_regression$actual\nnum_predicted &lt;- lst_regression$predicted\nnum_weights &lt;- lst_regression$w",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-mark",
    "href": "benchmark.html#sec-bench-mark",
    "title": "3  Benchmarking",
    "section": "3.2 Benchmarking",
    "text": "3.2 Benchmarking\nTo conduct the benchmark {bench} is used. Before the benchmarks are conducted, a benchmark()-wrapper is created.\nThis wrapper conducts m (Default: 10) benchmarks, with 10 iterations for each benchmarked function passed into benchmark() - to allow for warm-up the first iteration is discarded. The wrapper is defined as follows:\n\nbenchmark &lt;- function(\n  ..., \n  m = 10) {\n  library(magrittr)\n  # 1) create list\n  # for storing values\n  performance &lt;- list()\n  \n  for (i in 1:m) {\n\n     # 1) run the benchmarks\n    results &lt;- bench::mark(\n      ...,\n      iterations = 10,\n      check = FALSE\n    )\n\n    # 2) extract values\n    # and calculate medians\n    performance$time[[i]]  &lt;- setNames(lapply(results$time, mean), results$expression)\n    performance$memory[[i]] &lt;- setNames(lapply(results$memory, function(x) { sum(x$bytes, na.rm = TRUE)}), results$expression)\n    performance$n_gc[[i]] &lt;- setNames(lapply(results$n_gc, sum), results$expression)\n\n  }\n\n  purrr::pmap_dfr(\n  list(performance$time, performance$memory, performance$n_gc), \n  ~{\n    tibble::tibble(\n      expression = names(..1),\n      time = unlist(..1),\n      memory = unlist(..2),\n      n_gc = unlist(..3)\n    )\n  }\n) %&gt;%\n  dplyr::mutate(expression = factor(expression, levels = unique(expression))) %&gt;%\n  dplyr::group_by(expression) %&gt;%\n  dplyr::filter(dplyr::row_number() &gt; 1) %&gt;%\n  dplyr::summarize(\n    execution_time = bench::as_bench_time(median(time)),\n    memory_usage = bench::as_bench_bytes(median(memory)),\n    gc_calls = median(n_gc),\n    .groups = \"drop\"\n  )\n\n}\n\n\n3.2.1 Regression metrics\n\nCode\nbenchmark(\n    `{RMSE}`  = SLmetrics::rmse(num_actual, num_predicted),\n    `{Pinball Loss}` = SLmetrics::pinball(num_actual, num_predicted),\n    `{Huber Loss}` = SLmetrics::huberloss(num_actual, num_predicted)\n)\n\n\n\n\nTable 3.1: Benchmarks of Root Mean Squared Error (RMSE), Pinball Loss and Huber Loss. Each benchmark is run 10 times with two input vectors of 10 million elements.\n\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {RMSE}                 9.57ms           0B        0\n#&gt; 2 {Pinball Loss}         9.61ms           0B        0\n#&gt; 3 {Huber Loss}          55.91ms           0B        0\n\n\n\n\n\nCode\nbenchmark(\n    `{SLmetrics}` = SLmetrics::rmse(num_actual, num_predicted),\n    `{MLmetrics}` = MLmetrics::RMSE(num_actual, num_predicted),\n    `{yardstick}` = yardstick::rmse_vec(num_actual, num_predicted),\n    `{mlr3measures}` = mlr3measures::rmse(num_actual, num_predicted)\n)\n\n\n\n\nTable 3.2: Benchmarking Root Mean Squared Error (RMSE) across {pkgs}. Each benchmark is run 10 times with two input vectors of 10 million elements.\n\n\n\n#&gt; # A tibble: 4 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {SLmetrics}            9.58ms           0B        0\n#&gt; 2 {MLmetrics}           59.82ms       76.3MB        1\n#&gt; 3 {yardstick}          164.15ms      419.6MB        9\n#&gt; 4 {mlr3measures}        83.39ms       76.3MB        1\n\n\n\n\n\n\n3.2.2 Classification metrics\n\nCode\nbenchmark(\n    `{Confusion Matrix}`  = SLmetrics::cmatrix(fct_actual, fct_predicted),\n    `{Accuracy}` = SLmetrics::accuracy(fct_actual, fct_predicted),\n    `{F-beta}` = SLmetrics::fbeta(fct_actual, fct_predicted)\n)\n\n\n\n\nTable 3.3: Benchmarks of the Confusion Matrix, Accuracy, and F-Beta score. Each benchmark is run 10 times with two input vectors of 10 million elements.\n\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression         execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                    &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {Confusion Matrix}         8.53ms           0B        0\n#&gt; 2 {Accuracy}                 8.53ms           0B        0\n#&gt; 3 {F-beta}                   8.54ms           0B        0\n\n\n\n\n\nCode\nbenchmark(\n    `{SLmetrics}`    = SLmetrics::cmatrix(fct_actual, fct_predicted),\n    `{MLmetrics}`    = MLmetrics::ConfusionMatrix(fct_predicted, fct_actual),\n    `{yardstick}`    = yardstick::conf_mat(table(fct_actual, fct_predicted))\n)\n\n\n\n\nTable 3.4: Benchmarking a 3×3 confusion matrix across {pkgs}. Each benchmark is run 10 times with two input vectors of 10 million elements and three classes.\n\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression  execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;             &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {SLmetrics}         8.53ms           0B        0\n#&gt; 2 {MLmetrics}       245.89ms        381MB        7\n#&gt; 3 {yardstick}       246.51ms        381MB        7",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-discussion",
    "href": "benchmark.html#sec-bench-discussion",
    "title": "3  Benchmarking",
    "section": "3.3 Discussion",
    "text": "3.3 Discussion\nDoes speed really matter at the milliseconds level, and justify the raîson d’être for {SLmetrics} - the answer is inevitably no. A reduction of a few milliseconds may marginally improve performance, perhaps shaving off minutes or hours in large-scale grid searches or multi-model experiments. While this might slightly reduce cloud expenses, the overall impact is often negligible unless you’re operating at an enormous scale or in latency-critical environments.\nHowever, the memory efficiency of {SLmetrics} is where its real value lies. Its near-zero RAM usage allows more memory to be allocated for valuable tasks, such as feeding larger datasets into models. This can directly lead to higher-performing models, as more data generally improves learning outcomes. Furthermore, by optimizing memory usage, {SLmetrics} can reduce infrastructure costs significantly, as less powerful machines or fewer cloud resources may be required to achieve the same — or better — results.\nIn short, while speed optimization may seem like a more visible metric, it’s the memory efficiency of {SLmetrics} that has a broader, more transformative impact on machine learning workflows, from enabling better model performance to substantial cost reductions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-conclusion",
    "href": "benchmark.html#sec-bench-conclusion",
    "title": "3  Benchmarking",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nThe benchmarks conducted in Section 3.2 suggests that {SLmetrics} is the memory-efficient and fast alternative to {MLmetrics}, {yardstick} and {mlr3measures}.\nIn the worst performing benchmarks {SLmetrics} is on par with low-level implementations of equivalent metrics and is consistently more memory-efficient in all benchmarks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "regression_functions.html",
    "href": "regression_functions.html",
    "title": "Regression functions",
    "section": "",
    "text": "Examples\nIn this section all available regression metrics and related documentation is described. Common for all regression functions is that they use the class numeric.\n## actual\nactual &lt;- c(1.3, 2.4, 0.7, 0.1)\n\n## predicted\npredicted &lt;- c(0.7, 2.9, 0.76, 0.07)\nSLmetrics::rmse(\n    actual,\n    predicted\n)\n\n[1] 0.3919503",
    "crumbs": [
      "Regression functions"
    ]
  },
  {
    "objectID": "ref_regression/ccc.html",
    "href": "ref_regression/ccc.html",
    "title": "Concordance Correlation Coefficient",
    "section": "",
    "text": "ccc.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the concordance correlation coefficient. Use weighted.ccc() for the weighted concordance correlation coefficient.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nccc(actual, predicted, correction = FALSE, ...)\n\n## S3 method for class 'numeric'\nweighted.ccc(actual, predicted, w, correction = FALSE, ...)\n\nccc(\n ...,\n correction = FALSE\n)\n\nweighted.ccc(\n ...,\n w,\n correction = FALSE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\ncorrection\n\n\n\nA  vector of length \\(1\\) (default: FALSE). If TRUE the variance and covariance will be adjusted with \\(\\frac{1-n}{n}\\)\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nLet \\(\\rho_c \\in [0,1]\\) measure the agreement between \\(y\\) and \\(\\upsilon\\). The classifier agreement is calculated as,\n\n\\[\n  \\rho_c = \\frac{2 \\rho \\sigma_{\\upsilon} \\sigma_y}{\\sigma_{\\upsilon}^2 + \\sigma_y^2 + (\\mu_{\\upsilon} - \\mu_y)^2}\n\\]\n\nWhere:\n\n\n\n\n\\(\\rho\\) is the pearson correlation coefficient\n\n\n\n\n\\(\\sigma_y\\) is the unbiased standard deviation of \\(y\\)\n\n\n\n\n\\(\\sigma_{\\upsilon}\\) is the unbiased standard deviation of \\(\\upsilon\\)\n\n\n\n\n\\(\\mu_y\\) is the mean of \\(y\\)\n\n\n\n\n\\(\\mu_{\\upsilon}\\) is the mean of \\(\\upsilon\\)\n\n\n\n\nIf correction == TRUE each \\(\\sigma_{i \\in [y, \\upsilon]}\\) is adjusted by \\(\\frac{1-n}{n}\\)\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance\ncat(\n  \"Concordance Correlation Coefficient\", ccc(\n    actual     = actual,\n    predicted  = predicted,\n    correction = FALSE\n  ),\n  \"Concordance Correlation Coefficient (corrected)\", ccc(\n    actual     = actual,\n    predicted  = predicted,\n    correction = TRUE\n  ),\n  \"Concordance Correlation Coefficient (weigthed)\", weighted.ccc(\n    actual     = actual,\n    predicted  = predicted,\n    w          = mtcars$mpg/mean(mtcars$mpg),\n    correction = FALSE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Concordance Correlation Coefficient"
    ]
  },
  {
    "objectID": "ref_regression/huberloss.html",
    "href": "ref_regression/huberloss.html",
    "title": "Huber Loss Function",
    "section": "",
    "text": "huberloss.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe huberloss()-function computes the simple and weighted huber loss between the predicted and observed &lt;numeric&gt; vectors. The weighted.huberloss() function computes the weighted Huber Loss.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nhuberloss(actual, predicted, delta = 1, ...)\n\n## S3 method for class 'numeric'\nweighted.huberloss(actual, predicted, w, delta = 1, ...)\n\n## Generic S3 method\nhuberloss(\n actual,\n predicted,\n delta = 1,\n ...\n)\n\n## Generic S3 method\nweighted.huberloss(\n actual,\n predicted,\n w,\n delta = 1,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\ndelta\n\n\n\nA &lt;numeric&gt;-vector of length \\(1\\) (default: \\(1\\)). The threshold value for switch between functions (see calculation).\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as follows,\n\n\\[\n\\frac{1}{2} (y - \\upsilon)^2 ~for~ |y - \\upsilon| \\leq \\delta\n\\]\n\nand\n\n\\[\n  \\delta |y-\\upsilon|-\\frac{1}{2} \\delta^2 ~for~ \\text{otherwise}\n\\]\n\nwhere \\(y\\) and \\(\\upsilon\\) are the actual and predicted values respectively. If w is not NULL, then all values are aggregated using the weights.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n\n# 2) calculate the metric\n# with delta 0.5\nhuberloss(\n  actual = actual,\n  predicted = predicted,\n  delta = 0.5\n)\n\n# 3) caclulate weighted\n# metric using arbitrary weights\nw &lt;- rbeta(\n  n = 1e3,\n  shape1 = 10,\n  shape2 = 2\n)\n\nhuberloss(\n  actual = actual,\n  predicted = predicted,\n  delta = 0.5,\n  w     = w\n)",
    "crumbs": [
      "Regression functions",
      "Huber Loss Function"
    ]
  },
  {
    "objectID": "ref_regression/mae.html",
    "href": "ref_regression/mae.html",
    "title": "Mean Absolute Error",
    "section": "",
    "text": "mae.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe mae()-function computes the mean absolute error between the observed and predicted &lt;numeric&gt; vectors. The weighted.mae() function computes the weighted mean absolute error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nmae(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.mae(actual, predicted, w, ...)\n\n## Generic S3 method\nmae(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.mae(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calulated as follows,\n\n\\[\n  \\frac{\\sum_i^n |y_i - \\upsilon_i|}{n}\n\\]\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Mean Absolute Error (MAE)\ncat(\n  \"Mean Absolute Error\", mae(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Mean Absolute Error (weighted)\", weighted.mae(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Mean Absolute Error"
    ]
  },
  {
    "objectID": "ref_regression/mape.html",
    "href": "ref_regression/mape.html",
    "title": "Mean Absolute Percentage error",
    "section": "",
    "text": "mape.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe mape()-function computes the mean absolute percentage error between the observed and predicted &lt;numeric&gt; vectors. The weighted.mape() function computes the weighted mean absolute percentage error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nmape(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.mape(actual, predicted, w, ...)\n\n## Generic S3 method\nmape(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.mape(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\frac{1}{n} \\sum_i^n \\frac{|y_i - \\upsilon_i|}{|y_i|}\n\\]\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Mean Absolute Percentage Error (MAPE)\ncat(\n  \"Mean Absolute Percentage Error\", mape(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Mean Absolute Percentage Error (weighted)\", weighted.mape(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Mean Absolute Percentage error"
    ]
  },
  {
    "objectID": "ref_regression/mcc.html",
    "href": "ref_regression/mcc.html",
    "title": "Matthews Correlation Coefficient",
    "section": "",
    "text": "mcc.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe mcc()-function computes the Matthews Correlation Coefficient (MCC), also known as the \\(\\phi\\)-coefficient, between two vectors of predicted and observed factor() values. The weighted.mcc() function computes the weighted Matthews Correlation Coefficient.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nmcc(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.mcc(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\nmcc(x, ...)\n\n## S3 method for class 'factor'\nphi(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.phi(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\nphi(x, ...)\n\n## Generic S3 method\nmcc(...)\n\n## Generic S3 method\nweighted.mcc(\n ...,\n w\n)\n\n## Generic S3 method\nphi(...)\n\n## Generic S3 method\nweighted.phi(\n ...,\n w\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nThe metric is calculated as follows,\n\n\\[\n  \\frac{\\#TP \\times \\#TN - \\#FP \\times \\#FN}{\\sqrt{(\\#TP + \\#FP)(\\#TP + \\#FN)(\\#TN + \\#FP)(\\#TN + \\#FN)}}\n\\]\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate performance\n# using Matthews Correlation Coefficient\ncat(\n  \"Matthews Correlation Coefficient\", mcc(\n    actual    = actual,\n    predicted = predicted\n  ),\n  \"Matthews Correlation Coefficient (weighted)\", weighted.mcc(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Matthews Correlation Coefficient"
    ]
  },
  {
    "objectID": "ref_regression/mse.html",
    "href": "ref_regression/mse.html",
    "title": "Mean Squared Error",
    "section": "",
    "text": "mse.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe mse()-function computes the mean squared error between the observed and predicted &lt;numeric&gt; vectors. The weighted.mse() function computes the weighted mean squared error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nmse(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.mse(actual, predicted, w, ...)\n\n## Generic S3 method\nmse(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.mse(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\frac{1}{n} \\sum_i^n (y_i - \\upsilon_i)^2\n\\]\n\nWhere \\(y_i\\) and \\(\\upsilon_i\\) are the actual and predicted values respectively.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Mean Squared Error (MSE)\ncat(\n  \"Mean Squared Error\", mse(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Mean Squared Error (weighted)\", weighted.mse(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Mean Squared Error"
    ]
  },
  {
    "objectID": "ref_regression/pinball.html",
    "href": "ref_regression/pinball.html",
    "title": "Pinball Loss",
    "section": "",
    "text": "pinball.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe pinball()-function computes the pinball loss between the observed and predicted &lt;numeric&gt; vectors. The weighted.pinball() function computes the weighted Pinball Loss.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\npinball(actual, predicted, alpha = 0.5, deviance = FALSE, ...)\n\n## S3 method for class 'numeric'\nweighted.pinball(actual, predicted, w, alpha = 0.5, deviance = FALSE, ...)\n\n## Generic S3 method\npinball(\n actual,\n predicted,\n alpha    = 0.5,\n deviance = FALSE,\n ...\n)\n\n## Generic S3 method\nweighted.pinball(\n actual,\n predicted,\n w,\n alpha    = 0.5,\n deviance = FALSE,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\nalpha\n\n\n\nA &lt;numeric&gt;-value of length \\(1\\) (default: \\(0.5\\)). The slope of the pinball loss function.\n\n\n\n\n\ndeviance\n\n\n\nA -value of length 1 (default: FALSE). If TRUE the function returns the \\(D^2\\) loss.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\\text{PinballLoss}_{\\text{unweighted}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\alpha \\cdot \\max(0, y_i - \\hat{y}_i) - (1 - \\alpha) \\cdot \\max(0, \\hat{y}_i - y_i) \\right]\\]\n\nwhere \\(y_i\\) is the actual value, \\(\\hat{y}_i\\) is the predicted value and \\(\\alpha\\) is the quantile level.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Pinball Loss\ncat(\n  \"Pinball Loss\", pinball(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Pinball Loss (weighted)\", weighted.pinball(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Pinball Loss"
    ]
  },
  {
    "objectID": "ref_regression/rae.html",
    "href": "ref_regression/rae.html",
    "title": "Relative Absolute Error",
    "section": "",
    "text": "rae.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe rae()-function calculates the normalized relative absolute error between the predicted and observed &lt;numeric&gt; vectors. The weighted.rae() function computes the weigthed relative absolute error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nrae(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.rae(actual, predicted, w, ...)\n\n## Generic S3 method\nrae(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.rae(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe Relative Absolute Error (RAE) is calculated as:\n\n\\[\n  \\text{RAE} = \\frac{\\sum_{i=1}^n |y_i - \\upsilon_i|}{\\sum_{i=1}^n |y_i - \\bar{y}|}\n\\]\n\nWhere \\(y_i\\) are the actual values, \\(\\upsilon_i\\) are the predicted values, and \\(\\bar{y}\\) is the mean of the actual values.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Relative Absolute Error (RAE)\ncat(\n  \"Relative Absolute Error\", rae(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Relative Absolute Error (weighted)\", weighted.rae(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Relative Absolute Error"
    ]
  },
  {
    "objectID": "ref_regression/rmse.html",
    "href": "ref_regression/rmse.html",
    "title": "Root Mean Squared Error",
    "section": "",
    "text": "rmse.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe rmse()-function computes the root mean squared error between the observed and predicted &lt;numeric&gt; vectors. The weighted.rmse() function computes the weighted root mean squared error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nrmse(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.rmse(actual, predicted, w, ...)\n\n## Generic S3 method\nrmse(\n actual,\n predicted,\n ...\n)\n\nweighted.rmse(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\sqrt{\\frac{1}{n} \\sum_i^n (y_i - \\upsilon_i)^2}\n\\]\n\nWhere \\(y_i\\) and \\(\\upsilon_i\\) are the actual and predicted values respectively.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Root Mean Squared Error (RMSE)\ncat(\n  \"Root Mean Squared Error\", rmse(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Root Mean Squared Error (weighted)\", weighted.rmse(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Root Mean Squared Error"
    ]
  },
  {
    "objectID": "ref_regression/rmsle.html",
    "href": "ref_regression/rmsle.html",
    "title": "Root Mean Squared Logarithmic Error",
    "section": "",
    "text": "rmsle.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe rmsle()-function computes the root mean squared logarithmic error between the observed and predicted &lt;numeric&gt; vectors. The weighted.rmsle() function computes the weighted root mean squared logarithmic error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nrmsle(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.rmsle(actual, predicted, w, ...)\n\n## Generic S3 method\nrmsle(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.rmsle(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\sqrt{\\frac{1}{n} \\sum_i^n (\\log(1 + y_i) - \\log(1 + \\upsilon_i))^2}\n\\]\n\nWhere \\(y_i\\) and \\(\\upsilon_i\\) are the actual and predicted values respectively.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n\n# 2) evaluate in-sample model\n# performance using Root Mean Squared Logarithmic Error (RMSLE)\ncat(\n  \"Root Mean Squared Logarithmic Error\", rmsle(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Root Mean Squared Logarithmic Error (weighted)\", weighted.rmsle(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Root Mean Squared Logarithmic Error"
    ]
  },
  {
    "objectID": "ref_regression/rrmse.html",
    "href": "ref_regression/rrmse.html",
    "title": "Relative Root Mean Squared Error",
    "section": "",
    "text": "rrmse.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe rrmse()-function computes the Relative Root Mean Squared Error between the observed and predicted &lt;numeric&gt; vectors. The weighted.rrmse() function computes the weighted Relative Root Mean Squared Error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nrrmse(actual, predicted, normalization = 1L, ...)\n\n## S3 method for class 'numeric'\nweighted.rrmse(actual, predicted, w, normalization = 1L, ...)\n\n## Generic S3 method\nrrmse(\n actual,\n predicted,\n normalization = 1,\n ...\n)\n\n## Generic S3 method\nweighted.rrmse(\n actual,\n predicted,\n w,\n normalization = 1,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\nnormalization\n\n\n\nA &lt;numeric&gt;-value of length \\(1\\) (default: \\(1\\)). \\(0\\): mean-normalization, \\(1\\): range-normalization, \\(2\\): IQR-normalization.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\frac{RMSE}{\\gamma}\n\\]\n\nWhere \\(\\gamma\\) is the normalization factor.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Relative Root Mean Squared Error (RRMSE)\ncat(\n  \"IQR Relative Root Mean Squared Error\", rrmse(\n    actual        = actual,\n    predicted     = predicted,\n    normalization = 2\n  ),\n  \"IQR Relative Root Mean Squared Error (weighted)\", weighted.rrmse(\n    actual        = actual,\n    predicted     = predicted,\n    w             = mtcars$mpg/mean(mtcars$mpg),\n    normalization = 2\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Relative Root Mean Squared Error"
    ]
  },
  {
    "objectID": "ref_regression/rrse.html",
    "href": "ref_regression/rrse.html",
    "title": "Root Relative Squared Error",
    "section": "",
    "text": "rrse.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe rrse()-function calculates the root relative squared error between the predicted and observed &lt;numeric&gt; vectors. The weighted.rrse() function computes the weighed root relative squared errorr.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nrrse(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.rrse(actual, predicted, w, ...)\n\n## Generic S3 method\nrrse(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.rrse(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\text{RRSE} = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\upsilon_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]\n\nWhere \\(y_i\\) are the actual values, \\(\\upsilon_i\\) are the predicted values, and \\(\\bar{y}\\) is the mean of the actual values.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Relative Root Squared Errror (RRSE)\ncat(\n  \"Relative Root Squared Errror\", rrse(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Relative Root Squared Errror (weighted)\", weighted.rrse(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Root Relative Squared Error"
    ]
  },
  {
    "objectID": "ref_regression/smape.html",
    "href": "ref_regression/smape.html",
    "title": "Symmetric Mean Absolute Percentage Error",
    "section": "",
    "text": "smape.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe smape()-function computes the symmetric mean absolute percentage error between the observed and predicted &lt;numeric&gt; vectors. The weighted.smape() function computes the weighted symmetric mean absolute percentage error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nsmape(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.smape(actual, predicted, w, ...)\n\n## Generic S3 method\nsmape(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.smape(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as follows,\n\n\\[\n  \\sum_i^n \\frac{1}{n} \\frac{|y_i - \\upsilon_i|}{\\frac{|y_i|+|\\upsilon_i|}{2}}\n\\]\n\nwhere \\(y_i\\) and \\(\\upsilon_i\\) is the actual and predicted values respectively.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Symmetric Mean Absolute Percentage Error (MAPE)\ncat(\n  \"Symmetric Mean Absolute Percentage Error\", mape(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Symmetric Mean Absolute Percentage Error (weighted)\", weighted.mape(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Regression functions",
      "Symmetric Mean Absolute Percentage Error"
    ]
  },
  {
    "objectID": "classification_functions.html",
    "href": "classification_functions.html",
    "title": "Classification functions",
    "section": "",
    "text": "A primer on factors\nIn this section all available classification metrics and related documentation is described. Common for all classifcation functions is that they use the method foo.factor or foo.cmatrix.\nConsider a classification problem with three classes: A, B, and C. The actual vector of factor values is defined as follows:\n## set seed\nset.seed(1903)\n\n## actual\nactual &lt;- factor(\n    x = sample(x = 1:3, size = 10, replace = TRUE),\n    levels = c(1, 2, 3),\n    labels = c(\"A\", \"B\", \"C\")\n)\n\n## print values\nprint(actual)\n\n#&gt;  [1] B A B B A C B C C A\n#&gt; Levels: A B C\nHere, the values 1, 2, and 3 are mapped to A, B, and C, respectively. Now, suppose your model does not predict any B’s. The predicted vector of factor values would be defined as follows:\n## set seed\nset.seed(1903)\n\n## predicted\npredicted &lt;- factor(\n    x = sample(x = c(1, 3), size = 10, replace = TRUE),\n    levels = c(1, 2, 3),\n    labels = c(\"A\", \"B\", \"C\")\n)\n\n## print values\nprint(predicted)\n\n#&gt;  [1] C A C C C C C C A C\n#&gt; Levels: A B C\nIn both cases, \\(k = 3\\), determined indirectly by the levels argument.",
    "crumbs": [
      "Classification functions"
    ]
  },
  {
    "objectID": "classification_functions.html#examples",
    "href": "classification_functions.html#examples",
    "title": "Classification functions",
    "section": "Examples",
    "text": "Examples\nIn this section a brief introduction to the two methods are given.\n\nfactor method\n\n## factor method\nSLmetrics::accuracy(\n  actual,\n  predicted\n)\n\n#&gt; [1] 0.3\n\n\n\n\ncmatrix method\n\n## 1) generate confusion\n## matrix (cmatrix class)\nconfusion_matrix &lt;- SLmetrics::cmatrix(\n  actual,\n  predicted\n)\n\n## 2) check class\nclass(confusion_matrix)\n\n#&gt; [1] \"cmatrix\"\n\n## 3) summarise\nsummary(confusion_matrix)\n\n#&gt; Confusion Matrix (3 x 3) \n#&gt; ================================================================================\n#&gt;   A B C\n#&gt; A 1 0 2\n#&gt; B 0 0 4\n#&gt; C 1 0 2\n#&gt; ================================================================================\n#&gt; Overall Statistics (micro average)\n#&gt;  - Accuracy:          0.30\n#&gt;  - Balanced Accuracy: 0.33\n#&gt;  - Sensitivity:       0.30\n#&gt;  - Specificity:       0.65\n#&gt;  - Precision:         0.30\n\n\nThe confusion_matrix can be passed into accuracy() as follows:\n\nSLmetrics::accuracy(\n  confusion_matrix\n)\n\n#&gt; [1] 0.3\n\n\nUsing the cmatrix-method is more efficient if more than one classification metric is going to be calculated, as the metrics are calculated directly from the cmatrix-object, instead of looping though all the values in actual and predicted values for each metrics. See below:\n\ncat(\n  sep = \"\\n\",\n  paste(\"Accuracy:\", SLmetrics::accuracy(\n  confusion_matrix)),\n  paste(\"Balanced Accuracy:\", SLmetrics::baccuracy(\n  confusion_matrix))\n)\n\n#&gt; Accuracy: 0.3\n#&gt; Balanced Accuracy: 0.333333333333333",
    "crumbs": [
      "Classification functions"
    ]
  },
  {
    "objectID": "ref_classification/ROC.html",
    "href": "ref_classification/ROC.html",
    "title": "Receiver Operator Characteristics",
    "section": "",
    "text": "ROC.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe ROC()-function computes the tpr() and fpr() at thresholds provided by the \\(response\\)- or \\(thresholds\\)-vector. The function constructs a data.frame() grouped by \\(k\\)-classes where each class is treated as a binary classification problem.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nROC(actual, response, thresholds = NULL, presorted = FALSE, ...)\n\n## S3 method for class 'factor'\nweighted.ROC(actual, response, w, thresholds = NULL, presorted = FALSE, ...)\n\nroc.auc(actual, response, micro = NULL, method = 0, ...)\n\nweighted.roc.auc(actual, response, w, micro = NULL, method = 0, ...)\n\n## Generic S3 method\nROC(\n actual,\n response,\n thresholds = NULL,\n presorted  = FALSE,\n ...\n)\n\n## Generic S3 method\nweighted.ROC(\n actual,\n response,\n w,\n thresholds = NULL,\n presorted  = FALSE,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nresponse\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated response probabilities.\n\n\n\n\n\nthresholds\n\n\n\nAn optional &lt;numeric&gt; \\(n \\times k\\) matrix (default: NULL).\n\n\n\n\n\npresorted\n\n\n\nA -value length 1 (default: FALSE). If TRUE the input will not be sorted by threshold.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nmethod\n\n\n\nA &lt;numeric&gt; value (default: \\(0\\)). Defines the underlying method of calculating the area under the curve. If \\(0\\) it is calculated using the trapezoid-method, if \\(1\\) it is calculated using the step-method.\n\n\n\n\n\n\nValue\n\nA data.frame on the following form,\n\n\n\n\nthreshold\n\n\n\n&lt;numeric&gt; Thresholds used to determine tpr() and fpr()\n\n\n\n\n\nlevel\n\n\n\n The level of the actual \n\n\n\n\n\nlabel\n\n\n\n The levels of the actual \n\n\n\n\n\nfpr\n\n\n\n&lt;numeric&gt; The false positive rate\n\n\n\n\n\ntpr\n\n\n\n&lt;numeric&gt; The true positve rate\n\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\nresponse &lt;- predict(model, type = \"response\")\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) generate reciever\n# operator characteristics\n\n# 4.1) calculate residual\n# probability and store as matrix\nresponse &lt;- matrix(\n  data = cbind(response, 1-response),\n  nrow = length(actual)\n)\n\n# 4.2) construct \n# data.frame\nroc &lt;- ROC(\n  actual   = actual,\n  response = response\n)\n\n# 5) plot by species\nplot(roc)\n\n# 5.1) summarise\nsummary(roc)\n\n# 6) provide custom\n# threholds\nroc &lt;- ROC(\n  actual     = actual,\n  response   = response,\n  thresholds = seq(\n    1,\n    0,\n    length.out = 20\n  )\n)\n\n# 5) plot by species\nplot(roc)",
    "crumbs": [
      "Classification functions",
      "Receiver Operator Characteristics"
    ]
  },
  {
    "objectID": "ref_classification/accuracy.html",
    "href": "ref_classification/accuracy.html",
    "title": "Accuracy",
    "section": "",
    "text": "accuracy.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the (normalized) accuracy in classification tasks. Use weighted.accuracy() for the weighted accuracy.\n\n\n\nUsage\n\n## S3 method for class 'factor'\naccuracy(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.accuracy(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\naccuracy(x, ...)\n\n## Generic S3 method\naccuracy(...)\n\n## Generic S3 method\nweighted.accuracy(\n...,\nw\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\alpha} \\in [0, 1]\\) be the proportion of correctly predicted classes. The accuracy of the classifier is calculated as,\n\n\\[\n  \\hat{\\alpha} = \\frac{\\#TP + \\#TN}{\\#TP + \\#TN + \\#FP + \\#FN}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TP\\) is the number of true positives,\n\n\n\n\n\\(\\#TN\\) is the number of true negatives,\n\n\n\n\n\\(\\#FP\\) is the number of false positives, and\n\n\n\n\n\\(\\#FN\\) is the number of false negatives.\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model\n# performance\ncat(\n  \"Accuracy\", accuracy(\n    actual    = actual,\n    predicted = predicted\n  ),\n\n  \"Accuracy (weigthed)\", weighted.accuracy(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Accuracy"
    ]
  },
  {
    "objectID": "ref_classification/baccuracy.html",
    "href": "ref_classification/baccuracy.html",
    "title": "Balanced Accuracy",
    "section": "",
    "text": "baccuracy.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the (normalized) balanced accuracy. Use weighted.baccuracy() for the weighted balanced accuracy.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nbaccuracy(actual, predicted, adjust = FALSE, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.baccuracy(actual, predicted, w, adjust = FALSE, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nbaccuracy(x, adjust = FALSE, na.rm = TRUE, ...)\n\n## Generic S3 method\nbaccuracy(\n  ...,\n  adjust = FALSE,\n  na.rm  = TRUE\n)\n\n## Generic S3 method\nweighted.baccuracy(\n  ...,\n  w,\n  adjust = FALSE,\n  na.rm  = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\nadjust\n\n\n\nA logical value (default: FALSE). If TRUE the metric is adjusted for random chance \\(\\frac{1}{k}\\).\n\n\n\n\n\nna.rm\n\n\n\nA logical value (default: TRUE). If TRUE calculation of the metric is based on valid classes.\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA numeric-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\alpha} \\in [0, 1]\\) be the proportion of correctly predicted classes. If adjust == false, the balanced accuracy of the classifier is calculated as,\n\n\\[\n  \\hat{\\alpha} = \\frac{\\text{sensitivity} + \\text{specificity}}{2}\n\\]\n\notherwise,\n\n\\[\n  \\hat{\\alpha} = \\frac{\\text{sensitivity} + \\text{specificity}}{2} \\frac{1}{k}\n\\]\n\nWhere:\n\n\n\n\n\\(k\\) is the number of classes\n\n\n\n\n\\(\\text{sensitivity}\\) is the overall sensitivity, and\n\n\n\n\n\\(\\text{specificity}\\) is the overall specificity\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate the\n# model\ncat(\n  \"Balanced accuracy\", baccuracy(\n    actual    = actual,\n    predicted = predicted\n  ),\n  \n  \"Balanced accuracy (weigthed)\", weighted.baccuracy(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Balanced Accuracy"
    ]
  },
  {
    "objectID": "ref_classification/ckappa.html",
    "href": "ref_classification/ckappa.html",
    "title": "Cohen’s Kappa Statistic",
    "section": "",
    "text": "ckappa.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for Cohen’s \\(\\kappa\\)-statistic. Use weighted.ckappa() for the weighted \\(\\kappa\\)-statistic.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nckappa(actual, predicted, beta = 0, ...)\n\n## S3 method for class 'factor'\nweighted.ckappa(actual, predicted, w, beta = 0, ...)\n\n## S3 method for class 'cmatrix'\nckappa(x, beta = 0, ...)\n\nckappa(\n ...,\n beta = 0\n)\n\nweighted.ckappa(\n ...,\n w,\n beta = 0\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nbeta\n\n\n\nA &lt;numeric&gt; value of length 1 (default: 0). If \\(\\beta \\neq 0\\) the off-diagonals of the confusion matrix are penalized with a factor of \\((y_{+} - y_{i,-})^\\beta\\).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\kappa \\in [0, 1]\\) be the inter-rater (intra-rater) reliability. The inter-rater (intra-rater) reliability is calculated as,\n\n\\[\n  \\kappa = \\frac{\\rho_p - \\rho_e}{1-\\rho_e}\n\\]\n\nWhere:\n\n\n\n\n\\(\\rho_p\\) is the empirical probability of agreement between predicted and actual values\n\n\n\n\n\\(\\rho_e\\) is the expected probability of agreement under random chance\n\n\n\n\nIf \\(\\beta \\neq 0\\) the off-diagonals in the confusion matrix is penalized before \\(\\rho\\) is calculated. More formally,\n\n\\[\n\\chi = X \\circ Y^{\\beta}\n\\]\n\nWhere:\n\n\n\n\n\\(X\\) is the confusion matrix\n\n\n\n\n\\(Y\\) is the penalizing matrix and\n\n\n\n\n\\(\\beta\\) is the penalizing factor\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model performance with\n# Cohens Kappa statistic\ncat(\n  \"Kappa\", ckappa(\n    actual    = actual,\n    predicted = predicted\n  ),\n  \"Kappa (penalized)\", ckappa(\n    actual    = actual,\n    predicted = predicted,\n    beta      = 2\n  ),\n  \"Kappa (weigthed)\", weighted.ckappa(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Cohen's Kappa Statistic"
    ]
  },
  {
    "objectID": "ref_classification/cmatrix.html",
    "href": "ref_classification/cmatrix.html",
    "title": "Confusion Matrix",
    "section": "",
    "text": "cmatrix.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe cmatrix()-function uses cross-classifying factors to build a confusion matrix of the counts at each combination of the factor levels. Each row of the matrix represents the actual factor levels, while each column represents the predicted factor levels.\n\n\n\nUsage\n\n## S3 method for class 'factor'\ncmatrix(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.cmatrix(actual, predicted, w, ...)\n\n## Generic S3 method\ncmatrix(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.cmatrix(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\) (default: NULL) If passed it will return a weighted confusion matrix.\n\n\n\n\n\n\nValue\n\nA named \\(k\\) x \\(k\\) \n\n\n\nDimensions\n\nThere is no robust defensive measure against mis-specifying the confusion matrix. If the arguments are correctly specified, the resulting confusion matrix is on the form:\n\n\n\n\n\n\nA (Predicted)\n\n\nB (Predicted)\n\n\n\n\nA (Actual)\n\n\nValue\n\n\nValue\n\n\n\n\nB (Actual)\n\n\nValue\n\n\nValue\n\n\n\n\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) summarise performance\n# in a confusion matrix\n\n# 4.1) unweighted matrix\nconfusion_matrix &lt;- cmatrix(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.1.1) summarise matrix\nsummary(\n  confusion_matrix\n)\n\n# 4.1.2) plot confusion\n# matrix\nplot(\n  confusion_matrix\n)\n\n# 4.2) weighted matrix\nconfusion_matrix &lt;- weighted.cmatrix(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 4.2.1) summarise matrix\nsummary(\n  confusion_matrix\n)\n\n# 4.2.1) plot confusion\n# matrix\nplot(\n  confusion_matrix\n)",
    "crumbs": [
      "Classification functions",
      "Confusion Matrix"
    ]
  },
  {
    "objectID": "ref_classification/dor.html",
    "href": "ref_classification/dor.html",
    "title": "Diagnostic Odds Ratio",
    "section": "",
    "text": "dor.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the diagnostic odds ratio in classification tasks. Use weighted.dor() weighted diagnostic odds ratio.\n\n\n\nUsage\n\n## S3 method for class 'factor'\ndor(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.dor(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\ndor(x, ...)\n\n## Generic S3 method\ndor(...)\n\n## Generic S3 method\nweighted.dor(\n ...,\n w\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\alpha} \\in [0, \\infty]\\) be the effectiveness of the classifier. The diagnostic odds ratio of the classifier is calculated as,\n\n\\[\n  \\hat{\\alpha} = \\frac{\\text{\\#TP} \\text{\\#TN}}{\\text{\\#FP} \\text{\\#FN}}\n\\]\n\nWhere:\n\n\n\n\n\\(\\text{\\#TP}\\) is the number of true positives\n\n\n\n\n\\(\\text{\\#TN}\\) is the number of true negatives\n\n\n\n\n\\(\\text{\\#FP}\\) is the number of false positives\n\n\n\n\n\\(\\text{\\#FN}\\) is the number of false negatives\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n\n# 4) evaluate model performance\n# with Diagnostic Odds Ratio\ncat(\"Diagnostic Odds Ratio\", sep = \"\\n\")\ndor(\n  actual    = actual, \n  predicted = predicted\n)\n\ncat(\"Diagnostic Odds Ratio (weighted)\", sep = \"\\n\")\nweighted.dor(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)",
    "crumbs": [
      "Classification functions",
      "Diagnostic Odds Ratio"
    ]
  },
  {
    "objectID": "ref_classification/entropy.html",
    "href": "ref_classification/entropy.html",
    "title": "Entropy",
    "section": "",
    "text": "entropy.matrix\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe entropy() function calculates the Entropy of given probability distributions.\n\n\n\nUsage\n\n## S3 method for class 'matrix'\nentropy(pk, dim = 0L, base = -1, ...)\n\n## S3 method for class 'matrix'\nrelative.entropy(pk, qk, dim = 0L, base = -1, ...)\n\n## S3 method for class 'matrix'\ncross.entropy(pk, qk, dim = 0L, base = -1, ...)\n\n## Generic S3 method\nentropy(\n pk,\n dim  = 0,\n base = -1,\n ...\n)\n\n## Generic S3 method\nrelative.entropy(\n pk,\n qk,\n dim  = 0,\n base = -1,\n ...\n)\n\n## Generic S3 method\ncross.entropy(\n pk,\n qk,\n dim  = 0,\n base = -1,\n ...\n)\n\n\n\nArguments\n\n\n\npk\n\n\n\nA \\(n \\times k\\) &lt;numeric&gt;-matrix of observed probabilities. The \\(i\\)-th row should sum to 1 (i.e., a valid probability distribution over the \\(k\\) classes). The first column corresponds to the first factor level in actual, the second column to the second factor level, and so on.\n\n\n\n\n\ndim\n\n\n\nAn  value of length 1 (Default: 0). Defines the dimension along which to calculate the entropy (0: total, 1: row-wise, 2: column-wise).\n\n\n\n\n\nbase\n\n\n\nA &lt;numeric&gt; value of length 1 (Default: -1). The logarithmic base to use. Default value specifies natural logarithms.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nqk\n\n\n\nA \\(n \\times k\\) &lt;numeric&gt;-matrix of predicted probabilities. The \\(i\\)-th row should sum to 1 (i.e., a valid probability distribution over the \\(k\\) classes). The first column corresponds to the first factor level in actual, the second column to the second factor level, and so on.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; value or vector:\n\n\n\n\nA single &lt;numeric&gt; value (length 1) if dim == 0.\n\n\n\n\nA &lt;numeric&gt; vector with length equal to the length of rows if dim == 1.\n\n\n\n\nA &lt;numeric&gt; vector with length equal to the length of columns if dim == 2.\n\n\n\n\n\nDefinition\n\nEntropy:\n\n\\[H(pk) = -\\sum_{i} pk_i \\log(pk_i)\\]\n\nCross Entropy:\n\n\\[H(pk, qk) = -\\sum_{i} pk_i \\log(qk_i)\\]\n\nRelative Entropy\n\n\\[D_{KL}(pk \\parallel qk) = \\sum_{i} pk_i \\log\\left(\\frac{pk_i}{qk_i}\\right)\\]\n\n\nExamples\n\n# 1) Define actual\n# and observed probabilities\n\n# 1.1) actual probabilies\npk &lt;- matrix(\n  cbind(1/2, 1/2),\n  ncol = 2\n)\n\n# 1.2) observed (estimated) probabilites\nqk &lt;- matrix(\n  cbind(9/10, 1/10), \n  ncol = 2\n)\n\n# 2) calculate\n# Entropy\ncat(\n  \"Entropy\", entropy(\n    pk\n  ),\n  \"Relative Entropy\", relative.entropy(\n    pk,\n    qk\n  ),\n  \"Cross Entropy\", cross.entropy(\n    pk,\n    qk\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Entropy"
    ]
  },
  {
    "objectID": "ref_classification/fbeta.html",
    "href": "ref_classification/fbeta.html",
    "title": "F-beta Score",
    "section": "",
    "text": "fbeta.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the \\(F_{\\beta}\\)-score. Use weighted.fbeta() for the weighted \\(F_{\\beta}\\)-score.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfbeta(actual, predicted, beta = 1, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fbeta(actual, predicted, w, beta = 1, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfbeta(x, beta = 1, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nfbeta(\n ...,\n beta  = 1,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.fbeta(\n ...,\n w,\n beta = 1,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nbeta\n\n\n\nA &lt;numeric&gt; vector of length \\(1\\) (default: \\(1\\)).\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{F}_{\\beta} \\in [0, 1]\\) be the \\(F_{\\beta}\\) score, which is a weighted harmonic mean of precision and recall. \\(F_{\\beta}\\) score of the classifier is calculated as,\n\n\\[\n  \\hat{F}_{\\beta} = \\left(1 + \\beta^2\\right) \\frac{\\text{Precision} \\times \\text{Recall}}\n                                          {\\beta^2 \\times \\text{Precision} + \\text{Recall}}\n\\]\n\nSubstituting \\(\\text{Precision} = \\frac{\\#TP_k}{\\#TP_k + \\#FP_k}\\) and \\(\\text{Recall} = \\frac{\\#TP_k}{\\#TP_k + \\#FN_k}\\) yields:\n\n\\[\n  \\hat{F}_{\\beta} = \\left(1 + \\beta^2\\right)\n    \\frac{\\frac{\\#TP_k}{\\#TP_k + \\#FP_k} \\times \\frac{\\#TP_k}{\\#TP_k + \\#FN_k}}\n         {\\beta^2 \\times \\frac{\\#TP_k}{\\#TP_k + \\#FP_k} + \\frac{\\#TP_k}{\\#TP_k + \\#FN_k}}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TP_k\\) is the number of true positives,\n\n\n\n\n\\(\\#FP_k\\) is the number of false positives,\n\n\n\n\n\\(\\#FN_k\\) is the number of false negatives, and\n\n\n\n\n\\(\\beta\\) is a non-negative real number that determines the relative importance of precision vs. recall in the score.\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using F1-score\n\n# 4.1) unweighted F1-score\nfbeta(\n  actual    = actual,\n  predicted = predicted,\n  beta      = 1\n)\n\n# 4.2) weighted F1-score\nweighted.fbeta(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length),\n  beta      = 1\n)\n\n# 5) evaluate overall performance\n# using micro-averaged F1-score\ncat(\n  \"Micro-averaged F1-score\", fbeta(\n    actual    = actual,\n    predicted = predicted,\n    beta      = 1,\n    micro     = TRUE\n  ),\n  \"Micro-averaged F1-score (weighted)\", weighted.fbeta(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    beta      = 1,\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "F-beta Score"
    ]
  },
  {
    "objectID": "ref_classification/fdr.html",
    "href": "ref_classification/fdr.html",
    "title": "False Discovery Rate",
    "section": "",
    "text": "fdr.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the False Discovery Rate. Use weighted.fdr() for the weighted False Discovery Rate.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfdr(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fdr(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfdr(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nfdr(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.fdr(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\alpha} \\in [0, 1]\\) be the proportion of false positives among the preditced positives. The false discovery rate of the classifier is calculated as,\n\n\\[\n  \\hat{\\alpha} = \\frac{\\#FP_k}{\\#TP_k+\\#FP_k}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TP_k\\) is the number of true positives, and\n\n\n\n\n\\(\\#FP_k\\) is the number of false positives\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using False Discovery Rate\n\n# 4.1) unweighted False Discovery Rate\nfdr(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted False Discovery Rate\nweighted.fdr(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged False Discovery Rate\ncat(\n  \"Micro-averaged False Discovery Rate\", fdr(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged False Discovery Rate (weighted)\", weighted.fdr(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "False Discovery Rate"
    ]
  },
  {
    "objectID": "ref_classification/fer.html",
    "href": "ref_classification/fer.html",
    "title": "False Omission Rate",
    "section": "",
    "text": "fer.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the false omission rate. Use weighted.fdr() for the weighted false omission rate.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfer(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fer(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfer(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nfer(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.fer(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\beta} \\in [0, 1]\\) be the proportion of false negatives among the predicted negatives. The false omission rate of the classifier is calculated as,\n\n\\[\n  \\hat{\\beta} = \\frac{\\#FN_k}{\\#TN_k + \\#FN_k}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TN_k\\) is the number of true negatives, and\n\n\n\n\n\\(\\#FN_k\\) is the number of false negatives.\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using False Omission Rate\n\n# 4.1) unweighted False Omission Rate\nfer(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted False Omission Rate\nweighted.fer(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged False Omission Rate\ncat(\n  \"Micro-averaged False Omission Rate\", fer(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged False Omission Rate (weighted)\", weighted.fer(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "False Omission Rate"
    ]
  },
  {
    "objectID": "ref_classification/fmi.html",
    "href": "ref_classification/fmi.html",
    "title": "Fowlkes-Mallows Index",
    "section": "",
    "text": "fmi.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe fmi()-function computes the Fowlkes-Mallows Index (FMI), a measure of the similarity between two sets of clusterings, between two vectors of predicted and observed factor() values.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfmi(actual, predicted, ...)\n\n## S3 method for class 'cmatrix'\nfmi(x, ...)\n\n## Generic S3 method\nfmi(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  \\sqrt{\\frac{\\#TP_k}{\\#TP_k + \\#FP_k} \\times \\frac{\\#TP_k}{\\#TP_k + \\#FN_k}}\n\\]\n\nWhere \\(\\#TP_k\\), \\(\\#FP_k\\), and \\(\\#FN_k\\) represent the number of true positives, false positives, and false negatives for each class \\(k\\), respectively.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model performance\n# using Fowlkes Mallows Index\ncat(\n  \"Fowlkes Mallows Index\", fmi(\n  actual    = actual,\n  predicted = predicted\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Fowlkes-Mallows Index"
    ]
  },
  {
    "objectID": "ref_classification/fpr.html",
    "href": "ref_classification/fpr.html",
    "title": "False Positive Rate",
    "section": "",
    "text": "fpr.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the False Positive Rate. Use weighted.fpr() for the weighted False Positive Rate.\n\n\nOther names\n\nFallout\n\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfpr(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fpr(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfpr(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nfallout(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fallout(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfallout(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nfpr(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nfallout(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.fpr(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.fallout(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\gamma} \\in [0, 1]\\) be the proportion of false positives among the actual negatives. The false positive rate of the classifier is calculated as,\n\n\\[\n  \\hat{\\gamma} = \\frac{\\#FP_k}{\\#TN_k + \\#FP_k}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TN_k\\) is the number of true negatives, and\n\n\n\n\n\\(\\#FP_k\\) is the number of false positives.\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using False Positive Rate\n\n# 4.1) unweighted False Positive Rate\nfpr(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted False Positive Rate\nweighted.fpr(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged False Positive Rate\ncat(\n  \"Micro-averaged False Positive Rate\", fpr(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged False Positive Rate (weighted)\", weighted.fpr(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "False Positive Rate"
    ]
  },
  {
    "objectID": "ref_classification/jaccard.html",
    "href": "ref_classification/jaccard.html",
    "title": "Jaccard Score",
    "section": "",
    "text": "jaccard.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe jaccard()-function computes the Jaccard Index, also known as the Intersection over Union, between two vectors of predicted and observed factor() values. The weighted.jaccard() function computes the weighted Jaccard Index.\n\n\n\nUsage\n\n## S3 method for class 'factor'\njaccard(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.jaccard(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\njaccard(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\ncsi(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.csi(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\ncsi(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\ntscore(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.tscore(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\ntscore(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\njaccard(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\ncsi(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\ntscore(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.jaccard(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.csi(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.tscore(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  \\frac{\\#TP_k}{\\#TP_k + \\#FP_k + \\#FN_k}\n\\]\n\nWhere \\(\\#TP_k\\), \\(\\#FP_k\\), and \\(\\#FN_k\\) represent the number of true positives, false positives, and false negatives for each class \\(k\\), respectively.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using Jaccard Index\n\n# 4.1) unweighted Jaccard Index\njaccard(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted Jaccard Index\nweighted.jaccard(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged Jaccard Index\ncat(\n  \"Micro-averaged Jaccard Index\", jaccard(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged Jaccard Index (weighted)\", weighted.jaccard(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Jaccard Score"
    ]
  },
  {
    "objectID": "ref_classification/logloss.html",
    "href": "ref_classification/logloss.html",
    "title": "Log Loss",
    "section": "",
    "text": "logloss.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe logloss() function computes the Log Loss between observed classes (as a ) and their predicted probability distributions (a &lt;numeric&gt; matrix). The weighted.logloss() function is the weighted version, applying observation-specific weights.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nlogloss(actual, response, normalize = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.logloss(actual, response, w, normalize = TRUE, ...)\n\n## S3 method for class 'integer'\nlogloss(actual, response, normalize = TRUE, ...)\n\n## S3 method for class 'integer'\nweighted.logloss(actual, response, w, normalize = TRUE, ...)\n\n## Generic S3 method\nlogloss(\n actual,\n response,\n normalize = TRUE,\n ...\n)\n\n## Generic S3 method\nweighted.logloss(\n actual,\n response,\n w,\n normalize = TRUE,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\nresponse\n\n\n\nA \\(n \\times k\\) &lt;numeric&gt;-matrix of predicted probabilities. The \\(i\\)-th row should sum to 1 (i.e., a valid probability distribution over the \\(k\\) classes). The first column corresponds to the first factor level in actual, the second column to the second factor level, and so on.\n\n\n\n\n\nnormalize\n\n\n\nA -value (default: TRUE). If TRUE, the mean cross-entropy across all observations is returned; otherwise, the sum of cross-entropies is returned.\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\\[H(p, response) = -\\sum_{i} \\sum_{j} y_{ij} \\log_2(response_{ij})\\]\n\nwhere:\n\n\n\n\n\\(y_{ij}\\) is the actual-values, where \\(y_{ij}\\) = 1 if the i-th sample belongs to class j, and 0 otherwise.\n\n\n\n\n\\(response_{ij}\\) is the estimated probability for the i-th sample belonging to class j.\n\n\n\n\n\nExamples\n\n# 1) Recode the iris data set to a binary classification problem\n#    Here, the positive class (\"Virginica\") is coded as 1,\n#    and the rest (\"Others\") is coded as 0.\niris$species_num &lt;- as.numeric(iris$Species == \"virginica\")\n\n# 2) Fit a logistic regression model predicting species_num from Sepal.Length &amp; Sepal.Width\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(link = \"logit\")\n)\n\n# 3) Generate predicted classes: \"Virginica\" vs. \"Others\"\npredicted &lt;- factor(\n  as.numeric(predict(model, type = \"response\") &gt; 0.5),\n  levels = c(1, 0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) Generate actual classes\nactual &lt;- factor(\n  x      = iris$species_num,\n  levels = c(1, 0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# For Log Loss, we need predicted probabilities for each class.\n# Since it's a binary model, we create a 2-column matrix:\n#   1st column = P(\"Virginica\")\n#   2nd column = P(\"Others\") = 1 - P(\"Virginica\")\npredicted_probs &lt;- predict(model, type = \"response\")\nresponse_matrix &lt;- cbind(predicted_probs, 1 - predicted_probs)\n\n# 4) Evaluate unweighted Log Loss\n#    'logloss' takes (actual, response_matrix, normalize=TRUE/FALSE).\n#    The factor 'actual' must have the positive class (Virginica) as its first level.\nunweighted_LogLoss &lt;- logloss(\n  actual    = actual,           # factor\n  response  = response_matrix,  # numeric matrix of probabilities\n  normalize = TRUE              # normalize = TRUE\n)\n\n# 5) Evaluate weighted Log Loss\n#    We introduce a weight vector, for example:\nweights &lt;- iris$Petal.Length / mean(iris$Petal.Length)\nweighted_LogLoss &lt;- weighted.logloss(\n  actual    = actual,\n  response  = response_matrix,\n  w         = weights,\n  normalize = TRUE\n)\n\n# 6) Print Results\ncat(\n  \"Unweighted Log Loss:\", unweighted_LogLoss,\n  \"Weighted Log Loss:\", weighted_LogLoss,\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Log Loss"
    ]
  },
  {
    "objectID": "ref_classification/mpe.html",
    "href": "ref_classification/mpe.html",
    "title": "Mean Percentage Error",
    "section": "",
    "text": "mpe.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe mpe()-function computes the mean percentage error between the observed and predicted &lt;numeric&gt; vectors. The weighted.mpe() function computes the weighted mean percentage error.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nmpe(actual, predicted, ...)\n\n## S3 method for class 'numeric'\nweighted.mpe(actual, predicted, w, ...)\n\n## Generic S3 method\nmpe(\n actual,\n predicted,\n ...\n)\n\n## Generic S3 method\nweighted.mpe(\n actual,\n predicted,\n w,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nThe metric is calculated as,\n\n\\[\n  \\frac{1}{n} \\sum_i^n \\frac{y_i - \\upsilon_i}{y_i}\n\\]\n\nWhere \\(y_i\\) and \\(\\upsilon_i\\) are the actual and predicted values respectively.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance using Mean Percentage Error (MPE)\ncat(\n  \"Mean Percentage Error\", mpe(\n    actual    = actual,\n    predicted = predicted,\n  ),\n  \"Mean Percentage Error (weighted)\", weighted.mpe(\n    actual    = actual,\n    predicted = predicted,\n    w         = mtcars$mpg/mean(mtcars$mpg)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Mean Percentage Error"
    ]
  },
  {
    "objectID": "ref_classification/nlr.html",
    "href": "ref_classification/nlr.html",
    "title": "Negative Likelihood Ratio",
    "section": "",
    "text": "nlr.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the negative likelihood ratio in classification tasks. Use weighted.nlr() weighted negative likelihood ratio.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nnlr(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.nlr(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\nnlr(x, ...)\n\n## Generic S3 method\nnlr(...)\n\n## Generic S3 method\nweighted.nlr(\n ...,\n w\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\alpha} \\in [0, \\infty]\\) be the likelihood of a negative outcome. The negative likelihood ratio of the classifier is calculated as,\n\n\\[\n  \\hat{\\alpha} = \\frac{1 - \\frac{\\#TP}{\\#TP + \\#FN}}{\\frac{\\#TN}{\\#TN + \\#FP}}\n\\]\n\nWhere:\n\n\n\n\n\\(\\frac{\\#TP}{\\#TP + \\#FN}\\) is the sensitivity, or true positive rate\n\n\n\n\n\\(\\frac{\\#TN}{\\#TN + \\#FP}\\) is the specificity, or true negative rate\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model performance\n# with class-wise negative likelihood ratios\ncat(\"Negative Likelihood Ratio\", sep = \"\\n\")\nnlr(\n  actual    = actual, \n  predicted = predicted\n)\n\ncat(\"Negative Likelihood Ratio (weighted)\", sep = \"\\n\")\nweighted.nlr(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)",
    "crumbs": [
      "Classification functions",
      "Negative Likelihood Ratio"
    ]
  },
  {
    "objectID": "ref_classification/npv.html",
    "href": "ref_classification/npv.html",
    "title": "Negative Predictive Value",
    "section": "",
    "text": "npv.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe npv()-function computes the negative predictive value, also known as the True Negative Predictive Value, between two vectors of predicted and observed factor() values. The weighted.npv() function computes the weighted negative predictive value.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nnpv(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.npv(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nnpv(x, micro = NULL, na.rm = TRUE, ...)\n\nnpv(...)\n\nweighted.npv(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  \\frac{\\#TN_k}{\\#TN_k + \\#FN_k}\n\\]\n\nWhere \\(\\#TN_k\\) and \\(\\#FN_k\\) are the number of true negatives and false negatives, respectively, for each class \\(k\\).\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using Negative Predictive Value\n\n# 4.1) unweighted Negative Predictive Value\nnpv(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted Negative Predictive Value\nweighted.npv(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged Negative Predictive Value\ncat(\n  \"Micro-averaged Negative Predictive Value\", npv(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged Negative Predictive Value (weighted)\", weighted.npv(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Negative Predictive Value"
    ]
  },
  {
    "objectID": "ref_classification/plr.html",
    "href": "ref_classification/plr.html",
    "title": "Positive Likelihood Ratio",
    "section": "",
    "text": "plr.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the positive likelihood ratio in classification tasks. Use weighted.plr() weighted positive likelihood ratio.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nplr(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.plr(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\nplr(x, ...)\n\n## Generic S3 method\nplr(...)\n\n## Generic S3 method\nweighted.plr(\n ...,\n w\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\alpha} \\in [0, \\infty]\\) be the likelihood of a positive outcome. The positive likelihood ratio of the classifier is calculated as,\n\n\\[\n  \\hat{\\alpha} = \\frac{\\frac{\\#TP}{\\#TP + \\#FN}}{1 - \\frac{\\#TN}{\\#TN + \\#FP}}\n\\]\n\nWhere:\n\n\n\n\n\\(\\frac{\\#TP}{\\#TP + \\#FN}\\) is the sensitivity, or true positive rate\n\n\n\n\n\\(\\frac{\\#TN}{\\#TN + \\#FP}\\) is the specificity, or true negative rate\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model performance\n# with class-wise positive likelihood ratios\ncat(\"Positive Likelihood Ratio\", sep = \"\\n\")\nplr(\n  actual    = actual, \n  predicted = predicted\n)\n\ncat(\"Positive Likelihood Ratio (weighted)\", sep = \"\\n\")\nweighted.plr(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)",
    "crumbs": [
      "Classification functions",
      "Positive Likelihood Ratio"
    ]
  },
  {
    "objectID": "ref_classification/prROC.html",
    "href": "ref_classification/prROC.html",
    "title": "Precision-Recall Curve",
    "section": "",
    "text": "prROC.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe prROC()-function computes the precision() and recall() at thresholds provided by the \\(response\\)- or \\(thresholds\\)-vector. The function constructs a data.frame() grouped by \\(k\\)-classes where each class is treated as a binary classification problem.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nprROC(actual, response, thresholds = NULL, presorted = FALSE, ...)\n\n## S3 method for class 'factor'\nweighted.prROC(actual, response, w, thresholds = NULL, presorted = FALSE, ...)\n\npr.auc(actual, response, micro = NULL, method = 0, ...)\n\nweighted.pr.auc(actual, response, w, micro = NULL, method = 0, ...)\n\n## Generic S3 method\nprROC(\n actual,\n response,\n thresholds = NULL,\n presorted  = FALSE,\n ...\n)\n\n## Generic S3 method\nweighted.prROC(\n actual,\n response,\n w,\n thresholds = NULL,\n presorted  = FALSE,\n ...\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nresponse\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated response probabilities.\n\n\n\n\n\nthresholds\n\n\n\nAn optional &lt;numeric&gt; \\(n \\times k\\) matrix (default: NULL).\n\n\n\n\n\npresorted\n\n\n\nA -value length 1 (default: FALSE). If TRUE the input will not be sorted by threshold.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nmethod\n\n\n\nA &lt;numeric&gt; value (default: \\(0\\)). Defines the underlying method of calculating the area under the curve. If \\(0\\) it is calculated using the trapezoid-method, if \\(1\\) it is calculated using the step-method.\n\n\n\n\n\n\nValue\n\nA data.frame on the following form,\n\n\n\n\nthreshold\n\n\n\n&lt;numeric&gt; Thresholds used to determine recall() and precision()\n\n\n\n\n\nlevel\n\n\n\n The level of the actual \n\n\n\n\n\nlabel\n\n\n\n The levels of the actual \n\n\n\n\n\nrecall\n\n\n\n&lt;numeric&gt; The recall\n\n\n\n\n\nprecision\n\n\n\n&lt;numeric&gt; The precision\n\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\nresponse &lt;- predict(model, type = \"response\")\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) generate precision-recall\n# data\n\n# 4.1) calculate residual\n# probability and store as matrix\nresponse &lt;- matrix(\n  data = cbind(response, 1-response),\n  nrow = length(actual)\n)\n\n# 4.2) generate precision-recall\n# data\nroc &lt;- prROC(\n  actual   = actual,\n  response = response\n)\n\n# 5) plot by species\nplot(roc)\n\n# 5.1) summarise\nsummary(roc)\n\n# 6) provide custom\n# threholds\nroc &lt;- prROC(\n  actual     = actual,\n  response   = response,\n  thresholds = seq(\n    1,\n    0,\n    length.out = 20\n  )\n)\n\n# 5) plot by species\nplot(roc)",
    "crumbs": [
      "Classification functions",
      "Precision-Recall Curve"
    ]
  },
  {
    "objectID": "ref_classification/precision.html",
    "href": "ref_classification/precision.html",
    "title": "Precision",
    "section": "",
    "text": "precision.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic funcion for the precision. Use weighted.fdr() for the weighted precision.\n\n\nOther names\n\nPositive Predictive Value\n\n\n\n\nUsage\n\n## S3 method for class 'factor'\nprecision(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.precision(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nprecision(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nppv(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.ppv(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nppv(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nprecision(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.precision(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nppv(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.ppv(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\pi} \\in [0, 1]\\) be the proportion of true positives among the predicted positives. The precision of the classifier is calculated as,\n\n\\[\n  \\hat{\\pi} = \\frac{\\#TP_k}{\\#TP_k + \\#FP_k}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TP_k\\) is the number of true positives, and\n\n\n\n\n\\(\\#FP_k\\) is the number of false positives.\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using Precision\n\n# 4.1) unweighted Precision\nprecision(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted Precision\nweighted.precision(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged Precision\ncat(\n  \"Micro-averaged Precision\", precision(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged Precision (weighted)\", weighted.precision(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Precision"
    ]
  },
  {
    "objectID": "ref_classification/recall.html",
    "href": "ref_classification/recall.html",
    "title": "Recall",
    "section": "",
    "text": "recall.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic funcion for the Recall. Use weighted.fdr() for the weighted Recall.\n\n\nOther names\n\nSensitivity, True Positive Rate\n\n\n\n\nUsage\n\n## S3 method for class 'factor'\nrecall(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.recall(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nrecall(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nsensitivity(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.sensitivity(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nsensitivity(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\ntpr(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.tpr(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\ntpr(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nrecall(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nsensitivity(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\ntpr(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.recall(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.sensitivity(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.tpr(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nLet \\(\\hat{\\rho} \\in [0, 1]\\) be the proportion of true positives among the actual positives. The recall of the classifier is calculated as,\n\n\\[\n  \\hat{\\rho} = \\frac{\\#TP_k}{\\#TP_k + \\#FN_k}\n\\]\n\nWhere:\n\n\n\n\n\\(\\#TP_k\\) is the number of true positives, and\n\n\n\n\n\\(\\#FN_k\\) is the number of false negatives.\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using Recall\n\n# 4.1) unweighted Recall\nrecall(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted Recall\nweighted.recall(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged Recall\ncat(\n  \"Micro-averaged Recall\", recall(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged Recall (weighted)\", weighted.recall(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Recall"
    ]
  },
  {
    "objectID": "ref_classification/rsq.html",
    "href": "ref_classification/rsq.html",
    "title": "Coefficient of Determination",
    "section": "",
    "text": "rsq.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic function for the \\(R^2\\). The unadjusted \\(R^2\\) is returned by default. Use weighted.rsq() for the weighted \\(R^2\\).\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nrsq(actual, predicted, k = 0, ...)\n\n## S3 method for class 'numeric'\nweighted.rsq(actual, predicted, w, k = 0, ...)\n\n## Generic S3 method\nrsq(\n ...,\n k = 0\n)\n\n## Generic S3 method\nweighted.rsq(\n ...,\n w,\n k = 0\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\nk\n\n\n\nA &lt;numeric&gt;-vector of length 1 (default: 0). For adjusted \\(R^2\\) set \\(k = \\kappa - 1\\), where \\(\\kappa\\) is the number of parameters.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nDefinition\n\nLet \\(R^2 \\in [-\\infty, 1]\\) be the explained variation. The \\(R^2\\) is calculated as,\n\n\\[\n  R^2 = 1 - \\frac{\\sum{(y_i - \\hat{y}_i)^2}}{\\sum{(y_i-\\bar{y})^2}} \\frac{n-1}{n - (k + 1)}\n\\]\n\nWhere:\n\n\n\n\n\\(n\\) is the number of observations\n\n\n\n\n\\(k\\) is the number of features\n\n\n\n\n\\(y\\) is the actual values\n\n\n\n\n\\(\\hat{y}_i\\) is the predicted values\n\n\n\n\n\\(\\sum{(y_i - \\hat{y}_i)^2}\\) is the sum of squared errors and,\n\n\n\n\n\\(\\sum{(y_i-\\bar{y})^2}\\) is total sum of squared errors\n\n\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel &lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure in-sample performance\nactual    &lt;- mtcars$mpg\npredicted &lt;- fitted(model)\n\n# 2) calculate performance\n# using R squared adjusted and\n# unadjused for features\ncat(\n  \"Rsq\", rsq(\n    actual    = actual,\n    predicted = fitted(model)\n  ),\n  \"Rsq (Adjusted)\", rsq(\n    actual    = actual,\n    predicted = fitted(model),\n    k = ncol(model.matrix(model)) - 1\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Coefficient of Determination"
    ]
  },
  {
    "objectID": "ref_classification/specificity.html",
    "href": "ref_classification/specificity.html",
    "title": "Specificity",
    "section": "",
    "text": "specificity.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nA generic funcion for the Specificity. Use weighted.specificity() for the weighted Specificity.\n\n\nOther names\n\nTrue Negative Rate, Selectivity\n\n\n\n\nUsage\n\n## S3 method for class 'factor'\nspecificity(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.specificity(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nspecificity(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\ntnr(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.tnr(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\ntnr(x, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nselectivity(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.selectivity(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nselectivity(x, micro = NULL, na.rm = TRUE, ...)\n\n## Generic S3 method\nspecificity(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\ntnr(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nselectivity(\n ...,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.specificity(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.tnr(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n## Generic S3 method\nweighted.selectivity(\n ...,\n w,\n micro = NULL,\n na.rm = TRUE\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of - of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of -vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA -value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA  value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using Specificity\n\n# 4.1) unweighted Specificity\nspecificity(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted Specificity\nweighted.specificity(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged Specificity\ncat(\n  \"Micro-averaged Specificity\", specificity(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged Specificity (weighted)\", weighted.specificity(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Specificity"
    ]
  },
  {
    "objectID": "ref_classification/zerooneloss.html",
    "href": "ref_classification/zerooneloss.html",
    "title": "Zero-One Loss",
    "section": "",
    "text": "zerooneloss.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe zerooneloss()-function computes the zero-one Loss, a classification loss function that calculates the proportion of misclassified instances between two vectors of predicted and observed factor() values. The weighted.zerooneloss() function computes the weighted zero-one loss.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nzerooneloss(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.zerooneloss(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\nzerooneloss(x, ...)\n\n## Generic S3 method\nzerooneloss(...)\n\n## Generic S3 method\nweighted.zerooneloss(\n ...,\n w\n)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of  with length \\(n\\), and \\(k\\) levels\n\n\n\n\n\n…\n\n\n\nmicro = NULL, na.rm = TRUE Arguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nDefinition\n\nThe metric is calculated as follows,\n\n\\[\n  \\frac{\\#FP + \\#FN}{\\#TP + \\#TN + \\#FP + \\#FN}\n\\]\n\nWhere \\(\\#TP\\), \\(\\#TN\\), \\(\\#FP\\), and \\(\\#FN\\) represent the true positives, true negatives, false positives, and false negatives, respectively.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num &lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel &lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted &lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt; 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual &lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model\n# performance using Zero-One Loss\ncat(\n  \"Zero-One Loss\", zerooneloss(\n    actual    = actual,\n    predicted = predicted\n  ),\n  \"Zero-One Loss (weigthed)\", weighted.zerooneloss(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Classification functions",
      "Zero-One Loss"
    ]
  },
  {
    "objectID": "openmp.html",
    "href": "openmp.html",
    "title": "4  OpenMP",
    "section": "",
    "text": "4.1 Enabling/Disabling OpenMP\n{SLmetrics} supports parallelization through OpenMP. In this section this functionality is introduced.\nOpenMP is disabled by default, but can be enabled as follows:\nSLmetrics::openmp.on()\n\n#&gt; OpenMP enabled!\nAnd disabled as follows:\nSLmetrics::openmp.off()\n\n#&gt; OpenMP disabled!\nBy default all threads are used. To control the amount of threads, see the following code:\nSLmetrics::openmp.threads(3)\n\n#&gt; Using 3 threads.\nTo use all threads:\nSLmetrics::openmp.threads(NULL)\n\n#&gt; Using 4 threads.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenMP</span>"
    ]
  },
  {
    "objectID": "openmp.html#available-threads",
    "href": "openmp.html#available-threads",
    "title": "4  OpenMP",
    "section": "4.2 Available threads",
    "text": "4.2 Available threads\nThe number of available threads are detected automatically, but can also be viewed using SLmetrics::openmp.threads() without passing any arguments. See below:\n\nSLmetrics::openmp.threads()\n\n#&gt; [1] 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenMP</span>"
    ]
  },
  {
    "objectID": "openmp.html#benchmarking-openmp",
    "href": "openmp.html#benchmarking-openmp",
    "title": "4  OpenMP",
    "section": "4.3 Benchmarking OpenMP",
    "text": "4.3 Benchmarking OpenMP\nTo benchmark the performance gain on enabling OpenMP, the same setup as in Chapter 3 is used. Below is the actual and predicted values are generated.\n\n# 1) set seed for reproducibility\nset.seed(1903)\n\n# 2) create classification\n# problem\nfct_actual &lt;- create_factor()\nfct_predicted &lt;- create_factor()\n\n\nCode\nSLmetrics::openmp.on()\n\nbenchmark(\n    `With OpenMP` = SLmetrics::cmatrix(fct_actual, fct_predicted)\n)\n\n\n\n\nTable 4.1: Benchmark of computing a 3x3 confusion matrix with OpenMP enabled. Each benchmark is run 10 times with two input vectors of 10 million elements.\n\n\n\n#&gt; # A tibble: 1 × 4\n#&gt;   expression  execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;             &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 With OpenMP         3.86ms           0B        0\n\n\n\n\n\nCode\nSLmetrics::openmp.off()\n\nbenchmark(\n    `Wihtout OpenMP` = SLmetrics::cmatrix(fct_actual, fct_predicted)\n)\n\n\n\n\nTable 4.2: Benchmark of computing a 3x3 confusion matrix with OpenMP enabled. Each benchmark is run 10 times with two input vectors of 10 million elements.\n\n\n\n#&gt; # A tibble: 1 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 Wihtout OpenMP         8.55ms           0B        0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenMP</span>"
    ]
  },
  {
    "objectID": "openmp.html#key-take-aways",
    "href": "openmp.html#key-take-aways",
    "title": "4  OpenMP",
    "section": "4.4 Key take-aways",
    "text": "4.4 Key take-aways\nEnabling OpenMP support can decrease computation time significantly - but it should only be used consciously, and with care, to avoid function calls competing for the same threads. This is especially the case if you are running a, say, neural network in parallel.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OpenMP</span>"
    ]
  },
  {
    "objectID": "garbage.html",
    "href": "garbage.html",
    "title": "5  Garbage in, garbage out",
    "section": "",
    "text": "5.1 Implicit assumptions\nThis section examines the underlying assumptions in {SLmetrics}, and how it may affect your pipeline if you decide adopt it.\nAll evaluation functions in {SLmetrics} assumes that end-user follows the typical AI/ML workflow:\nflowchart LR\n    B(Data Cleaning)\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Training]\n    D --&gt; E{Evaluation}\nThe implications of this assumption is two-fold:\nHence, the implicit assumption is that the end-user has a high degree of control over the training process and an understanding of R beyond beginner-level. See, for example, the following code:\n# 1) define values\nactual    &lt;- c(-1.2, 1.3, 2.6, 3)\npredicted &lt;- rev(actual) \n\n# 2) evaluate with RMSLE\nSLmetrics::rmsle(\n    actual,\n    predicted\n)\n\n#&gt; [1] NaN\nThe actual- and predicted-vector contains negative values, and is being passed to the root mean squared logarithmic error (rmsle())-function. It returns NaN without any warnings. The same action in using base R would lead to verbose errors:\nmean(log(actual))\n\n#&gt; Warning in log(actual): NaNs produced\n\n\n#&gt; [1] NaN",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "garbage.html#implicit-assumptions",
    "href": "garbage.html#implicit-assumptions",
    "title": "5  Garbage in, garbage out",
    "section": "",
    "text": "There is no handling of missing data in input variables\nThere is no validity check of inputs",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "garbage.html#undefined-behavior",
    "href": "garbage.html#undefined-behavior",
    "title": "5  Garbage in, garbage out",
    "section": "5.2 Undefined behavior",
    "text": "5.2 Undefined behavior\n\n\n\n\n\n\nImportant\n\n\n\nDo NOT run the chunks in this section in an R-session where you have important work, as your session will crash.\n\n\n{SLmetrics} uses pointer arithmetics via C++ which, contrary to usual practice in R, performs computations on memory addresses rather than the object itself. If the memory address is ill-defined, which can occur in cases where values lack valid binary representations for the operations being performed, undefined behavior1 follows and will crash your R-session. See this code:\n\n# 1) define values\nactual &lt;- factor(c(NA, \"A\", \"B\", \"A\"))\npredicted &lt;- rev(actual)\n\n# 2) pass into\n# cmatrix\nSLmetrics::cmatrix(\n    actual,\n    predicted\n)\n#&gt; address 0x5946ff482178, cause 'memory not mapped'\n#&gt; An irrecoverable exception occurred. R is aborting now ...\n\nThis is not something that can prevented with, say, try(), as the error is undefined. See this SO-post for details.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "garbage.html#edge-cases",
    "href": "garbage.html#edge-cases",
    "title": "5  Garbage in, garbage out",
    "section": "5.3 Edge cases",
    "text": "5.3 Edge cases\nThere are cases, where it can be hard to predict what will happen when passing a given set of actual and predicted classes. Especially if the input is too large, and it becomes inefficient to check these every iteration. In such cases {SLmetrics} does help. See for example the following code:\n\n# 1) define values\nactual &lt;- factor(\n    sample(letters[1:3],size = 1e7, replace = TRUE, prob = c(0.5, 0.5, 0)),\n    levels = letters[1:3]\n    )\npredicted &lt;- rev(actual)\n\n# 2) pass into\n# cmatrix\nSLmetrics::fbeta(\n    actual,\n    predicted\n)\n\n#&gt;         a         b         c \n#&gt; 0.4999661 0.4999807       NaN\n\n\nOne class, c, is never predicted, nor is it present in the actual labels - therefore, by construction, the value is NaN as there is division by zero. During aggregation to micro or macro averages these are being handled according to na.rm. See below:\n\n# 1) macro average\nSLmetrics::fbeta(\n    actual,\n    predicted,\n    micro = FALSE,\n    na.rm = TRUE\n)\n\n#&gt; [1] 0.4999734\n\n# 2) macro average\nSLmetrics::fbeta(\n    actual,\n    predicted,\n    micro = FALSE,\n    na.rm = FALSE\n)\n\n#&gt; [1] 0.3333156\n\n\n\n# 1) define values\nactual    &lt;- c(-1.2, 1.3, 2.6, 3)\npredicted &lt;- rev(actual) \n\n# 2) evaluate with RMSLE\ntry(\n    SLmetrics::RMSLE(\n        actual,\n        predicted\n    )\n)\n\n#&gt; Error : 'RMSLE' is not an exported object from 'namespace:SLmetrics'\n\n\nIn these cases, there is no undefined behaviour or exploding R sessions as all of this is handled internally.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "garbage.html#staying-safe",
    "href": "garbage.html#staying-safe",
    "title": "5  Garbage in, garbage out",
    "section": "5.4 Staying “safe”",
    "text": "5.4 Staying “safe”\nTo avoid undefined behavior when passing ill-defined input one option is to write a wrapper function, or using existing infrastructure. Below is an example of a wrapper function:\n\n# 1) RMSLE\nconfusion_matrix &lt;- function(\n    actual, \n    predicted) {\n\n        if (any(is.na(actual))) {\n            stop(\"`actual` contains missing values\")\n        }\n\n        if (any(is.na(predicted))) {\n            stop(\"`predicted` contains missing values\")\n        }\n\n        SLmetrics::cmatrix(\n            actual,\n            predicted\n        )\n\n}\n\n\n# 1) define values\nactual &lt;- factor(c(NA, \"A\", \"B\", \"A\", \"B\"))\npredicted &lt;- rev(actual)\n\n# 2) \ntry(\n    confusion_matrix(\n    actual,\n    predicted\n    )\n)\n\n#&gt; Error in confusion_matrix(actual, predicted) : \n#&gt;   `actual` contains missing values\n\n\nAnother option is to use the existing infrastructure. {yardstick} does all kinds of safety checks before executing a function, and you can, via the metric_vec_template() pass a SLmetrics::foo() in the foo_impl()-function. This gives you the safety of {yardstick}, and the efficiency of {SLmetrics}.2\n\n\n\n\n\n\nImportant\n\n\n\nBe aware that using {SLmetrics} with {yardstick} will introduce some efficiency overhead - especially on large vectors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "garbage.html#key-take-aways",
    "href": "garbage.html#key-take-aways",
    "title": "5  Garbage in, garbage out",
    "section": "5.5 Key take-aways",
    "text": "5.5 Key take-aways\n{SLmetrics} assumes that the end-user follows the typical AI/ML workflow, and has an understanding of R beyond beginner-level. And therefore {SLmetrics} does not check the validity of the user-input, which may lead to undefined behavior if input is ill-defined.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "garbage.html#footnotes",
    "href": "garbage.html#footnotes",
    "title": "5  Garbage in, garbage out",
    "section": "",
    "text": "Undefined behavior refers to program operations that are not prescribed by the language specification, leading to unpredictable results or crashes.↩︎\nAn example would be appropriate. But my first attempt lead to a decrecated-warning, which is also one of the main reasons I developed this {pkg}, and gave up. See the {documentation} on how to create custom metrics using {yardstick}.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Garbage in, garbage out</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ansel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, et al. 2024. “PyTorch 2: Faster Machine Learning Through Dynamic Python\nBytecode Transformation and Graph Compilation.” In\n29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2 (ASPLOS ’24).\nACM. https://doi.org/10.1145/3620665.3640366.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.\nGrisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning\nin Python.” Journal of Machine Learning\nResearch 12: 2825–30.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\nReddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python.” Nature Methods\n17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.",
    "crumbs": [
      "References"
    ]
  }
]