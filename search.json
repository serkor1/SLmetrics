[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{SLmetrics}: Machine Learning Performance Evaluation on Steroids",
    "section": "",
    "text": "Preface\n{SLmetrics} started as a personal project to learn C++, and was never really meant to be published nor, infact, named {SLmetrics}. But as time went by, and the committed time and commits grew the name stayed, and the goal to publish a functioning data science R package seemed like the natural next step.\nThe primary goal of {SLmetrics} is to be a fast, memory efficient and reliable low-level successor to {MLmetrics}; and the current benchmarks in 3  Benchmarking suggets that this goal, in fact, have been achieved.\n\n\n\n\n\n\nWarning\n\n\n\n{SLmetrics} and the documentation is currently under development",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The status-quo of {pkgs}\nThere are currently three {pkgs} that are developed with machine leaning performance evaluation in mind: {MLmetrics}, {yardstick}, {mlr3measures}. These {pkgs} have historically bridged the gap between R and Python in terms of machine learning and data science.\n{MLmetrics} can be considered the legacy code when it comes to performance evaluation, and it served as a backend in {yardstick} up to version 0.0.2. It is built entirely on base R, and has been stable since its inception almost 10 years ago.\nHowever, it appears that the development has reached it’s peak and is currently stale - see, for example, this stale PR related to this issue. Micro- and macro-averages have been implented in {scikit-learn} for many years, and {MLmetrics} simply didn’t keep up with the development.\n{yardstick}, on the other hand, carried the torch forward and implemented these modern features. {yardstick} closely follows the syntax, naming and functionality of {scikit-learn} but is built with {tidyverse} tools; although the source code is nice to look at, it does introduce some serious overhead and carries the risk of deprecations.\nFurthermore, it complicates a simple application by its verbose function naming, see for example metric()-function for &lt;tbl&gt; and metric_vec()-function for &lt;numeric&gt; - the output is the same, but the call is different. {yardstick} can’t handle more than one positive class at a time, so the end-user is forced to run the same function more than once to get performance metrics for the adjacent classes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-status-quo-of-pkgs",
    "href": "intro.html#the-status-quo-of-pkgs",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Summary\nIn short, the existing {pkgs} are outdated, inefficient and insufficient for modern large-scale machine learning applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-slmetrics",
    "href": "intro.html#why-slmetrics",
    "title": "1  Introduction",
    "section": "1.2 Why {SLmetrics}?",
    "text": "1.2 Why {SLmetrics}?\nAs the name suggests, {SLmetrics} closely resembles {MLmetrics} in it’s simplistic and low-level implementation of machine learning metrics. The resemblance ends there, however.\n{SLmetrics} are developed with three things in mind: speed, efficiency and scalability. And therefore addresses the shortcomings of the status-quo by construction - the {pkg} is built on c++ and {Rcpp} from the ground up. See Table 1.1 where\n\nCode\nset.seed(1903)\nactual &lt;- rnorm(1e7)\npredicted &lt;- actual + rnorm(1e7)\n\nbench::mark(\n    `{SLmetrics}` = SLmetrics::rmse(actual, predicted),\n    `{MLmetrics}` = MLmetrics::RMSE(predicted, actual),\n    iterations    = 100\n)\n\n\n\n\nTable 1.1: Calculating RMSE on 1e7 vectors\n\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 {SLmetrics}   29.6ms   30.1ms      33.1    6.13MB      0  \n2 {MLmetrics}     61ms   61.2ms      16.3   76.39MB     79.7\n\n\n\n\nThis shows that well-written R-code is hard to beat speed-wise. {MLmetrics} is roughly 20% faster - but uses 30,000 times more memory. How about constructing a confusion matrix\n\nCode\nset.seed(1903)\nactual &lt;- factor(sample(letters[1:3], size = 1e7, replace = TRUE))\npredicted &lt;- factor(sample(letters[1:3], size = 1e7, replace = TRUE))\n\nbench::mark(\n    `{SLmetrics}` = SLmetrics::cmatrix(actual, predicted),\n    `{MLmetrics}` = MLmetrics::ConfusionMatrix(actual, predicted),\n    check         = FALSE,\n    iterations    = 100\n)\n\n\n\n\nTable 1.2: Computing a 3x3 confusion matrix on 1e7 vectors\n\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 {SLmetrics}   8.53ms   8.58ms    116.      4.12KB     0   \n2 {MLmetrics} 244.52ms 250.44ms      3.71   381.6MB     7.64\n\n\n\n\n{SLmetrics} uses 1/50th of the time {MLmetrics} and the memory usage is equivalent as the previous example but uses significantly less memory than {MLmetrics}.\n\n1.2.1 Summary\n{SLmetrics} is, in the worst-case scenario, on par with low-level R implementations of equivalent metrics and is a multitude more memory-efficient than any of the {pkgs}. A detailed benchmark can be found here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#key-takeaways",
    "href": "intro.html#key-takeaways",
    "title": "1  Introduction",
    "section": "1.3 Key takeaways",
    "text": "1.3 Key takeaways",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "2.1 Basic Usage",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html#installation",
    "href": "summary.html#installation",
    "title": "2  Summary",
    "section": "2.2 Installation",
    "text": "2.2 Installation\n\n2.2.1 Stable version\n\n## install stable release\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics@*release',\n  ref  = 'main'\n)\n\n\n\n2.2.2 Development version\n\n## install development version\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics',\n  ref  = 'development'\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "3  Benchmarking",
    "section": "",
    "text": "3.1 The setup\nIn this section a detailed benchmark of {SLmetrics} is conducted. The benchmarks will be conducted on randomly selected functions, and then compared to {pkg} discussed in Chapter 1. The benchmarks are conducted on three parameters: median execution time, memory usage and gc() calls.\nThis section strucutred as follows, Section 3.1 sets up the infrastructure needed to conduct the benchmark in an unbiased way, in Section 3.2 the benchmarks are conducted and discussed and summarized in Section 3.3 and Section 3.4 respectively.\nTo conduct the benchmarking two functions are defined. create_regression() and create_factor(), both functions returns a vector of actual and predicted values with a length of 10,000,000 rows.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-setup",
    "href": "benchmark.html#sec-bench-setup",
    "title": "3  Benchmarking",
    "section": "",
    "text": "3.1.1 Regression problems\nThe benchmarks on regression metrics is conducted on correlated absolute value &lt;numeric&gt;-vectors, with uniformly distributed weights. create_regression() returns a named list, and is defined below:\n\n# regression function\ncreate_regression &lt;- function(\n    n = 1e7) {\n\n  # 1) actual\n  # values\n  actual &lt;- abs(rnorm(n = n))\n\n  # 2) predicted\n  # values\n  predicted &lt;- actual + abs(rnorm(n = n))\n\n  # 3) generate\n  # weights\n  w &lt;- runif(n)\n\n  list(\n    actual    = actual,\n    predicted = predicted,\n    w         = w\n  )\n}\n\n\n\n3.1.2 Classification problems\nThe benchmarks on classification metrics is conducted on the randomly sampled letters c(\"a\", \"b\", \"c\"). create_regression() returns a vector of &lt;factor&gt;, and is defined below:\n\n# classification function\ncreate_factor &lt;- function(\n    k = 3,\n    balanced = TRUE,\n    n = 1e7) {\n\n  probs &lt;- NULL\n\n  if (!balanced) {\n\n    probs &lt;- rbeta(\n      n = k,\n      shape1 = 10,\n      shape2 = 2\n    )\n\n    probs[which.min(probs)] &lt;- 0\n\n    probs &lt;- probs / sum(probs)\n\n  }\n\n  factor(\n    x = sample(\n      1:k,\n      size = n,\n      replace = TRUE,\n      prob = probs\n    ),\n    labels = letters[1:k],\n    levels = 1:k\n  )\n}\n\n\n\n3.1.3 Staging the testing ground\nThe vectors used in the benchmarks are created with the seed 1903 for reproducibility, see below:\n\n# 1) set seed for reproducibility\nset.seed(1903)\n\n# 2) create classification\n# problem\nfct_actual &lt;- create_factor()\nfct_predicted &lt;- create_factor()\n\n# 3) create regression\n# problem\n\n# 3.1) store results\n# in regression\nlst_regression &lt;- create_regression()\n\n# 3.2) assign the values\n# accordingly\nnum_actual &lt;- lst_regression$actual\nnum_predicted &lt;- lst_regression$predicted\nnum_weights &lt;- lst_regression$w",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-mark",
    "href": "benchmark.html#sec-bench-mark",
    "title": "3  Benchmarking",
    "section": "3.2 Benchmarking",
    "text": "3.2 Benchmarking\nTo conduct the benchmark {bench} is used. Before the benchmarks are conducted, a benchmark()-wrapper is created.\nThis wrapper conducts m (Default: 10) benchmarks, with 10 iterations for each benchmarked function passed into benchmark() - to allow for warm-up the first iteration is discarded. The wrapper is defined as follows:\n\nbenchmark &lt;- function(\n  ..., \n  m = 10) {\n  library(magrittr)\n  # 1) create list\n  # for storing values\n  performance &lt;- list()\n  \n  for (i in 1:m) {\n\n     # 1) run the benchmarks\n    results &lt;- bench::mark(\n      ...,\n      iterations = 10,\n      check = FALSE\n    )\n\n    # 2) extract values\n    # and calculate medians\n    performance$time[[i]]  &lt;- setNames(lapply(results$time, mean), results$expression)\n    performance$memory[[i]] &lt;- setNames(lapply(results$memory, function(x) { sum(x$bytes, na.rm = TRUE)}), results$expression)\n    performance$n_gc[[i]] &lt;- setNames(lapply(results$n_gc, sum), results$expression)\n\n  }\n\n  purrr::pmap_dfr(\n  list(performance$time, performance$memory, performance$n_gc), \n  ~{\n    tibble::tibble(\n      expression = names(..1),\n      time = unlist(..1),\n      memory = unlist(..2),\n      n_gc = unlist(..3)\n    )\n  }\n) %&gt;%\n  dplyr::mutate(expression = factor(expression, levels = unique(expression))) %&gt;%\n  dplyr::group_by(expression) %&gt;%\n  dplyr::filter(dplyr::row_number() &gt; 1) %&gt;%\n  dplyr::summarize(\n    execution_time = bench::as_bench_time(median(time)),\n    memory_usage = bench::as_bench_bytes(median(memory)),\n    gc_calls = median(n_gc),\n    .groups = \"drop\"\n  )\n\n}\n\n\n3.2.1 Regression metrics\n\nCode\nbenchmark(\n    `{RMSE}`  = SLmetrics::rmse(num_actual, num_predicted),\n    `{Pinball Loss}` = SLmetrics::pinball(num_actual, num_predicted),\n    `{Huber Loss}` = SLmetrics::huberloss(num_actual, num_predicted)\n)\n\n\n\n\nTable 3.1: Benchmarking selected regression metrics\n\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {RMSE}                 30.6ms           0B        0\n#&gt; 2 {Pinball Loss}         30.7ms           0B        0\n#&gt; 3 {Huber Loss}           75.5ms           0B        0\n\n\n\n\n\nCode\nbenchmark(\n    `{SLmetrics}` = SLmetrics::rmse(num_actual, num_predicted),\n    `{MLmetrics}` = MLmetrics::RMSE(num_actual, num_predicted),\n    `{yardstick}` = yardstick::rmse_vec(num_actual, num_predicted),\n    `{mlr3measures}` = mlr3measures::rmse(num_actual, num_predicted)\n)\n\n\n\n\nTable 3.2: Benchmarking RMSE across\n\n\n\n#&gt; # A tibble: 4 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {SLmetrics}            30.7ms           0B        0\n#&gt; 2 {MLmetrics}            61.6ms       76.3MB        1\n#&gt; 3 {yardstick}           169.9ms      419.6MB       10\n#&gt; 4 {mlr3measures}         86.8ms       76.3MB        1\n\n\n\n\n\n\n3.2.2 Classification metrics\n\nCode\nbenchmark(\n    `{Confusion Matrix}`  = SLmetrics::cmatrix(fct_actual, fct_predicted),\n    `{Accuracy}` = SLmetrics::accuracy(fct_actual, fct_predicted),\n    `{F-beta}` = SLmetrics::fbeta(fct_actual, fct_predicted)\n)\n\n\n\n\nTable 3.3: Benchmarking selected classification metrics\n\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression         execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                    &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {Confusion Matrix}         8.59ms           0B        0\n#&gt; 2 {Accuracy}                 8.59ms           0B        0\n#&gt; 3 {F-beta}                   8.59ms           0B        0\n\n\n\n\n\nCode\nbenchmark(\n    `{SLmetrics}`    = SLmetrics::cmatrix(fct_actual, fct_predicted),\n    `{MLmetrics}`    = MLmetrics::ConfusionMatrix(fct_predicted, fct_actual),\n    `{yardstick}`    = yardstick::conf_mat(table(fct_actual, fct_predicted))\n)\n\n\n\n\nTable 3.4: Benchmarking a 3x3 confusion matrix across\n\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression  execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;             &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {SLmetrics}         8.61ms           0B        0\n#&gt; 2 {MLmetrics}       248.64ms        381MB        7\n#&gt; 3 {yardstick}       249.36ms        381MB        7",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-discussion",
    "href": "benchmark.html#sec-bench-discussion",
    "title": "3  Benchmarking",
    "section": "3.3 Discussion",
    "text": "3.3 Discussion\nDoes speed really matter at the milliseconds level, and justify the raîson d’être for {SLmetrics} - the answer is inevitably no. A reduction of a few milliseconds may marginally improve performance, perhaps shaving off minutes or hours in large-scale grid searches or multi-model experiments. While this might slightly reduce cloud expenses, the overall impact is often negligible unless you’re operating at an enormous scale or in latency-critical environments.\nHowever, the memory efficiency of {SLmetrics} is where its real value lies. Its near-zero RAM usage allows more memory to be allocated for valuable tasks, such as feeding larger datasets into models. This can directly lead to higher-performing models, as more data generally improves learning outcomes. Furthermore, by optimizing memory usage, {SLmetrics} can reduce infrastructure costs significantly, as less powerful machines or fewer cloud resources may be required to achieve the same — or better — results.\nIn short, while speed optimization may seem like a more visible metric, it’s the memory efficiency of {SLmetrics} that has a broader, more transformative impact on machine learning workflows, from enabling better model performance to substantial cost reductions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#sec-bench-conclusion",
    "href": "benchmark.html#sec-bench-conclusion",
    "title": "3  Benchmarking",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nThe benchmarks conducted in Section 3.2 suggests that {SLmetrics} is the memory-efficient and fast alternative to {MLmetrics}, {yardstick} and {mlr3measures}.\nIn the worst performing benchmarks {SLmetrics} is on par with low-level implementations of equivalent metrics and is consistently more memory-efficient in all benchmarks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "references/accuracy.html",
    "href": "references/accuracy.html",
    "title": "accuracy",
    "section": "",
    "text": "accuracy.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe accuracy() function computes the accuracy between two vectors of predicted and observed factor() values. The weighted.accuracy() function computes the weighted accuracy.\n\n\n\nUsage\n\n## S3 method for class 'factor'\naccuracy(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.accuracy(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\naccuracy(x, ...)\n\naccuracy(...)\n\nweighted.accuracy(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt;-vector of length 1\n\n\n\nCalculation\n\nThe metric is calculated as follows,\n\n\\[\n  \\frac{\\#TP + \\#TN}{\\#TP + \\#TN + \\#FP + \\#FN}\n\\]\n\nWhere \\(\\#TP\\), \\(\\#TN\\), \\(\\#FP\\), and \\(\\#FN\\) is the number of true positives, true negatives, false positives, and false negatives, respectively.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model\n# performance\ncat(\n  \"Accuracy\", accuracy(\n    actual    = actual,\n    predicted = predicted\n  ),\n\n  \"Accuracy (weigthed)\", weighted.accuracy(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "accuracy"
    ]
  },
  {
    "objectID": "references/baccuracy.html",
    "href": "references/baccuracy.html",
    "title": "balanced accuracy",
    "section": "",
    "text": "baccuracy.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe baccuracy()-function computes the balanced accuracy between two vectors of predicted and observed factor() values. The weighted.baccuracy() function computes the weighted balanced accuracy.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nbaccuracy(actual, predicted, adjust = FALSE, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.baccuracy(actual, predicted, w, adjust = FALSE, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nbaccuracy(x, adjust = FALSE, na.rm = TRUE, ...)\n\nbaccuracy(...)\n\nweighted.baccuracy(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels\n\n\n\n\n\nadjust\n\n\n\nA logical value (default: FALSE). If TRUE the metric is adjusted for random chance \\(\\frac{1}{k}\\).\n\n\n\n\n\nna.rm\n\n\n\nA logical values (default: TRUE). If TRUE calculation of the metric is based on valid classes.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix()\n\n\n\n\n\n\nValue\n\nA numeric-vector of length 1\n\n\n\nCalculation\n\nThe metric is calculated as follows,\n\n\\[\n  \\frac{\\text{sensitivity} + \\text{specificty}}{2}\n\\]\n\nSee the sensitivity()- and/or specificity()-function for more details.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate the\n# model\ncat(\n  \"Balanced accuracy\", baccuracy(\n    actual    = actual,\n    predicted = predicted\n  ),\n  \n  \"Balanced accuracy (weigthed)\", weighted.baccuracy(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "balanced accuracy"
    ]
  },
  {
    "objectID": "references/ccc.html",
    "href": "references/ccc.html",
    "title": "concordance correlation coefficient",
    "section": "",
    "text": "ccc.numeric\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe ccc()-function computes the simple and weighted concordance correlation coefficient between the two vectors of predicted and observed &lt;numeric&gt; values. The weighted.ccc() function computes the weighted Concordance Correlation Coefficient. If correction is TRUE \\(\\sigma^2\\) is adjusted by \\(\\frac{1-n}{n}\\) in the intermediate steps.\n\n\n\nUsage\n\n## S3 method for class 'numeric'\nccc(actual, predicted, correction = FALSE, ...)\n\n## S3 method for class 'numeric'\nweighted.ccc(actual, predicted, w, correction = FALSE, ...)\n\nccc(...)\n\nweighted.ccc(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The observed (continuous) response variable.\n\n\n\n\n\npredicted\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The estimated (continuous) response variable.\n\n\n\n\n\ncorrection\n\n\n\nA &lt;logical&gt; vector of length \\(1\\) (default: FALSE). If TRUE the variance and covariance will be adjusted with \\(\\frac{1-n}{n}\\)\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). The weight assigned to each observation in the data.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; vector of length 1.\n\n\n\nCalculation\n\nThe metric is calculated as follows,\n\n\\[\n  \\rho_c = \\frac{2 \\rho \\sigma_x \\sigma_y}{\\sigma_x^2 + \\sigma_y^2 + (\\mu_x - \\mu_y)^2}\n\\]\n\nWhere \\(\\rho\\) is the \\(\\text{pearson correlation coefficient}\\), \\(\\sigma\\) is the \\(\\text{standard deviation}\\) and \\(\\mu\\) is the simple mean of actual and predicted.\n\n\n\nExamples\n\n# 1) fit a linear\n# regression\nmodel `&lt;- lm(\n  mpg ~ .,\n  data = mtcars\n)\n\n# 1.1) define actual\n# and predicted values\n# to measure performance\nactual    `&lt;- mtcars$mpg\npredicted `&lt;- fitted(model)\n\n# 2) evaluate in-sample model\n# performance\ncat(\n  \"Concordance Correlation Coefficient\", ccc(\n    actual     = actual,\n    predicted  = predicted,\n    correction = FALSE\n  ),\n  \"Concordance Correlation Coefficient (corrected)\", ccc(\n    actual     = actual,\n    predicted  = predicted,\n    correction = TRUE\n  ),\n  \"Concordance Correlation Coefficient (weigthed)\", weighted.ccc(\n    actual     = actual,\n    predicted  = predicted,\n    w          = mtcars$mpg/mean(mtcars$mpg),\n    correction = FALSE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "concordance correlation coefficient"
    ]
  },
  {
    "objectID": "references/ckappa.html",
    "href": "references/ckappa.html",
    "title": "cohens kappa",
    "section": "",
    "text": "ckappa.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe kappa()-function computes Cohen’s \\(\\kappa\\), a statistic that measures inter-rater agreement for categorical items between two vectors of predicted and observed factor() values. The weighted.ckappa() function computes the weighted \\(\\kappa\\)-statistic.\n\n\nIf \\(\\beta \\neq 0\\) the off-diagonals of the confusion matrix are penalized with a factor of \\((y_{+} - y_{i,-})^\\beta\\). See below for further details.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nckappa(actual, predicted, beta = 0, ...)\n\n## S3 method for class 'factor'\nweighted.ckappa(actual, predicted, w, beta = 0, ...)\n\n## S3 method for class 'cmatrix'\nckappa(x, beta = 0, ...)\n\nckappa(...)\n\nweighted.ckappa(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nbeta\n\n\n\nA &lt;numeric&gt; value of length 1 (default: 0). If set to a value different from zero, the off-diagonal confusion matrix will be penalized.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nCalculation\n\\[\n  \\frac{\\rho_p - \\rho_e}{1-\\rho_e}\n\\]\n\nwhere \\(\\rho_p\\) is the empirical probability of agreement between predicted and actual values, and \\(\\rho_e\\) is the expected probability of agreement under random chance.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate model performance with\n# Cohens Kappa statistic\ncat(\n  \"Kappa\", ckappa(\n    actual    = actual,\n    predicted = predicted\n  ),\n  \"Kappa (penalized)\", ckappa(\n    actual    = actual,\n    predicted = predicted,\n    beta      = 2\n  ),\n  \"Kappa (weigthed)\", weighted.ckappa(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length)\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "cohens kappa"
    ]
  },
  {
    "objectID": "references/cmatrix.html",
    "href": "references/cmatrix.html",
    "title": "confusion matrix",
    "section": "",
    "text": "cmatrix.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe cmatrix()-function uses cross-classifying factors to build a confusion matrix of the counts at each combination of the factor levels. Each row of the matrix represents the actual factor levels, while each column represents the predicted factor levels.\n\n\n\nUsage\n\n## S3 method for class 'factor'\ncmatrix(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.cmatrix(actual, predicted, w, ...)\n\ncmatrix(...)\n\nweighted.cmatrix(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods.\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\) (default: NULL) If passed it will return a weighted confusion matrix.\n\n\n\n\n\n\nValue\n\nA named \\(k\\) x \\(k\\) &lt;matrix&gt; of class \n\n\n\nDimensions\n\nThere is no robust defensive measure against misspecififying the confusion matrix. If the arguments are correctly specified, the resulting confusion matrix is on the form:\n\n\n\n\n\n\nA (Predicted)\n\n\nB (Predicted)\n\n\n\n\nA (Actual)\n\n\nValue\n\n\nValue\n\n\n\n\nB (Actual)\n\n\nValue\n\n\nValue\n\n\n\n\n\n\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) summarise performance\n# in a confusion matrix\n\n# 4.1) unweighted matrix\nconfusion_matrix `&lt;- cmatrix(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.1.1) summarise matrix\nsummary(\n  confusion_matrix\n)\n\n# 4.1.2) plot confusion\n# matrix\nplot(\n  confusion_matrix\n)\n\n# 4.2) weighted matrix\nconfusion_matrix `&lt;- weighted.cmatrix(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 4.2.1) summarise matrix\nsummary(\n  confusion_matrix\n)\n\n# 4.2.1) plot confusion\n# matrix\nplot(\n  confusion_matrix\n)",
    "crumbs": [
      "Functions",
      "confusion matrix"
    ]
  },
  {
    "objectID": "references/dor.html",
    "href": "references/dor.html",
    "title": "diagnostic odds ratio",
    "section": "",
    "text": "dor.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe dor()-function computes the Diagnostic Odds Ratio (DOR), a single indicator of test performance, between two vectors of predicted and observed factor() values. The weighted.dor() function computes the weighted diagnostic odds ratio.\n\n\nWhen aggregate = TRUE, the function returns the micro-average DOR across all classes \\(k\\). By default, it returns the class-wise DOR.\n\n\n\nUsage\n\n## S3 method for class 'factor'\ndor(actual, predicted, ...)\n\n## S3 method for class 'factor'\nweighted.dor(actual, predicted, w, ...)\n\n## S3 method for class 'cmatrix'\ndor(x, ...)\n\ndor(...)\n\nweighted.dor(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nCalculation\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  \\text{DOR}_k = \\frac{\\text{PLR}_k}{\\text{NLR}_k}\n\\]\n\nWhere \\(\\text{PLR}_k\\) and \\(\\text{NLR}_k\\) is the positive and negative likelihood ratio for class \\(k\\), respectively. See plr() and nlr() for more details.\n\n\nWhen aggregate = TRUE, the micro-average is calculated as,\n\n\\[\n  \\overline{\\text{DOR}} = \\frac{\\overline{\\text{PLR}_k}}{\\overline{\\text{NLR}_k}}\n\\]\n\nWhere \\(\\overline{\\text{PLR}}\\) and \\(\\overline{\\text{NLR}}\\) is the micro-averaged is the positive and negative likelihood ratio, respectively.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n\n# 4) evaluate model performance\n# with Diagnostic Odds Ratio\ncat(\"Diagnostic Odds Ratio\", sep = \"\\n\")\ndor(\n  actual    = actual, \n  predicted = predicted\n)\n\ncat(\"Diagnostic Odds Ratio (weighted)\", sep = \"\\n\")\nweighted.dor(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)",
    "crumbs": [
      "Functions",
      "diagnostic odds ratio"
    ]
  },
  {
    "objectID": "references/entropy.html",
    "href": "references/entropy.html",
    "title": "entropy",
    "section": "",
    "text": "entropy.matrix\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe entropy() function calculates the Entropy of given probability distributions.\n\n\n\nUsage\n\n## S3 method for class 'matrix'\nentropy(pk, dim = 0L, base = -1, ...)\n\n## S3 method for class 'matrix'\nrelative.entropy(pk, qk, dim = 0L, base = -1, ...)\n\n## S3 method for class 'matrix'\ncross.entropy(pk, qk, dim = 0L, base = -1, ...)\n\nentropy(...)\n\nrelative.entropy(...)\n\ncross.entropy(...)\n\n\n\nArguments\n\n\n\npk\n\n\n\nA \\(n \\times k\\) &lt;numeric&gt;-matrix of observed probabilities. The \\(i\\)-th row should sum to 1 (i.e., a valid probability distribution over the \\(k\\) classes). The first column corresponds to the first factor level in actual, the second column to the second factor level, and so on.\n\n\n\n\n\ndim\n\n\n\nAn &lt;integer&gt; value of length 1 (Default: 0). Defines the dimensions of to calculate the entropy. 0: Total entropy, 1: row-wise, 2: column-wise\n\n\n\n\n\nbase\n\n\n\nA &lt;numeric&gt; value of length 1 (Default: -1). The logarithmic base to use. Default value specifies natural logarithms.\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nqk\n\n\n\nA \\(n \\times k\\) &lt;numeric&gt;-matrix of predicted probabilities. The \\(i\\)-th row should sum to 1 (i.e., a valid probability distribution over the \\(k\\) classes). The first column corresponds to the first factor level in actual, the second column to the second factor level, and so on.\n\n\n\n\n\n\nValue\n\nA &lt;numeric&gt; value or vector:\n\n\n\n\nA single &lt;numeric&gt; value (length 1) if dim == 0.\n\n\n\n\nA &lt;numeric&gt; vector with length equal to the length of rows if dim == 1.\n\n\n\n\nA &lt;numeric&gt; vector with length equal to the length of columns if dim == 2.\n\n\n\n\n\nCalculation\n\nEntropy:\n\n\\[H(pk) = -\\sum_{i} pk_i \\log(pk_i)\\]\n\nCross Entropy:\n\n\\[H(pk, qk) = -\\sum_{i} pk_i \\log(qk_i)\\]\n\nRelative Entropy\n\n\\[D_{KL}(pk \\parallel qk) = \\sum_{i} pk_i \\log\\left(\\frac{pk_i}{qk_i}\\right)\\]\n\n\nExamples\n\n# 1) Define actual\n# and observed probabilities\n\n# 1.1) actual probabilies\npk `&lt;- matrix(\n  cbind(1/2, 1/2),\n  ncol = 2\n)\n\n# 1.2) observed (estimated) probabilites\nqk `&lt;- matrix(\n  cbind(9/10, 1/10), \n  ncol = 2\n)\n\n# 2) calculate\n# Entropy\ncat(\n  \"Entropy\", entropy(\n    pk\n  ),\n  \"Relative Entropy\", relative.entropy(\n    pk,\n    qk\n  ),\n  \"Cross Entropy\", cross.entropy(\n    pk,\n    qk\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "entropy"
    ]
  },
  {
    "objectID": "references/fbeta.html",
    "href": "references/fbeta.html",
    "title": "f-beta score",
    "section": "",
    "text": "fbeta.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe fbeta()-function computes the \\(F_\\beta\\) score, the weighted harmonic mean of precision() and recall(), between two vectors of predicted and observed factor() values. The parameter \\(\\beta\\) determines the weight of precision and recall in the combined score. The weighted.fbeta() function computes the weighted \\(F_\\beta\\) score.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfbeta(actual, predicted, beta = 1, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fbeta(actual, predicted, w, beta = 1, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfbeta(x, beta = 1, micro = NULL, na.rm = TRUE, ...)\n\nfbeta(...)\n\nweighted.fbeta(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nbeta\n\n\n\nA &lt;numeric&gt; vector of length \\(1\\) (default: \\(1\\)).\n\n\n\n\n\nmicro\n\n\n\nA &lt;logical&gt;-value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA &lt;logical&gt; value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nCalculation\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  (1 + \\beta^2) \\frac{\\text{Precision}_k \\cdot \\text{Recall}_k}{(\\beta^2 \\cdot \\text{Precision}_k) + \\text{Recall}_k}\n\\]\n\nWhere precision is \\(\\frac{\\#TP_k}{\\#TP_k + \\#FP_k}\\) and recall (sensitivity) is \\(\\frac{\\#TP_k}{\\#TP_k + \\#FN_k}\\), and \\(\\beta\\) determines the weight of precision relative to recall.\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using F1-score\n\n# 4.1) unweighted F1-score\nfbeta(\n  actual    = actual,\n  predicted = predicted,\n  beta      = 1\n)\n\n# 4.2) weighted F1-score\nweighted.fbeta(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length),\n  beta      = 1\n)\n\n# 5) evaluate overall performance\n# using micro-averaged F1-score\ncat(\n  \"Micro-averaged F1-score\", fbeta(\n    actual    = actual,\n    predicted = predicted,\n    beta      = 1,\n    micro     = TRUE\n  ),\n  \"Micro-averaged F1-score (weighted)\", weighted.fbeta(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    beta      = 1,\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "f-beta score"
    ]
  },
  {
    "objectID": "references/fdr.html",
    "href": "references/fdr.html",
    "title": "false discovery rate",
    "section": "",
    "text": "fdr.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe fdr()-function computes the false discovery rate (FDR), the proportion of false positives among the predicted positives, between two vectors of predicted and observed factor() values. The weighted.fdr() function computes the weighted false discovery rate.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfdr(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fdr(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfdr(x, micro = NULL, na.rm = TRUE, ...)\n\nfdr(...)\n\nweighted.fdr(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA &lt;logical&gt;-value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA &lt;logical&gt; value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nCalculation\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  \\frac{\\#FP_k}{\\#TP_k+\\#FP_k}\n\\]\n\nWhere \\(\\#TP_k\\) and \\(\\#FP_k\\) is the number of true psotives and false positives, respectively, for each class \\(k\\).\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using False Discovery Rate\n\n# 4.1) unweighted False Discovery Rate\nfdr(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted False Discovery Rate\nweighted.fdr(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged False Discovery Rate\ncat(\n  \"Micro-averaged False Discovery Rate\", fdr(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged False Discovery Rate (weighted)\", weighted.fdr(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "false discovery rate"
    ]
  },
  {
    "objectID": "references/fer.html",
    "href": "references/fer.html",
    "title": "false omission rate",
    "section": "",
    "text": "fer.factor\n\n\nR Documentation\n\n\n\n\nDescription\n\nThe fer()-function computes the false omission rate (FOR), the proportion of false negatives among the predicted negatives, between two vectors of predicted and observed factor() values. The weighted.fer() function computes the weighted false omission rate.\n\n\n\nUsage\n\n## S3 method for class 'factor'\nfer(actual, predicted, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'factor'\nweighted.fer(actual, predicted, w, micro = NULL, na.rm = TRUE, ...)\n\n## S3 method for class 'cmatrix'\nfer(x, micro = NULL, na.rm = TRUE, ...)\n\nfer(...)\n\nweighted.fer(...)\n\n\n\nArguments\n\n\n\nactual\n\n\n\nA vector of &lt;factor&gt;- of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\npredicted\n\n\n\nA vector of &lt;factor&gt;-vector of length \\(n\\), and \\(k\\) levels.\n\n\n\n\n\nmicro\n\n\n\nA &lt;logical&gt;-value of length \\(1\\) (default: NULL). If TRUE it returns the micro average across all \\(k\\) classes, if FALSE it returns the macro average.\n\n\n\n\n\nna.rm\n\n\n\nA &lt;logical&gt; value of length \\(1\\) (default: TRUE). If TRUE, NA values are removed from the computation. This argument is only relevant when micro != NULL. When na.rm = TRUE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(na.omit(c(1, 2, NA))). When na.rm = FALSE, the computation corresponds to sum(c(1, 2, NA), na.rm = TRUE) / length(c(1, 2, NA)).\n\n\n\n\n\n…\n\n\n\nArguments passed into other methods\n\n\n\n\n\nw\n\n\n\nA &lt;numeric&gt;-vector of length \\(n\\). NULL by default.\n\n\n\n\n\nx\n\n\n\nA confusion matrix created cmatrix().\n\n\n\n\n\n\nValue\n\nIf micro is NULL (the default), a named &lt;numeric&gt;-vector of length k\n\n\nIf micro is TRUE or FALSE, a &lt;numeric&gt;-vector of length 1\n\n\n\nCalculation\n\nThe metric is calculated for each class \\(k\\) as follows,\n\n\\[\n  \\frac{\\#FN_k}{\\#FN_k + \\#TN_k}\n\\]\n\nWhere \\(\\#FN_k\\) and \\(\\#TN_k\\) are the number of false negatives and true negatives, respectively, for each class \\(k\\).\n\n\n\nExamples\n\n# 1) recode Iris\n# to binary classification\n# problem\niris$species_num `&lt;- as.numeric(\n  iris$Species == \"virginica\"\n)\n\n# 2) fit the logistic\n# regression\nmodel `&lt;- glm(\n  formula = species_num ~ Sepal.Length + Sepal.Width,\n  data    = iris,\n  family  = binomial(\n    link = \"logit\"\n  )\n)\n\n# 3) generate predicted\n# classes\npredicted `&lt;- factor(\n  as.numeric(\n    predict(model, type = \"response\") &gt;` 0.5\n  ),\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 3.1) generate actual\n# classes\nactual `&lt;- factor(\n  x = iris$species_num,\n  levels = c(1,0),\n  labels = c(\"Virginica\", \"Others\")\n)\n\n# 4) evaluate class-wise performance\n# using False Omission Rate\n\n# 4.1) unweighted False Omission Rate\nfer(\n  actual    = actual,\n  predicted = predicted\n)\n\n# 4.2) weighted False Omission Rate\nweighted.fer(\n  actual    = actual,\n  predicted = predicted,\n  w         = iris$Petal.Length/mean(iris$Petal.Length)\n)\n\n# 5) evaluate overall performance\n# using micro-averaged False Omission Rate\ncat(\n  \"Micro-averaged False Omission Rate\", fer(\n    actual    = actual,\n    predicted = predicted,\n    micro     = TRUE\n  ),\n  \"Micro-averaged False Omission Rate (weighted)\", weighted.fer(\n    actual    = actual,\n    predicted = predicted,\n    w         = iris$Petal.Length/mean(iris$Petal.Length),\n    micro     = TRUE\n  ),\n  sep = \"\\n\"\n)",
    "crumbs": [
      "Functions",
      "false omission rate"
    ]
  }
]