[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{SLmetrics}: Machine Learning Performance Evaluation on Steroids",
    "section": "",
    "text": "Preface\n{SLmetrics} started as a personal project to learn C++, and was never really meant to be published nor, infact, named {SLmetrics}. But as time went by, and the committed time and commits grew the name stayed, and the goal to publish a functioning data science R package seemed like the natural next step.\nThe primary goal of {SLmetrics} is to be a fast, memory efficient and reliable low-level successor to {MLmetrics}; and the current benchmarks (Reference) suggets that this goal, in fact, have been achieved.\n\n\n\n\n\n\nWarning\n\n\n\n{SLmetrics} and the documentation is currently under development",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The status-quo of {pkgs}\nThere are currently three {pkgs} that are developed with machine leaning performance evaluation in mind: {MLmetrics}, {yardstick}, {mlr3measures}. These {pkgs} have historically bridged the gap between R and Python in terms of machine learning and data science.\n{MLmetrics} can be considered the legacy code when it comes to performance evaluation, and it served as a backend in {yardstick} up to version 0.0.2. It is built entirely on base R, and has been stable since its inception almost 10 years ago.\nHowever, it appears that the development has reached it’s peak and is currently stale - see, for example, this stale PR related to this issue. Micro- and macro-averages have been implented in {scikit-learn} for many years, and {MLmetrics} simply didn’t keep up with the development.\n{yardstick}, on the other hand, carried the torch forward and implemented these modern features. {yardstick} closely follows the syntax, naming and functionality of {scikit-learn} but is built with {tidyverse} tools; although the source code is nice to look at, it does introduce some serious overhead and carries the risk of deprecations.\nFurthermore, it complicates a simple application by its verbose function naming, see for example metric()-function for &lt;tbl&gt; and metric_vec()-function for &lt;numeric&gt; - the output is the same, but the call is different. {yardstick} can’t handle more than one positive class at a time, so the end-user is forced to run the same function more than once to get performance metrics for the adjacent classes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-status-quo-of-pkgs",
    "href": "intro.html#the-status-quo-of-pkgs",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Summary\nIn short, the existing {pkgs} are outdated, inefficient and insufficient for modern large-scale machine learning applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-slmetrics",
    "href": "intro.html#why-slmetrics",
    "title": "1  Introduction",
    "section": "1.2 Why {SLmetrics}?",
    "text": "1.2 Why {SLmetrics}?\nAs the name suggests, {SLmetrics} closely resembles {MLmetrics} in it’s simplistic and low-level implementation of machine learning metrics. The resemblance ends there, however.\n{SLmetrics} are developed with three things in mind: speed, efficiency and scalability. And therefore addresses the shortcomings of the status-quo by construction - the {pkg} is built on c++ and {Rcpp} from the ground up. See Table 1.1 where\n\nCode\nset.seed(1903)\nactual &lt;- rnorm(1e7)\npredicted &lt;- actual + rnorm(1e7)\n\nbench::mark(\n    `{SLmetrics}` = SLmetrics::rmse(actual, predicted),\n    `{MLmetrics}` = MLmetrics::RMSE(predicted, actual),\n    iterations    = 100\n)\n\n\n\n\nTable 1.1: Calculating RMSE on 1e7 vectors\n\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 {SLmetrics}   30.2ms     31ms      32.2    6.13MB      0  \n2 {MLmetrics}   61.3ms   61.8ms      16.2   76.39MB     79.1\n\n\n\n\nThis shows that well-written R-code is hard to beat speed-wise. {MLmetrics} is roughly 20% faster - but uses 30,000 times more memory. How about constructing a confusion matrix\n\nCode\nset.seed(1903)\nactual &lt;- factor(sample(letters[1:3], size = 1e7, replace = TRUE))\npredicted &lt;- factor(sample(letters[1:3], size = 1e7, replace = TRUE))\n\nbench::mark(\n    `{SLmetrics}` = SLmetrics::cmatrix(actual, predicted),\n    `{MLmetrics}` = MLmetrics::ConfusionMatrix(actual, predicted),\n    check         = FALSE,\n    iterations    = 100\n)\n\n\n\n\nTable 1.2: Computing a 3x3 confusion matrix on 1e7 vectors\n\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 {SLmetrics}   8.55ms   8.58ms    116.      4.12KB     0   \n2 {MLmetrics} 249.44ms 255.15ms      3.60   381.6MB     7.43\n\n\n\n\n{SLmetrics} uses 1/50th of the time {MLmetrics} and the memory usage is equivalent as the previous example but uses significantly less memory than {MLmetrics}.\n\n1.2.1 Summary\n{SLmetrics} is, in the worst-case scenario, on par with low-level R implementations of equivalent metrics and is a multitude more memory-efficient than any of the {pkgs}. A detailed benchmark can be found here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#key-takeaways",
    "href": "intro.html#key-takeaways",
    "title": "1  Introduction",
    "section": "1.3 Key takeaways",
    "text": "1.3 Key takeaways",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "2.1 Basic Usage",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html#installation",
    "href": "summary.html#installation",
    "title": "2  Summary",
    "section": "2.2 Installation",
    "text": "2.2 Installation\n\n2.2.1 Stable version\n\n## install stable release\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics@*release',\n  ref  = 'main'\n)\n\n\n\n2.2.2 Development version\n\n## install development version\ndevtools::install_github(\n  repo = 'https://github.com/serkor1/SLmetrics',\n  ref  = 'development'\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "3  Benchmarking",
    "section": "",
    "text": "3.1 The setup",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#the-setup",
    "href": "benchmark.html#the-setup",
    "title": "3  Benchmarking",
    "section": "",
    "text": "3.1.1 Regression problems\n\ncreate_regression &lt;- function(\n    n = 1e7) {\n\n  # 1) actual\n  # values\n  actual &lt;- abs(rnorm(n = n))\n\n  # 2) predicted\n  # values\n  predicted &lt;- actual + abs(rnorm(n = n))\n\n  # 3) generate\n  # weights\n  w &lt;- runif(n)\n\n  list(\n    actual    = actual,\n    predicted = predicted,\n    w         = w\n  )\n}\n\n\n\n3.1.2 Classification problems\n\ncreate_factor &lt;- function(\n    k = 3,\n    balanced = TRUE,\n    n = 1e7) {\n\n  probs &lt;- NULL\n\n  if (!balanced) {\n\n    probs &lt;- rbeta(\n      n = k,\n      shape1 = 10,\n      shape2 = 2\n    )\n\n    probs[which.min(probs)] &lt;- 0\n\n    probs &lt;- probs / sum(probs)\n\n  }\n\n  factor(\n    x = sample(\n      1:k,\n      size = n,\n      replace = TRUE,\n      prob = probs\n    ),\n    labels = letters[1:k],\n    levels = 1:k\n  )\n}\n\n\n\n3.1.3 Staging the testing ground\n\n# 1) set seed for reproducibility\nset.seed(1903)\n\n# 2) create classification\n# problem\nfct_actual &lt;- create_factor()\nfct_predicted &lt;- create_factor()\n\n# 3) create regression\n# problem\n\n# 3.1) store results\n# in regression\nlst_regression &lt;- create_regression()\n\n# 3.2) assign the values\n# accordingly\nnum_actual &lt;- lst_regression$actual\nnum_predicted &lt;- lst_regression$predicted\nnum_weights &lt;- lst_regression$w\n\n\nbenchmark &lt;- function(\n  ..., \n  m = 10) {\n  library(magrittr)\n  # 1) create list\n  # for storing values\n  performance &lt;- list()\n  \n  for (i in 1:m) {\n\n     # 1) run the benchmarks\n    results &lt;- bench::mark(\n      ...,\n      iterations = 10,\n      check = FALSE\n    )\n\n    # 2) extract values\n    # and calculate medians\n    performance$time[[i]]  &lt;- setNames(lapply(results$time, mean), results$expression)\n    performance$memory[[i]] &lt;- setNames(lapply(results$memory, function(x) { sum(x$bytes, na.rm = TRUE)}), results$expression)\n    performance$n_gc[[i]] &lt;- setNames(lapply(results$n_gc, sum), results$expression)\n\n  }\n\n  purrr::pmap_dfr(\n  list(performance$time, performance$memory, performance$n_gc), \n  ~{\n    tibble::tibble(\n      expression = names(..1),\n      time = unlist(..1),\n      memory = unlist(..2),\n      n_gc = unlist(..3)\n    )\n  }\n) %&gt;%\n  dplyr::mutate(expression = factor(expression, levels = unique(expression))) %&gt;%\n  dplyr::group_by(expression) %&gt;%\n  dplyr::filter(dplyr::row_number() &gt; 1) %&gt;%\n  dplyr::summarize(\n    execution_time = bench::as_bench_time(median(time)),\n    memory_usage = bench::as_bench_bytes(median(memory)),\n    gc_calls = median(n_gc),\n    .groups = \"drop\"\n  )\n\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#benchmarking",
    "href": "benchmark.html#benchmarking",
    "title": "3  Benchmarking",
    "section": "3.2 Benchmarking",
    "text": "3.2 Benchmarking\n\n3.2.1 Regression metrics\n\n\nCode\nbenchmark(\n    `{RMSE}`  = SLmetrics::rmse(num_actual, num_predicted),\n    `{Pinball Loss}` = SLmetrics::pinball(num_actual, num_predicted),\n    `{Huber Loss}` = SLmetrics::huberloss(num_actual, num_predicted)\n)\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {RMSE}                 30.9ms           0B        0\n#&gt; 2 {Pinball Loss}           31ms           0B        0\n#&gt; 3 {Huber Loss}           75.9ms           0B        0\n\n\n\n\nCode\nbenchmark(\n    `{SLmetrics}` = SLmetrics::rmse(num_actual, num_predicted),\n    `{MLmetrics}` = MLmetrics::RMSE(num_actual, num_predicted),\n    `{yardstick}` = yardstick::rmse_vec(num_actual, num_predicted),\n    `{mlr3measures}` = mlr3measures::rmse(num_actual, num_predicted)\n)\n\n\n#&gt; # A tibble: 4 × 4\n#&gt;   expression     execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {SLmetrics}            31.3ms           0B        0\n#&gt; 2 {MLmetrics}            62.3ms       76.3MB        1\n#&gt; 3 {yardstick}           171.3ms      419.6MB       10\n#&gt; 4 {mlr3measures}         87.8ms       76.3MB        1\n\n\n\n\n3.2.2 Classification metrics\n\n\nCode\nbenchmark(\n    `{Confusion Matrix}`  = SLmetrics::cmatrix(fct_actual, fct_predicted),\n    `{Accuracy}` = SLmetrics::accuracy(fct_actual, fct_predicted),\n    `{F-beta}` = SLmetrics::fbeta(fct_actual, fct_predicted)\n)\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression         execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;                    &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {Confusion Matrix}         8.61ms           0B        0\n#&gt; 2 {Accuracy}                 8.62ms           0B        0\n#&gt; 3 {F-beta}                   8.62ms           0B        0\n\n\n\n\nCode\nbenchmark(\n    `{SLmetrics}`    = SLmetrics::cmatrix(fct_actual, fct_predicted),\n    `{MLmetrics}`    = MLmetrics::ConfusionMatrix(fct_predicted, fct_actual),\n    `{yardstick}`    = yardstick::conf_mat(table(fct_actual, fct_predicted))\n)\n\n\n#&gt; # A tibble: 3 × 4\n#&gt;   expression  execution_time memory_usage gc_calls\n#&gt;   &lt;fct&gt;             &lt;bch:tm&gt;    &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 {SLmetrics}         8.61ms           0B        0\n#&gt; 2 {MLmetrics}       250.51ms        381MB        7\n#&gt; 3 {yardstick}       251.19ms        381MB        7",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#discussion",
    "href": "benchmark.html#discussion",
    "title": "3  Benchmarking",
    "section": "3.3 Discussion",
    "text": "3.3 Discussion\nDoes speed really matter at the milliseconds level, and justify the raîson d’être for {SLmetrics} - the answer is inevitably no. A reduction of a few milliseconds may marginally improve performance, perhaps shaving off minutes or hours in large-scale grid searches or multi-model experiments. While this might slightly reduce cloud expenses, the overall impact is often negligible unless you’re operating at an enormous scale or in latency-critical environments.\nHowever, the memory efficiency of {SLmetrics} is where its real value lies. Its near-zero RAM usage allows more memory to be allocated for valuable tasks, such as feeding larger datasets into models. This can directly lead to higher-performing models, as more data generally improves learning outcomes. Furthermore, by optimizing memory usage, {SLmetrics} can reduce infrastructure costs significantly, as less powerful machines or fewer cloud resources may be required to achieve the same — or better — results.\nIn short, while speed optimization may seem like a more visible metric, it’s the memory efficiency of {SLmetrics} that has a broader, more transformative impact on machine learning workflows, from enabling better model performance to substantial cost reductions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  },
  {
    "objectID": "benchmark.html#conclusion",
    "href": "benchmark.html#conclusion",
    "title": "3  Benchmarking",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\n{SLmetrics} is, in the worst-case scenario, on par with low-level R implementations of equivalent metrics and is a multitude more memory-efficient than any of the {pkgs}.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Benchmarking</span>"
    ]
  }
]