::: {.callout-note}
The disucssion in this section is academic, I have the outmost respect for all the developers, contributors and users of the {pkgs}. We are, afterall, united in our love for programming, data-science and `R`
:::

# Introduction

There are currently three {pkgs} that are developed with machine leaning performance evaluation in mind: [{MLmetrics}](https://github.com/yanyachen/MLmetrics), [{yardstick}](https://github.com/tidymodels/yardstick), [{mlr3measures}](https://github.com/mlr-org/mlr3measures). These {pkgs} have historically bridged the gap between `R` and `Python` in terms of machine learning and data science.

## The status-quo of {pkgs}

[{MLmetrics}](https://github.com/yanyachen/MLmetrics) can be considered *the* legacy code when it comes to performance evaluation, and it served as a backend in [{yardstick}](https://github.com/tidymodels/yardstick) up to [version 0.0.2](https://yardstick.tidymodels.org/news/index.html#yardstick-002). It is built entirely on base R, and has been stable since its inception almost 10 years ago.

However, it appears that the development has reached it's peak and is currently stale - see, for example, this stale [PR](https://github.com/yanyachen/MLmetrics/pull/3) related to this [issue](https://github.com/yanyachen/MLmetrics/issues/2). Micro- and macro-averages have been implented in [{scikit-learn}](https://github.com/scikit-learn/scikit-learn) for many years, and [{MLmetrics}](https://github.com/yanyachen/MLmetrics) simply didn't keep up with the development.

[{yardstick}](https://github.com/tidymodels/yardstick), on the other hand, carried the torch forward and implemented these modern features. [{yardstick}](https://github.com/tidymodels/yardstick) closely follows the syntax, naming and functionality of [{scikit-learn}](https://github.com/scikit-learn/scikit-learn) but is built with [{tidyverse}](https://github.com/tidyverse) tools; although the source code is nice to look at, it does introduce some serious overhead and carries the risk of deprecations.

Furthermore, it complicates a simple application by its verbose function naming, see for example `metric()`-function for `<tbl>` and `metric_vec()`-function for `<numeric>` - the output is the same, but the call is different. [{yardstick}](https://github.com/tidymodels/yardstick) can't handle more than one positive class at a time, so the end-user is forced to run the same function more than once to get performance metrics for the adjacent classes.

### Summary

In short, the existing {pkgs} are outdated, inefficient and insufficient for modern large-scale machine learning applications.

## Why {SLmetrics}?

As the name suggests, [{SLmetrics}](https://github.com/serkor1/SLmetrics) closely resembles [{MLmetrics}](https://github.com/yanyachen/MLmetrics) in it's *simplistic* and *low-level* implementation of machine learning metrics. The resemblance ends there, however.

[{SLmetrics}](https://github.com/serkor1/SLmetrics) are developed with three things in mind: *speed*, *efficiency* and *scalability*. And therefore addresses the shortcomings of the status-quo by construction - the {pkg} is built on `c++` and [{Rcpp}](https://github.com/RcppCore/Rcpp) from the ground up. See @tbl-rmse-speed where 

```{r}
#| code-fold: true
#| label: tbl-rmse-speed
#| tbl-cap: Calculating RMSE on 1e7 vectors
#| messages: false
#| warnings: false
#| cache: TRUE
set.seed(1903)
actual <- rnorm(1e7)
predicted <- actual + rnorm(1e7)

bench::mark(
    `{SLmetrics}` = SLmetrics::rmse(actual, predicted),
    `{MLmetrics}` = MLmetrics::RMSE(predicted, actual),
    iterations    = 100
)
```

This shows that well-written `R`-code is hard to beat speed-wise. [{MLmetrics}](https://github.com/yanyachen/MLmetrics) is roughly 20\% faster - but uses 30,000 times more memory. How about constructing a confusion matrix

```{r}
#| code-fold: true
#| label: tbl-confusion_matrix-speed
#| tbl-cap: Computing a 3x3 confusion matrix on 1e7 vectors
#| message: false
#| warning: false
#| cache: TRUE
set.seed(1903)
actual <- factor(sample(letters[1:3], size = 1e7, replace = TRUE))
predicted <- factor(sample(letters[1:3], size = 1e7, replace = TRUE))

bench::mark(
    `{SLmetrics}` = SLmetrics::cmatrix(actual, predicted),
    `{MLmetrics}` = MLmetrics::ConfusionMatrix(actual, predicted),
    check         = FALSE,
    iterations    = 100
)
```

[{SLmetrics}](https://github.com/serkor1/SLmetrics) uses 1/50th of the time [{MLmetrics}](https://github.com/yanyachen/MLmetrics) and the memory usage is equivalent as the previous example but uses significantly less memory than [{MLmetrics}](https://github.com/yanyachen/MLmetrics).

### Summary

[{SLmetrics}](https://github.com/serkor1/SLmetrics) is, in the worst-case scenario, on par with low-level `R` implementations of equivalent metrics and is a multitude more memory-efficient than *any* of the {pkgs}. A detailed benchmark can be found here.

## Key takeaways

