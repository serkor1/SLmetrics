{
  "hash": "6f8a663956d151f936ec0be4aa2f61eb",
  "result": {
    "engine": "knitr",
    "markdown": "::: {.callout-note}\nThe disucssion in this section is academic, I have the outmost respect for all the developers, contributors and users of the {pkgs}. We are, afterall, united in our love for programming, data-science and `R`\n:::\n\n# Introduction\n\nThere are currently three {pkgs} that are developed with machine leaning performance evaluation in mind: [{MLmetrics}](https://github.com/yanyachen/MLmetrics), [{yardstick}](https://github.com/tidymodels/yardstick), [{mlr3measures}](https://github.com/mlr-org/mlr3measures). These {pkgs} have historically bridged the gap between `R` and `Python` in terms of machine learning and data science.\n\n## The status-quo of {pkgs}\n\n[{MLmetrics}](https://github.com/yanyachen/MLmetrics) can be considered *the* legacy code when it comes to performance evaluation, and it served as a backend in [{yardstick}](https://github.com/tidymodels/yardstick) up to [version 0.0.2](https://yardstick.tidymodels.org/news/index.html#yardstick-002). It is built entirely on base R, and has been stable since its inception almost 10 years ago.\n\nHowever, it appears that the development has reached it's peak and is currently stale - see, for example, this stale [PR](https://github.com/yanyachen/MLmetrics/pull/3) related to this [issue](https://github.com/yanyachen/MLmetrics/issues/2). Micro- and macro-averages have been implented in [{scikit-learn}](https://github.com/scikit-learn/scikit-learn) for many years, and [{MLmetrics}](https://github.com/yanyachen/MLmetrics) simply didn't keep up with the development.\n\n[{yardstick}](https://github.com/tidymodels/yardstick), on the other hand, carried the torch forward and implemented these modern features. [{yardstick}](https://github.com/tidymodels/yardstick) closely follows the syntax, naming and functionality of [{scikit-learn}](https://github.com/scikit-learn/scikit-learn) but is built with [{tidyverse}](https://github.com/tidyverse) tools; although the source code is nice to look at, it does introduce some serious overhead and carries the risk of deprecations.\n\nFurthermore, it complicates a simple application by its verbose function naming, see for example `metric()`-function for `<tbl>` and `metric_vec()`-function for `<numeric>` - the output is the same, but the call is different. [{yardstick}](https://github.com/tidymodels/yardstick) can't handle more than one positive class at a time, so the end-user is forced to run the same function more than once to get performance metrics for the adjacent classes.\n\n### Summary\n\nIn short, the existing {pkgs} are outdated, inefficient and insufficient for modern large-scale machine learning applications.\n\n## Why {SLmetrics}?\n\nAs the name suggests, [{SLmetrics}](https://github.com/serkor1/SLmetrics) closely resembles [{MLmetrics}](https://github.com/yanyachen/MLmetrics) in it's *simplistic* and *low-level* implementation of machine learning metrics. The resemblance ends there, however.\n\n[{SLmetrics}](https://github.com/serkor1/SLmetrics) are developed with three things in mind: *speed*, *efficiency* and *scalability*. And therefore addresses the shortcomings of the status-quo by construction - the {pkg} is built on `c++` and [{Rcpp}](https://github.com/RcppCore/Rcpp) from the ground up. See @tbl-rmse-speed where \n\n\n\n\n\n\n\n\n::: {#tbl-rmse-speed .cell tbl-cap='Calculating RMSE on 1e7 vectors' messages='false' warnings='false'}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1903)\nactual <- rnorm(1e7)\npredicted <- actual + rnorm(1e7)\n\nbench::mark(\n    `{SLmetrics}` = SLmetrics::rmse(actual, predicted),\n    `{MLmetrics}` = MLmetrics::RMSE(predicted, actual),\n    iterations    = 100\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 {SLmetrics}     81ms   82.5ms      12.1    2.44KB       0 \n2 {MLmetrics}   68.2ms   68.5ms      14.6   76.37MB     350.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThis shows that well-written `R`-code is hard to beat speed-wise. [{MLmetrics}](https://github.com/yanyachen/MLmetrics) is roughly 20\\% faster - but uses 30,000 times more memory. How about constructing a confusion matrix\n\n\n\n\n\n\n\n\n::: {#tbl-confusion_matrix-speed .cell tbl-cap='Computing a 3x3 confusion matrix on 1e7 vectors'}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1903)\nactual <- factor(sample(letters[1:3], size = 1e7, replace = TRUE))\npredicted <- factor(sample(letters[1:3], size = 1e7, replace = TRUE))\n\nbench::mark(\n    `{SLmetrics}` = SLmetrics::cmatrix(actual, predicted),\n    `{MLmetrics}` = MLmetrics::ConfusionMatrix(actual, predicted),\n    check         = FALSE,\n    iterations    = 100\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 {SLmetrics}   6.08ms   6.12ms    163.      2.51KB     0   \n2 {MLmetrics}  303.5ms 324.09ms      2.95  381.64MB     7.63\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n[{SLmetrics}](https://github.com/serkor1/SLmetrics) uses 1/50th of the time [{MLmetrics}](https://github.com/yanyachen/MLmetrics) and the memory usage is equivalent as the previous example but uses significantly less memory than [{MLmetrics}](https://github.com/yanyachen/MLmetrics).\n\n### Summary\n\n[{SLmetrics}](https://github.com/serkor1/SLmetrics) is, in the worst-case scenario, on par with low-level `R` implementations of equivalent metrics and is a multitude more memory-efficient than *any* of the {pkgs}. A detailed benchmark can be found here.\n\n## Key takeaways\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}